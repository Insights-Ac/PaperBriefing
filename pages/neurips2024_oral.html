
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - NeurIPS 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">NeurIPS 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated on 2024-11-18 13:27:02 by <a href="https://github.com/Logan-Lin/PubSummarizer">PubSummarizer</a></em></p>
        <div class="row" data-masonry='{"percentPosition": true }'>
            <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ge8GZn8Gtu&name=pdf" class="link-primary">https://openreview.net/attachment?id=ge8GZn8Gtu&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Gaussian Mixture Models</span>
<span class="badge bg-primary">Clustering</span>
<span class="badge bg-primary">Anisotropic Covariance</span>
<span class="badge bg-primary">Minimax Lower Bounds</span>
<span class="badge bg-primary">Lloyd's Algorithm</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates clustering in Gaussian Mixture Models (GMMs) with anisotropic covariance structures, addressing both homogeneous (identical covariance matrices across clusters) and heterogeneous scenarios (distinct covariance matrices). The authors derive minimax lower bounds for clustering accuracy that highlight the significance of covariance structures on clustering performance. They introduce an adjusted variant of Lloyd's algorithm, which iteratively estimates covariance information and demonstrates optimality within a logarithmic number of iterations. The proposed algorithm bridges theoretical insights and practical efficiency, outperforming existing methods in numerical studies, thus reinforcing the critical role of covariance in clustering tasks.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Aligner: Efficient Alignment by Learning to Correct</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=kq166jACVP&name=pdf" class="link-primary">https://openreview.net/attachment?id=kq166jACVP&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">alignment</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">efficiency</span>
<span class="badge bg-primary">human feedback</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Aligner, a novel model-agnostic alignment approach designed to enhance the performance of large language models (LLMs) by correcting their responses to better align with human values. Aligner operates by learning correctional residuals between preferred and dispreferred answers using a smaller model, making it efficient and easy to integrate with various existing LLMs without needing extensive retraining or access to their parameters. The experiments demonstrate that Aligner significantly improves helpfulness and harmlessness across multiple LLMs, achieving an average boost of 68.9% in helpfulness and 23.8% in harmlessness. Its plug-and-play nature allows for rapid deployment in real-world scenarios, addressing the challenges posed by traditional alignment methods such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Overall, Aligner provides a streamlined solution for aligning LLMs with human intentions while maintaining computational efficiency.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Bayesian-guided Label Mapping for Visual Reprogramming</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=135eKqDoRR&name=pdf" class="link-primary">https://openreview.net/attachment?id=135eKqDoRR&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">visual reprogramming</span>
<span class="badge bg-primary">label mapping</span>
<span class="badge bg-primary">Bayesian methods</span>
<span class="badge bg-primary">pretrained models</span>
<span class="badge bg-primary">deep learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a novel approach called Bayesian-guided Label Mapping (BLM) to enhance visual reprogramming (VR) by addressing the limitations of traditional one-to-one label mapping methods. BLM establishes a probabilistic mapping between pretrained and downstream labels, capturing the complex relationships that are often overlooked by existing methods. It employs Bayesian conditional probabilities to iteratively update a label mapping matrix, which better accounts for the many-to-many relationships between distinct label spaces. The paper demonstrates that BLM and its extension, BLM+, outperform existing methods across various datasets and models, providing both improved accuracy and a deeper understanding of the label correspondence in VR tasks. The findings underscore the effectiveness of probabilistic approaches in leveraging pretrained models for diverse applications without modifying the underlying model architecture.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Vi8AepAXGy&name=pdf" class="link-primary">https://openreview.net/attachment?id=Vi8AepAXGy&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">multimodal LLMs</span>
<span class="badge bg-primary">visual representation learning</span>
<span class="badge bg-primary">instruction tuning</span>
<span class="badge bg-primary">spatial vision aggregator</span>
<span class="badge bg-primary">benchmarking</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Cambrian-1, a series of multimodal large language models (MLLMs) designed to enhance visual understanding through a vision-centric approach. It addresses the disconnect between visual representation learning and existing MLLM capabilities, proposing a new benchmark called CV-Bench to assess visual grounding effectively. The study evaluates over 20 vision encoders and introduces the Spatial Vision Aggregator (SVA), a novel connector that integrates visual features with LLMs while optimizing token usage. Through extensive experiments, Cambrian-1 demonstrates superior performance across various tasks, underlining the importance of high-quality instruction-tuning data and effective model architectures in advancing multimodal systems. The models and associated resources are openly shared to promote further research and innovation in the field.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=TFZlFRl9Ks&name=pdf" class="link-primary">https://openreview.net/attachment?id=TFZlFRl9Ks&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D reconstruction</span>
<span class="badge bg-primary">multi-view synthesis</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">real-time rendering</span>
<span class="badge bg-primary">generative priors</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces CAT3D, a novel approach for rapid 3D scene creation using a multi-view diffusion model that generates consistent novel views from any number of input images. This method addresses the challenges of traditional 3D reconstruction methods, which typically require extensive image collections, by generating additional views that enhance the reconstruction process. CAT3D significantly reduces the time needed for creating 3D scenes, achieving results in as little as one minute and outperforming existing techniques across various benchmarks. The system's architecture decouples the view generation from the 3D reconstruction, enhancing efficiency and visual quality while allowing for flexible input types, including single images and text prompts. The results indicate CAT3D's potential for democratizing high-quality 3D content creation, though it has limitations in handling diverse camera intrinsics and generating fully consistent views.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Convolutional Differentiable Logic Gate Networks</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4bKEFyUHT4&name=pdf" class="link-primary">https://openreview.net/attachment?id=4bKEFyUHT4&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">logic gate networks</span>
<span class="badge bg-primary">efficient inference</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">convolutional architectures</span>
<span class="badge bg-primary">CIFAR-10</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents advancements in Convolutional Differentiable Logic Gate Networks (LGNs), which enhance inference efficiency in machine learning models by utilizing logic gate operators instead of conventional neural networks. The authors introduce several innovations, including deep logic gate tree convolutions, logical OR pooling, and residual initializations, enabling LGNs to scale significantly in size and accuracy. The proposed architecture, LogicTreeNet, achieves an accuracy of 86.29% on the CIFAR-10 dataset with only 61 million logic gates, demonstrating a 29-fold reduction in gate count compared to state-of-the-art methods. The approach not only improves accuracy but also significantly accelerates inference speed, making it suitable for hardware implementations in embedded and real-time applications.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bCMpdaQCNW&name=pdf" class="link-primary">https://openreview.net/attachment?id=bCMpdaQCNW&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">humor</span>
<span class="badge bg-primary">AI comprehension</span>
<span class="badge bg-primary">comics</span>
<span class="badge bg-primary">juxtaposition</span>
<span class="badge bg-primary">narrative reasoning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper examines the challenges faced by AI models, particularly large multimodal language models, in understanding humor derived from juxtaposition in comics, specifically focusing on comics with contradictory narratives. The authors introduce the Y ESBUT benchmark, a dataset designed to evaluate AI capabilities in recognizing and interpreting these comics through various tasks that assess levels of narrative comprehension and reasoning. Experimental results reveal that even state-of-the-art models struggle to match human performance in understanding the nuanced humor of comics, particularly in nonlinear narratives that require deep reasoning. The findings highlight the limitations of current AI in processing complex human expressions and suggest directions for future research to improve AI's interpretative abilities in comprehending humor and cultural contexts.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Pezt0xttae&name=pdf" class="link-primary">https://openreview.net/attachment?id=Pezt0xttae&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Federated Learning</span>
<span class="badge bg-primary">Domain Adaptation</span>
<span class="badge bg-primary">Edge Devices</span>
<span class="badge bg-primary">Model Pruning</span>
<span class="badge bg-primary">Heterogeneous Systems</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents DapperFL, a novel framework for Federated Learning (FL) designed to enhance model performance in heterogeneous edge computing environments that face system heterogeneity and domain shifts. DapperFL introduces two key modules: Model Fusion Pruning (MFP), which generates personalized compact local models by integrating knowledge from both local and other domains to mitigate the effects of heterogeneity; and Domain Adaptive Regularization (DAR), which improves the robustness of the model against domain shifts through regularization techniques. The framework also includes a specialized aggregation algorithm tailored for heterogeneous models. Experimental results demonstrate that DapperFL outperforms existing FL frameworks by achieving up to 2.28% higher accuracy while significantly reducing model size, making it suitable for deployment on resource-constrained edge devices.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=NPKZF1WDjZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=NPKZF1WDjZ&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reasoning framework</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">question decomposition</span>
<span class="badge bg-primary">cognitive intelligence</span>
<span class="badge bg-primary">error correction</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents DeAR (Decompose-Analyze-Rethink), a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) by mimicking human-like cognitive processes. DeAR iteratively constructs a reasoning tree through three stages: Decompose, where complex questions are broken down into simpler sub-questions; Analyze, which generates and evaluates rationales for these sub-questions; and Rethink, which updates the parent node rationales based on insights from the child nodes. The framework significantly improves accuracy and logical coherence in reasoning tasks compared to existing methods like Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT), as demonstrated through extensive experiments on various reasoning benchmarks. DeAR not only reduces logical errors but also optimizes the balance between reasoning accuracy and processing time, thereby showcasing its practical utility in complex problem-solving scenarios.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">DenoiseRep: Denoising Model for Representation Learning</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=OycU0bAus6&name=pdf" class="link-primary">https://openreview.net/attachment?id=OycU0bAus6&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">denoising</span>
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">discriminative tasks</span>
<span class="badge bg-primary">feature extraction</span>
<span class="badge bg-primary">generative models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces DenoiseRep, a novel denoising model aimed at enhancing representation learning for discriminative tasks. By treating each embedding layer in a neural network backbone as a denoising layer, DenoiseRep integrates feature extraction and denoising processes, effectively improving feature discrimination without incurring additional computational costs. It operates as a label-free algorithm, demonstrating its capability across various vision tasks, including person re-identification, image classification, object detection, and image segmentation. Experimental results affirm its stability and effectiveness, showcasing improvements in performance metrics across different datasets and architectures, while also validating its theoretical underpinnings regarding parameter fusion and denoising equivalence.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cFqAANINgW&name=pdf" class="link-primary">https://openreview.net/attachment?id=cFqAANINgW&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">code generation</span>
<span class="badge bg-primary">divide-and-conquer</span>
<span class="badge bg-primary">functional consensus</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">self-testing</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents FUNCODER, a novel code generation framework that integrates a divide-and-conquer strategy with functional consensus to tackle complex programming requirements. Unlike traditional methods that often require a complete plan from the start, FUNCODER decomposes tasks into smaller sub-functions, allowing for iterative problem-solving. It employs functional consensus by sampling multiple function implementations to select the most consistent one, thereby reducing error propagation. Experiments demonstrate FUNCODER's superiority over state-of-the-art methods, achieving significant performance improvements across various benchmarks, including HumanEval, MBPP, xCodeEval, and MATH, particularly excelling in generating correct code for complex tasks and enhancing the capabilities of smaller language models.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Do Finetti: On Causal Effects for Exchangeable Data</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4rCZeCZAON&name=pdf" class="link-primary">https://openreview.net/attachment?id=4rCZeCZAON&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">causal effect estimation</span>
<span class="badge bg-primary">exchangeable data</span>
<span class="badge bg-primary">independent causal mechanisms</span>
<span class="badge bg-primary">Do-Finetti algorithm</span>
<span class="badge bg-primary">multi-environment data</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of estimating causal effects in scenarios where data is exchangeable rather than independently and identically distributed (i.i.d.). Traditional causal frameworks, which rely on structural causal models and do-calculus, are insufficient for exchangeable data that follows independent causal mechanisms (ICM). The authors propose a generalized framework for such data, introducing a truncated factorization formula for identifying and estimating causal effects. They illustrate the practical implications of their approach through a causal Plya urn model and develop the Do-Finetti algorithm, which allows simultaneous causal structure recovery and effect estimation from multi-environment data. The findings suggest that exchangeable data can enhance causal identification capabilities compared to i.i.d. data, thereby advancing methodologies for causal inference in complex scenarios.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mp8u2Pcmqz&name=pdf" class="link-primary">https://openreview.net/attachment?id=mp8u2Pcmqz&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">outliers</span>
<span class="badge bg-primary">dual transformations</span>
<span class="badge bg-primary">performance optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces DuQuant, a novel quantization method for large language models (LLMs) designed to effectively address the challenges posed by both normal and massive outliers in activation data. Traditional quantization techniques struggle with massive outliers that can significantly degrade model performance when using low-bit representations. DuQuant employs dual transformations—rotation and permutation—to redistribute outlier values, enhancing the model's quantization process. By implementing a block-wise rotation matrix based on prior knowledge of outlier dimensions, followed by a zigzag permutation to balance outlier distributions across channels, DuQuant simplifies quantization and improves performance across a range of tasks. The method demonstrates superior results compared to existing state-of-the-art approaches, achieving notable improvements in both accuracy and efficiency, especially in low-bit quantization scenarios.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=47loYmzxep&name=pdf" class="link-primary">https://openreview.net/attachment?id=47loYmzxep&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">multimodal fusion</span>
<span class="badge bg-primary">object detection</span>
<span class="badge bg-primary">autonomous driving</span>
<span class="badge bg-primary">end-to-end learning</span>
<span class="badge bg-primary">optimization strategies</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents E2E-MFD, an innovative end-to-end algorithm for multimodal fusion detection, which integrates image fusion and object detection into a single training process. This approach addresses the inefficiencies of existing methods by utilizing synchronous joint optimization, allowing for improved performance in both tasks without the complexity of multi-step training. The authors introduce a novel Object-Region-Pixel Phylogenetic Tree (ORPPT) and a Gradient Matrix Task-Alignment (GMTA) technique to enhance the optimization of shared parameters. Extensive experiments on public datasets demonstrate that E2E-MFD achieves superior results in both image fusion and object detection, outperforming state-of-the-art methods by significant margins. The results indicate that the proposed method is particularly effective in challenging scenarios, such as detecting objects in diverse visual environments, thus holding promise for applications in autonomous driving and remote sensing.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Enhancing Preference-based Linear Bandits via Human Response Time</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aIPwlkdOut&name=pdf" class="link-primary">https://openreview.net/attachment?id=aIPwlkdOut&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">preference learning</span>
<span class="badge bg-primary">human response time</span>
<span class="badge bg-primary">bandit algorithms</span>
<span class="badge bg-primary">utility estimation</span>
<span class="badge bg-primary">computational efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a novel approach to enhance preference-based linear bandits by integrating human response times as a supplementary source of information alongside traditional binary choice feedback. The authors leverage the EZ-diffusion model to develop a computationally efficient estimator that combines choices and response times, providing deeper insights into preference strength. Theoretical analyses and empirical simulations demonstrate that this incorporation significantly improves the accuracy of utility estimates, particularly for queries where strong preferences exist. The proposed method is integrated into a bandit algorithm for fixed-budget best-arm identification, showing improved performance on three real-world datasets compared to conventional choice-only estimators. The findings suggest that response times can effectively augment preference learning in interactive systems, though challenges remain in ensuring participant engagement and data reliability in practical applications.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=R8SolCx62K&name=pdf" class="link-primary">https://openreview.net/attachment?id=R8SolCx62K&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Graph Contrastive Learning</span>
<span class="badge bg-primary">Representation Scattering</span>
<span class="badge bg-primary">Self-Supervised Learning</span>
<span class="badge bg-primary">Node Representation</span>
<span class="badge bg-primary">Topology-Based Constraints</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores a common latent mechanism known as representation scattering that underlies three mainstream frameworks of Graph Contrastive Learning (GCL): node discrimination, group discrimination, and bootstrapping schemes. The authors introduce Scattering Graph Representation Learning (SGRL), a novel framework that enhances representation diversity through a center-away strategy while incorporating a topology-based constraint to maintain graph structural properties. Through extensive evaluations on benchmark datasets, SGRL demonstrates superior performance compared to existing GCL methods, highlighting the significance of representation scattering in advancing graph representation learning.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=C4NbtYnyQg&name=pdf" class="link-primary">https://openreview.net/attachment?id=C4NbtYnyQg&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Flipped Classroom</span>
<span class="badge bg-primary">Generalized Category Discovery</span>
<span class="badge bg-primary">Teacher-Student Model</span>
<span class="badge bg-primary">Attention Alignment</span>
<span class="badge bg-primary">Semi-Supervised Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces FlipClass, a novel approach to Generalized Category Discovery (GCD) that enhances the traditional teacher-student model by dynamically aligning teacher attention with student learning. Recognizing the challenges faced in open-world settings, where new classes emerge without prior knowledge, the authors identify attention inconsistency as a key issue leading to suboptimal learning outcomes. FlipClass addresses this by allowing the teacher to adapt its focus based on real-time feedback from the student, promoting synchronized learning and improving performance on both old and new classes. Rigorous experiments across multiple datasets demonstrate that FlipClass outperforms existing state-of-the-art GCD methods significantly, establishing new benchmarks in the field.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=m1a4CrRJR7&name=pdf" class="link-primary">https://openreview.net/attachment?id=m1a4CrRJR7&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">recommender systems</span>
<span class="badge bg-primary">generalization error</span>
<span class="badge bg-primary">tree structure</span>
<span class="badge bg-primary">Rademacher complexity</span>
<span class="badge bg-primary">ranking model</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the generalization error bounds for two-stage recommender systems featuring a tree structure, comprising a fast tree-based retriever and a more accurate ranker. By employing an error decomposition framework and utilizing Rademacher complexity, the authors derive upper bounds for generalization errors across various models within the two-stage system. The findings reveal that increasing the number of branches in the tree structures and aligning training distributions across the retrieval and ranking stages significantly enhances the overall generalization performance. Empirical validation using real-world datasets supports these theoretical insights, emphasizing the importance of model architecture and data harmonization in improving recommender system effectiveness.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tnh4LK72yj&name=pdf" class="link-primary">https://openreview.net/attachment?id=tnh4LK72yj&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">spatiotemporal learning</span>
<span class="badge bg-primary">multi-task learning</span>
<span class="badge bg-primary">urban intelligence</span>
<span class="badge bg-primary">data adaptation</span>
<span class="badge bg-primary">machine learning frameworks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces a Continuous Multi-task Spatio-Temporal learning framework (CMuST) designed to overcome the limitations of traditional spatiotemporal models that operate in isolation and struggle with data sparsity and dynamic urban environments. CMuST promotes cooperative learning by leveraging a Multi-dimensional Spatio-Temporal Interaction Network (MSTI) to capture complex interdependencies among various urban data sources, and a Rolling Adaptation training scheme (RoAda) to enhance task-specific learning while maintaining task uniqueness. The framework effectively integrates data from different urban contexts, enabling improved generalization and adaptability across diverse spatiotemporal tasks. Experimental results from three city datasets demonstrate CMuST's superior performance compared to existing methods, particularly in scenarios with limited data, thus paving the way for enhanced urban intelligence through collaborative data modeling.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=SSCtCq2MH2&name=pdf" class="link-primary">https://openreview.net/attachment?id=SSCtCq2MH2&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Physical property identification</span>
<span class="badge bg-primary">Gaussian representation</span>
<span class="badge bg-primary">Dynamic reconstruction</span>
<span class="badge bg-primary">Simulation</span>
<span class="badge bg-primary">System identification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a novel hybrid framework called Gaussian-Informed Continuum (GIC) for estimating physical properties of objects through visual observations. By leveraging 3D Gaussian representations, the framework captures explicit object shapes and facilitates the generation of simulated continuums that render 2D shape surrogates for training. A key innovation is the dynamic 3D Gaussian network that employs motion factorization to recover object geometries across time states. The study introduces a coarse-to-fine filling strategy to produce density fields from Gaussian reconstructions, allowing for the extraction of object surfaces and integration of Gaussian attributes. Experimental results demonstrate that GIC achieves state-of-the-art performance in physical property estimation and dynamic scene reconstruction across multiple benchmarks, with practical applications shown through real-world examples in digital twins and robotic manipulation.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Graph Diffusion Transformers for Multi-Conditional Molecular Generation</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cfrDLD1wfO&name=pdf" class="link-primary">https://openreview.net/attachment?id=cfrDLD1wfO&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">molecular generation</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">graph neural networks</span>
<span class="badge bg-primary">conditional generation</span>
<span class="badge bg-primary">inverse design</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces the Graph Diffusion Transformer (Graph DiT), a novel approach for multi-conditional molecular generation through diffusion models. Unlike traditional methods that often simplify multiple property constraints into a single condition, Graph DiT effectively integrates both categorical and numerical property representations into its architecture. The model employs a unique graph-dependent noise model that enhances the accuracy of noise estimation during molecular denoising, allowing it to generate polymers and small molecules that satisfy various property constraints simultaneously. Extensive experiments demonstrate that Graph DiT significantly outperforms existing models across multiple evaluative metrics, showcasing its potential utility in material and drug discovery, particularly in tasks involving gas separation through polymer design.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Guiding a Diffusion Model with a Bad Version of Itself</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bg6fVPVs3s&name=pdf" class="link-primary">https://openreview.net/attachment?id=bg6fVPVs3s&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">image generation</span>
<span class="badge bg-primary">classifier-free guidance</span>
<span class="badge bg-primary">autoguidance</span>
<span class="badge bg-primary">image quality</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces a novel method called autoguidance for improving image quality in diffusion models by leveraging a smaller, less-trained version of the model itself for guidance, rather than an unconditional model as commonly practiced. The authors demonstrate that this approach allows for disentangled control over image quality and variation, achieving significant improvements in image generation quality while maintaining diversity in outputs. Quantitative evaluations show that autoguidance yields state-of-the-art results on ImageNet datasets, outperforming traditional classifier-free guidance methods, and is applicable to both conditional and unconditional diffusion models. The findings highlight the potential for better control in generative modeling and pave the way for future explorations in this area.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Human Expertise in Algorithmic Prediction</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=wpGJ2AX6SZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=wpGJ2AX6SZ&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">human-AI collaboration</span>
<span class="badge bg-primary">algorithmic predictions</span>
<span class="badge bg-primary">human expertise</span>
<span class="badge bg-primary">performance improvement</span>
<span class="badge bg-primary">healthcare applications</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a framework for integrating human expertise into algorithmic predictions, addressing the challenge of algorithmic indistinguishability, where inputs appear similar to predictive algorithms. The authors argue that human judgment can enhance algorithm performance by providing insights not captured in training data. They demonstrate that while algorithms often outperform human experts on average, there are specific instances where human input significantly improves predictions, particularly in clinical settings like X-ray classifications. The approach includes empirical testing, revealing that nearly 30% of patients exhibit cases where expert input surpasses algorithmic predictions, thus highlighting the potential for effective human-AI collaboration in prediction tasks.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qEpi8uWX3N&name=pdf" class="link-primary">https://openreview.net/attachment?id=qEpi8uWX3N&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Parameter-Efficient Fine-Tuning</span>
<span class="badge bg-primary">HydraLoRA</span>
<span class="badge bg-primary">Asymmetric Architecture</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Fine-Tuning Efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces HydraLoRA, an innovative asymmetric architecture for Parameter-Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs), addressing the limitations of existing methods like LoRA, particularly in complex domains. Through detailed experiments, the authors highlight that traditional single LoRA configurations can experience detrimental task interference, leading to suboptimal performance. HydraLoRA counteracts this by utilizing a shared low-rank matrix for commonalities across tasks while deploying distinct matrices for specific tasks, thereby enhancing both parameter efficiency and model performance without the need for domain expertise. Experimental results demonstrate that HydraLoRA consistently outperforms other PEFT approaches, showcasing its potential for efficient and effective adaptations in various domains.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=S2P6KPLtm8&name=pdf" class="link-primary">https://openreview.net/attachment?id=S2P6KPLtm8&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Mendelian Randomization</span>
<span class="badge bg-primary">Bi-Directional Causal Effects</span>
<span class="badge bg-primary">Instrumental Variables</span>
<span class="badge bg-primary">Causal Inference</span>
<span class="badge bg-primary">Observational Data</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the complexities of estimating causal effects in bi-directional Mendelian randomization (MR) using observational data, particularly when faced with invalid instruments and unmeasured confounding. It establishes necessary and sufficient conditions for identifying valid instrumental variable (IV) sets and the causal directions between two phenotypes. The authors propose a new algorithm, PReBiM, which effectively identifies valid IVs and estimates causal effects, demonstrating its accuracy through theoretical proofs and extensive simulations. The results indicate that PReBiM outperforms existing methods across various scenarios, including those with one-directional and bi-directional relationships, confirming its utility in the field of causal inference.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Improved Distribution Matching Distillation for Fast Image Synthesis</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tQukGCDaNT&name=pdf" class="link-primary">https://openreview.net/attachment?id=tQukGCDaNT&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">image synthesis</span>
<span class="badge bg-primary">distribution matching</span>
<span class="badge bg-primary">generative models</span>
<span class="badge bg-primary">GAN</span>
<span class="badge bg-primary">text-to-image</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces DMD2, an enhanced version of Distribution Matching Distillation (DMD), aimed at improving the efficiency of image synthesis from diffusion models. DMD2 eliminates the need for a regression loss that was previously integral to DMD, which required extensive data collection and limited the performance of student models. The authors propose several techniques, including a two time-scale update rule to stabilize training and the integration of a GAN loss to improve sample quality by training the model on real images. DMD2 supports multi-step generation and addresses input mismatches during training and inference, resulting in state-of-the-art performance with significantly reduced inference costs. The experimental results demonstrate that DMD2 outperforms prior methods and even its teacher model on various benchmarks, achieving impressive FID scores and showcasing high-quality image generation capabilities.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=UdxpjKO2F9&name=pdf" class="link-primary">https://openreview.net/attachment?id=UdxpjKO2F9&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Unsupervised Environment Design</span>
<span class="badge bg-primary">Environment Novelty</span>
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Curriculum Learning</span>
<span class="badge bg-primary">Gaussian Mixture Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel framework named Coverage-based Evaluation of Novelty In Environment (CENIE) aimed at improving Unsupervised Environment Design (UED) by effectively quantifying environment novelty. Traditional UED methods primarily utilize regret as a metric for curriculum design, which can lead to insufficient exploration and limited generalization of agents. CENIE leverages the state-action space coverage of student agents from previous experiences to measure and prioritize novel environments, enhancing exploration while progressively increasing complexity. The authors demonstrate that integrating CENIE with existing UED algorithms not only achieves state-of-the-art performance in zero-shot generalization across multiple benchmarks, but also underscores the importance of balancing novelty with regret in training robust reinforcement learning agents.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Learning diffusion at lightspeed</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=y10avdRFNK&name=pdf" class="link-primary">https://openreview.net/attachment?id=y10avdRFNK&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion processes</span>
<span class="badge bg-primary">JKOnet</span>
<span class="badge bg-primary">energy functionals</span>
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">cellular processes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces JKOnet, a novel method for learning diffusion processes from observational data, which addresses the limitations of existing models that rely on complex bilevel optimization strategies. By reformulating the problem using first-order optimality conditions, JKOnet simplifies the learning task, allowing for efficient recovery of potential, interaction, and internal energy components of diffusion processes. This method outperforms traditional baselines in terms of computational efficiency and prediction accuracy, particularly in applications involving cellular dynamics, where it achieves state-of-the-art results at significantly reduced computational costs. The authors provide extensive theoretical backing and experimental validation to demonstrate the robustness and scalability of JKOnet in high-dimensional contexts.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Learning Formal Mathematics From Intrinsic Motivation</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uNKlTQ8mBD&name=pdf" class="link-primary">https://openreview.net/attachment?id=uNKlTQ8mBD&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">intrinsic motivation</span>
<span class="badge bg-primary">mathematical reasoning</span>
<span class="badge bg-primary">theorem proving</span>
<span class="badge bg-primary">conjecturing</span>
<span class="badge bg-primary">reinforcement learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces MINIMO (Mathematics from Intrinsic Motivation), an AI agent designed to autonomously learn formal mathematics by generating and proving conjectures based solely on axioms within a dependent type theory framework. The agent employs a combination of constrained decoding and type-directed synthesis to formulate valid conjectures, which it then attempts to prove using a Monte Carlo Tree Search (MCTS) method guided by its own learned policy and value functions. A significant innovation is the use of hindsight relabeling, which enhances the agent's learning efficiency by allowing it to reinterpret failed proof attempts as successful learning experiences. Experiments across three mathematical domains—propositional logic, arithmetic, and group theory—demonstrate that MINIMO can effectively self-improve in both generating challenging conjectures and successfully proving them, indicating potential for autonomous mathematical discovery without reliance on human-generated content.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Learning rigid-body simulators over implicit shapes for large-scale scenes and vision</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=QDYts5dYgq&name=pdf" class="link-primary">https://openreview.net/attachment?id=QDYts5dYgq&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">rigid-body simulation</span>
<span class="badge bg-primary">learned simulators</span>
<span class="badge bg-primary">signed-distance functions</span>
<span class="badge bg-primary">graph networks</span>
<span class="badge bg-primary">large-scale scenes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces SDF-Sim, a novel learned rigid-body simulator designed to efficiently model large-scale scenes containing numerous rigid objects. By utilizing signed-distance functions (SDFs) to represent object shapes, SDF-Sim significantly improves runtime and memory efficiency compared to traditional mesh-based simulators. This work leverages graph networks to predict object dynamics and enables simulations of scenes with up to 1.1 million nodes, a substantial increase over previous methodologies. Moreover, SDF-Sim can extract SDFs directly from multi-view images, facilitating the simulation of complex real-world scenes. The findings highlight SDF-Sim's capability to balance scalability and simulation accuracy, making it suitable for applications in robotics, film, and virtual reality.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aVh9KRZdRk&name=pdf" class="link-primary">https://openreview.net/attachment?id=aVh9KRZdRk&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">in-context learning</span>
<span class="badge bg-primary">skill composition</span>
<span class="badge bg-primary">modular arithmetic</span>
<span class="badge bg-primary">generalization</span>
<span class="badge bg-primary">transformer models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the emergence of in-context learning (ICL) and skill composition capabilities in large language models (LLMs) through a series of modular arithmetic tasks. The authors empirically demonstrate that a GPT-style transformer transitions from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. Notably, they find that deeper models exhibit a transient phase of out-of-distribution generalization, requiring early stopping to optimize performance. An interpretability study reveals structured representations in attention heads and MLPs, indicating a significant algorithmic shift from memorization to generalization as the model encounters more diverse tasks. The findings underscore the complexities of how LLMs learn and apply learned functions in novel contexts.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">LLM Evaluators Recognize and Favor Their Own Generations</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4NJBV6Wp0h&name=pdf" class="link-primary">https://openreview.net/attachment?id=4NJBV6Wp0h&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LLM self-evaluation</span>
<span class="badge bg-primary">self-recognition</span>
<span class="badge bg-primary">self-preference bias</span>
<span class="badge bg-primary">AI safety</span>
<span class="badge bg-primary">language models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the phenomenon of self-preference in large language models (LLMs), where an LLM evaluator rates its own outputs more favorably compared to those generated by others, while human annotators perceive these outputs as equal in quality. The authors investigate whether this self-preference stems from the model's ability to recognize its own outputs, termed self-recognition. Through experiments with models like GPT-4 and Llama 2, they find that these LLMs can indeed distinguish their outputs with significant accuracy and that fine-tuning increases both self-recognition and self-preference in a linear correlation. The study highlights the implications of self-recognition for bias in evaluations and overall AI safety, suggesting the need for careful consideration in the design of self-evaluating systems.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=V0oJaLqY4E&name=pdf" class="link-primary">https://openreview.net/attachment?id=V0oJaLqY4E&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Inverse Reinforcement Learning</span>
<span class="badge bg-primary">Diffusion Models</span>
<span class="badge bg-primary">Energy-Based Models</span>
<span class="badge bg-primary">Maximum Entropy</span>
<span class="badge bg-primary">Sample Quality</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces a novel approach called Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI), aimed at enhancing the sample quality of diffusion generative models, particularly when using a limited number of generation steps. It combines an inverse reinforcement learning framework with energy-based models, allowing for joint training that improves the diffusion model's performance by using log probability density as a reward signal. The authors also propose a dynamic programming-based algorithm, Diffusion by Dynamic Programming (DxDP), to efficiently update the diffusion model without requiring extensive computational resources. Experimental results demonstrate that models fine-tuned with DxMI can generate high-quality samples in as few as 4 to 10 steps and effectively train energy-based models for anomaly detection, showing significant improvements over traditional methods.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=EKdk4vxKO4&name=pdf" class="link-primary">https://openreview.net/attachment?id=EKdk4vxKO4&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LLMs</span>
<span class="badge bg-primary">medical decision-making</span>
<span class="badge bg-primary">multi-agent collaboration</span>
<span class="badge bg-primary">adaptive frameworks</span>
<span class="badge bg-primary">medical diagnostics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces MDAgents, a novel framework that utilizes multiple Large Language Models (LLMs) to enhance medical decision-making through adaptive collaboration based on task complexity. The framework dynamically assigns roles to LLMs, mimicking real-world medical decision processes, and has been evaluated against various medical benchmarks, outperforming baseline methods in accuracy. MDAgents demonstrated significant improvements in multi-modal reasoning and efficiency, achieving the best performance in seven out of ten benchmarks. The framework effectively determines medical complexity to optimize collaboration, highlighting the importance of integrating diverse medical expertise in complex scenarios. The study emphasizes the adaptability of MDAgents in improving LLM-assisted medical diagnostics while addressing potential risks and limitations inherent in AI applications in healthcare.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=x7pjdDod6Z&name=pdf" class="link-primary">https://openreview.net/attachment?id=x7pjdDod6Z&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D Reconstruction</span>
<span class="badge bg-primary">Mesh Generation</span>
<span class="badge bg-primary">Machine Learning</span>
<span class="badge bg-primary">Transformer Networks</span>
<span class="badge bg-primary">Normal Maps</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces MeshFormer, a novel model for high-quality 3D mesh generation that efficiently reconstructs 3D textured meshes from a sparse set of multi-view RGB images and corresponding normal maps. Unlike traditional methods that rely on extensive training and dense input views, MeshFormer leverages a 3D voxel representation and integrates transformers with 3D convolutions to utilize spatial structure and projective biases. By incorporating normal maps as additional input and employing signed distance function (SDF) supervision, the model achieves fast training and generates detailed meshes in a single feed-forward pass. MeshFormer outperforms existing state-of-the-art methods in terms of quality and efficiency, requiring significantly fewer computational resources while enabling various applications, including text-to-3D generation.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Y8YVCOMEpz&name=pdf" class="link-primary">https://openreview.net/attachment?id=Y8YVCOMEpz&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">linear attention</span>
<span class="badge bg-primary">softmax approximation</span>
<span class="badge bg-primary">MetaLA</span>
<span class="badge bg-primary">Transformer models</span>
<span class="badge bg-primary">memory efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces MetaLA, a theoretical advancement and practical implementation designed to provide an optimal linear approximation to softmax attention mechanisms in Transformer architectures. It unifies various existing linear complexity models (e.g., LinFormer, SSM, and LinRNN) under a common framework and establishes three necessary conditions for an optimal linear design: dynamic memory ability, static approximation ability, and minimal parameter usage. By addressing the shortcomings of existing models, none of which fully meet these conditions, MetaLA demonstrates superior performance across various tasks including Multi-Query Associative Recall, language modeling, and image classification. The results indicate that MetaLA is more effective than previous linear models while maintaining computational efficiency, thus paving the way for future developments in linear attention mechanisms within deep learning applications.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Neural Pfaffians: Solving Many Many-Electron Schrdinger Equations</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HRkniCWM3E&name=pdf" class="link-primary">https://openreview.net/attachment?id=HRkniCWM3E&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">neural wave functions</span>
<span class="badge bg-primary">many-electron systems</span>
<span class="badge bg-primary">Pfafans</span>
<span class="badge bg-primary">quantum chemistry</span>
<span class="badge bg-primary">computational efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Neural Pfafans (NeurPf), a novel approach for approximating many-electron Schrdinger equations using overparametrized, fully learnable neural wave functions. By utilizing Pfafans instead of traditional Slater determinants, NeurPf effectively enforces the required antisymmetry of electrons across various molecular structures without imposing discrete orbital selection constraints. The authors demonstrate that NeurPf achieves chemical accuracy in calculating ground state and ionization energies, surpassing existing methods while significantly reducing computational costs. Empirical evaluations on datasets like TinyMol show that NeurPf outperforms gold-standard reference methods, thus offering a promising solution for generalizing neural wave functions in quantum chemistry.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8qu52Fl1Dt&name=pdf" class="link-primary">https://openreview.net/attachment?id=8qu52Fl1Dt&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">fMRI</span>
<span class="badge bg-primary">video reconstruction</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">neuroimaging</span>
<span class="badge bg-primary">video generation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents NeuroClips, a novel framework designed to enhance fMRI-to-video reconstruction by addressing the challenges of decoding both high-level semantics and low-level perceptual details from brain activity. Unlike prior methods that focused primarily on static images, NeuroClips integrates a Semantics Reconstructor to generate high-quality keyframes and a Perception Reconstructor to capture low-level visual details, ensuring smooth and coherent video outputs. The framework employs a pre-trained text-to-video (T2V) diffusion model to synthesize videos based on these reconstructed components. Evaluations on a publicly available fMRI-video dataset demonstrate that NeuroClips significantly outperforms existing state-of-the-art models in metrics such as the Structural Similarity Index (SSIM) and spatiotemporal consistency, achieving high-fidelity video reconstruction of up to 6 seconds at 8 frames per second (FPS).</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Not All Tokens Are What You Need for Pretraining</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0NMzBwqaAJ&name=pdf" class="link-primary">https://openreview.net/attachment?id=0NMzBwqaAJ&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Selective Language Modeling</span>
<span class="badge bg-primary">Token Importance</span>
<span class="badge bg-primary">Language Model Training</span>
<span class="badge bg-primary">Performance Improvement</span>
<span class="badge bg-primary">Few-Shot Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces RHO-1, a novel language model that employs Selective Language Modeling (SLM) to improve the efficiency and effectiveness of pre-training by focusing on the most relevant tokens in a dataset. Traditional methods treat all tokens equally, but this study reveals that not all tokens contribute equally to learning, with certain tokens exhibiting distinct loss patterns during training. By scoring tokens with a reference model and selectively applying loss to those deemed useful, RHO-1 achieves notable performance gains, including up to a 30% improvement in few-shot accuracy across various math tasks while utilizing significantly fewer tokens than existing models. The results demonstrate that SLM enhances data efficiency and model performance, suggesting a paradigm shift in token-level training strategies for language models.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Optimal Parallelization of Boosting</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=rtz4df9IF1&name=pdf" class="link-primary">https://openreview.net/attachment?id=rtz4df9IF1&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Boosting</span>
<span class="badge bg-primary">Parallelization</span>
<span class="badge bg-primary">Weak-to-Strong Learning</span>
<span class="badge bg-primary">Algorithms</span>
<span class="badge bg-primary">Theoretical Bounds</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the parallel complexity of Boosting algorithms, which have traditionally been constrained by their sequential nature due to dependency between training rounds. The authors present improved lower bounds on the tradeoff between the number of training rounds (p) and the total parallel work per round (t) and introduce a parallel Boosting algorithm that achieves near-optimal performance across this tradeoff spectrum. Their results effectively close the gap between theoretical limits and practical performance of Boosting algorithms, confirming the feasibility of efficient parallelization in weak-to-strong learning contexts. They also establish a new theoretical framework for analyzing parallel Boosting, contributing significantly to both algorithm design and theoretical understanding in this field.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Ddak3nSqQM&name=pdf" class="link-primary">https://openreview.net/attachment?id=Ddak3nSqQM&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">policy learning</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">decision-making</span>
<span class="badge bg-primary">tutorial books</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces a novel approach called Policy Learning from Tutorial Books (PLfB), which leverages the knowledge contained in tutorial texts to train decision-making agents. Unlike traditional reinforcement learning methods that rely heavily on real-world interactions for skill acquisition, PLfB utilizes a three-stage framework inspired by human learning: Understanding, Rehearsing, and Introspecting (URI). The framework first extracts knowledge from tutorial books to create a knowledge database, then simulates decision-making processes based on this knowledge to generate an imaginary dataset. Finally, it distills a policy network from the dataset using offline reinforcement learning techniques. Experimental results demonstrate that agents trained using this approach outperform existing methods in both simple (Tic-Tac-Toe) and complex (Football) environments, highlighting the potential of integrating textual knowledge with reinforcement learning for effective policy training.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=YvA8UF0I37&name=pdf" class="link-primary">https://openreview.net/attachment?id=YvA8UF0I37&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LLM compression</span>
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">PV-Tuning</span>
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces PV-Tuning, a novel framework for fine-tuning large language models (LLMs) to achieve extreme compression (1-2 bits per parameter), which is essential for deploying these models on resource-constrained devices. The authors critique existing methods that rely heavily on straight-through estimators (STE) for quantization-aware fine-tuning, arguing that they can be suboptimal. Instead, they propose a representation-agnostic approach that systematically optimizes both continuous and discrete parameters, leading to significant improvements in accuracy over state-of-the-art techniques such as QuIP# and AQLM. Experimental results demonstrate that PV-Tuning achieves Pareto-optimal quantization for Llama-2 models at 2 bits per parameter, outperforming previous methods across various model sizes and benchmarks while maintaining compatibility with existing inference architectures. The findings highlight the potential for enhanced performance in LLMs through improved optimization strategies during quantization.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Questioning the Survey Responses of Large Language Models</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Oo7dlLgqQX&name=pdf" class="link-primary">https://openreview.net/attachment?id=Oo7dlLgqQX&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">survey responses</span>
<span class="badge bg-primary">biases</span>
<span class="badge bg-primary">American Community Survey</span>
<span class="badge bg-primary">evaluation methodology</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper critically examines the methodology of using survey responses from large language models (LLMs) to infer their alignment with human demographics and opinions, focusing on the American Community Survey (ACS). The authors evaluate 43 LLMs and identify systematic biases in their responses, such as ordering and labeling biases, which lead to uniformly random answer distributions regardless of model size or training data. After adjusting for these biases through randomized answer ordering, the responses of the models do not closely resemble those of the U.S. census population, suggesting that prior assumptions about LLMs' alignment with human subgroups may be misleading. The findings caution against interpreting LLM survey responses as accurate representations of human populations, emphasizing the need for robust methodologies in evaluating LLM biases.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qf2uZAdy1N&name=pdf" class="link-primary">https://openreview.net/attachment?id=qf2uZAdy1N&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">latent dynamics</span>
<span class="badge bg-primary">algorithmic modularity</span>
<span class="badge bg-primary">statistical modularity</span>
<span class="badge bg-primary">representation learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the challenges and solutions associated with reinforcement learning (RL) in environments characterized by complex, high-dimensional observations and relatively simple latent dynamics. It highlights the inadequacies of existing approaches under general latent dynamics and proposes a unified framework that introduces concepts of statistical and algorithmic modularity. The authors demonstrate that many established settings for RL become intractable when applied to rich observations, yet they identify conditions like latent pushforward coverability that enable statistical tractability. They also present new algorithms that efficiently reduce RL problems with latent dynamics to simpler observable scenarios. The findings contribute to a foundational understanding of RL under latent dynamics, paving the way for future research and applications.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Return of Unconditional Generation: A Self-supervised Representation Generation Method</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=clTa4JFBML&name=pdf" class="link-primary">https://openreview.net/attachment?id=clTa4JFBML&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">unconditional generation</span>
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">image synthesis</span>
<span class="badge bg-primary">generative models</span>
<span class="badge bg-primary">representation learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel framework called Representation-Conditioned Generation (RCG) aimed at improving the quality of unconditional image generation by leveraging self-supervised representations. Traditional unconditional generative models have struggled to match the performance of their conditional counterparts due to a lack of semantic information typically provided by human labels. RCG addresses this gap by utilizing a pre-trained self-supervised encoder to generate semantic representations, which are then used to condition an image generator. The authors demonstrate that RCG significantly enhances the quality of unconditional generation, achieving a state-of-the-art FID score of 2.15 on the ImageNet 256x256 dataset, thus bridging the performance gap between unconditional and conditional generation methods. The framework's effectiveness is validated through comprehensive experiments across multiple generative models, indicating its potential to utilize large-scale unlabeled data more efficiently.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=r5spnrY6H3&name=pdf" class="link-primary">https://openreview.net/attachment?id=r5spnrY6H3&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D Referring Expression Segmentation</span>
<span class="badge bg-primary">Spatial Awareness</span>
<span class="badge bg-primary">Rule-Guided Supervision</span>
<span class="badge bg-primary">Point Clouds</span>
<span class="badge bg-primary">Deep Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces the Rule-Guided Spatial Awareness Network (RG-SAN) for 3D Referring Expression Segmentation (3D-RES), addressing challenges such as over-segmentation and mis-segmentation by emphasizing spatial relationships among instances described in text. RG-SAN employs a Text-driven Localization Module (TLM) to iteratively refine positional information of entities and a Rule-guided Weak Supervision (RWS) strategy that utilizes dependency tree rules to enhance positioning accuracy. Extensive experiments on the ScanRefer benchmark demonstrate RG-SAN's superiority over existing methods, achieving a mean Intersection over Union (mIoU) increase of 5.1 points and improving robustness against spatial ambiguities, thus underscoring the importance of integrating spatial awareness into segmentation tasks.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LEzx6QRkRH&name=pdf" class="link-primary">https://openreview.net/attachment?id=LEzx6QRkRH&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Hierarchical Framework</span>
<span class="badge bg-primary">Code-as-Policy</span>
<span class="badge bg-primary">Task Decomposition</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents RL-GPT, a novel framework that combines Reinforcement Learning (RL) with Large Language Models (LLMs) to enhance the ability of agents in performing complex tasks in open-world environments like Minecraft. The framework features a two-level hierarchical structure comprising a slow agent for high-level task decomposition and a fast agent for executing coding tasks. This separation allows each agent to focus on specific roles, significantly improving efficiency compared to traditional methods. RL-GPT demonstrates superior performance in various tasks within the MineDojo environment, achieving effective learning and task execution, including locating diamonds quickly on limited computational resources. The integration of high-level coding actions into the RL action space is a key innovation, facilitating automatic task learning with minimal human input.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Scale Equivariant Graph Metanetworks</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8Fxqn1tZM1&name=pdf" class="link-primary">https://openreview.net/attachment?id=8Fxqn1tZM1&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">scaling symmetries</span>
<span class="badge bg-primary">metanetworks</span>
<span class="badge bg-primary">neural networks</span>
<span class="badge bg-primary">equivariance</span>
<span class="badge bg-primary">machine learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces Scale Equivariant Graph MetaNetworks (ScaleGMNs), a novel framework designed to learn higher-order functions by processing neural networks (NNs) while considering scaling symmetries alongside the established permutation symmetries. The authors demonstrate that traditional activation functions reveal additional scaling symmetries, which have been overlooked in metanetwork design. The proposed architecture adapts the message-passing paradigm of Graph MetaNetworks to account for these symmetries, ensuring that neuron and edge representations are equivariant to scalings. Theoretical proofs confirm that under certain conditions, ScaleGMN can replicate the forward and backward passes of any feedforward NN. Experimental results show that ScaleGMN outperforms existing benchmarks across various datasets and activation functions, underscoring the potential of scaling symmetries as a powerful inductive bias for NN processing.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mSaqxZVZW8&name=pdf" class="link-primary">https://openreview.net/attachment?id=mSaqxZVZW8&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">A* search</span>
<span class="badge bg-primary">Monte-Carlo tree search</span>
<span class="badge bg-primary">heuristic search</span>
<span class="badge bg-primary">selective sampling</span>
<span class="badge bg-primary">exploration-enhanced algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents SeeA, a novel algorithm designed to enhance the A* search method by incorporating exploration through selective sampling of open nodes. Traditional A* search tends to be inefficient when heuristic estimations are inaccurate, leading to local optima traps. SeeA addresses this limitation by dynamically constructing a candidate subset of open nodes, allowing for the selection of nodes that may not have the best heuristic values, thereby promoting exploration of alternative paths. The authors introduce three sampling strategies—uniform, clustering, and UCT-like sampling—and provide theoretical proof of SeeA's superior efficiency over A* when the heuristic function's accuracy is compromised. Experimental results across various domains, including retrosynthetic planning, logic synthesis, and the Sokoban game, demonstrate that SeeA significantly outperforms state-of-the-art heuristic search algorithms, achieving high success rates and maintaining low node expansion.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pGEY8JQ3qx&name=pdf" class="link-primary">https://openreview.net/attachment?id=pGEY8JQ3qx&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Markov Decision Processes</span>
<span class="badge bg-primary">Sample Complexity</span>
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Average Reward</span>
<span class="badge bg-primary">Weakly Communicating MDPs</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates sample complexity in learning optimal policies for average-reward Markov Decision Processes (MDPs) under a generative model, focusing on weakly communicating MDPs. It establishes a sample complexity bound of \(eO(SAH^{2/\epsilon})\) for weakly communicating MDPs, where \(H\) denotes the span of the optimal policy's bias function, and \(S\) is the state-action space size. This result is minimax optimal up to logarithmic factors, improving on existing methods that either assume bounded mixing times or have suboptimal parameter dependencies. Furthermore, the authors extend the analysis to general average-reward MDPs, introducing a transient time parameter \(B\) and deriving a sample complexity bound of \(eO(SAB + H^{2/\epsilon})\), which also matches a corresponding lower bound. The findings enhance understanding of the relationship between average and discounted reward settings while presenting a quadratic rather than cubic dependence on the effective horizon for fixed MDP instances.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Statistical Efficiency of Distributional Temporal Difference Learning</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=eWUM5hRYgH&name=pdf" class="link-primary">https://openreview.net/attachment?id=eWUM5hRYgH&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">distributional reinforcement learning</span>
<span class="badge bg-primary">temporal difference learning</span>
<span class="badge bg-primary">statistical efficiency</span>
<span class="badge bg-primary">finite-sample analysis</span>
<span class="badge bg-primary">Wasserstein distance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the statistical efficiency of distributional temporal difference learning (TD) in reinforcement learning, extending previous asymptotic convergence results to finite-sample performance. It introduces non-parametric distributional TD (NTD) to analyze finite-sample complexity, proving that achieving an ε-optimal estimator requires O(1/(2p(1)^(2p+1))) iterations, which is minimax optimal up to logarithmic factors. The authors also revisit categorical temporal difference learning (CTD), demonstrating that it shares similar non-asymptotic convergence bounds in terms of the p-Wasserstein distance. A key contribution is the establishment of a new Freedman's inequality in Hilbert spaces, which is crucial for their theoretical analysis and holds independent significance beyond this study.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=J2wI2rCG2u&name=pdf" class="link-primary">https://openreview.net/attachment?id=J2wI2rCG2u&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Stochastic estimation</span>
<span class="badge bg-primary">Automatic differentiation</span>
<span class="badge bg-primary">Physics-informed neural networks</span>
<span class="badge bg-primary">High-dimensional PDEs</span>
<span class="badge bg-primary">Optimization algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents the Stochastic Taylor Derivative Estimator (STDE), a novel method for efficiently estimating arbitrary differential operators in the context of optimizing neural networks, particularly those involving high-dimensional and high-order derivatives. By employing a tailored approach using high-order automatic differentiation and randomization, STDE addresses the computational challenges associated with both the dimensionality and the order of derivatives. The authors demonstrate that STDE significantly accelerates the training of Physics-Informed Neural Networks (PINNs) by achieving over 1000 times speed-up and more than 30% reduction in memory usage compared to traditional methods. Experimental results indicate that STDE enables the effective resolution of complex, high-dimensional partial differential equations (PDEs), thus broadening the applicability of neural networks in scientific computing.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Stylus: Automatic Adapter Selection for Diffusion Models</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3Odq2tGSpp&name=pdf" class="link-primary">https://openreview.net/attachment?id=3Odq2tGSpp&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adapter selection</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">image generation</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">visual fidelity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents "Stylus," an innovative system designed for automatic adapter selection in diffusion models, particularly focused on enhancing image generation quality. As the use of fine-tuned adapters (like Low-Rank Adaptation, LoRA) grows, the challenge lies in efficiently selecting and composing relevant adapters based on user prompts, which often imply multiple specific tasks. Stylus employs a three-stage approach involving a refiner to improve adapter descriptions, a retriever to fetch relevant adapters, and a composer to assign these adapters to the segmented tasks of a prompt. The evaluation of Stylus on a curated dataset of over 75,000 adapters demonstrates significant improvements in visual fidelity, image diversity, and textual alignment compared to existing methods, showcasing its practical application in generating high-quality images from complex prompts.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">The Road Less Scheduled</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0XeNkkENuI&name=pdf" class="link-primary">https://openreview.net/attachment?id=0XeNkkENuI&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">learning rate</span>
<span class="badge bg-primary">schedule-free</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">momentum</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel optimization approach called Schedule-Free, which eliminates the need for predefined learning rate schedules, traditionally essential in various machine learning tasks. The authors argue that existing methods relying on such schedules often underperform, and they introduce a new framework that integrates concepts from iterate averaging and scheduling without requiring additional hyperparameters. The Schedule-Free method demonstrates state-of-the-art performance across a broad range of problems, including deep learning and convex optimization, while simplifying the optimization process. Experimental results show that Schedule-Free approaches not only match but frequently surpass the outcomes of conventional learning rate schedules, highlighting their practical advantages and theoretical robustness.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">The Sample-Communication Complexity Trade-off in Federated Q-Learning</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6YIpvnkjUK&name=pdf" class="link-primary">https://openreview.net/attachment?id=6YIpvnkjUK&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Federated Learning</span>
<span class="badge bg-primary">Q-learning</span>
<span class="badge bg-primary">Communication Complexity</span>
<span class="badge bg-primary">Sample Complexity</span>
<span class="badge bg-primary">Reinforcement Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the trade-off between sample and communication complexity in Federated Q-learning, where multiple agents collaboratively learn an optimal Q-function for a common Markov Decision Process (MDP). The authors establish a lower bound on the communication cost required for achieving statistical benefits from collaboration, demonstrating that any Federated Q-learning algorithm achieving linear speedup in sample complexity must incur significant communication costs. They introduce a novel algorithm, Fed-DVR-Q, which simultaneously achieves optimal sample and communication complexities, marking a significant advancement in understanding the sample-communication complexity trade-off in Federated Q-learning. The findings provide insights into algorithm design and the inherent limitations of federated approaches in reinforcement learning contexts.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=REIK4SZMJt&name=pdf" class="link-primary">https://openreview.net/attachment?id=REIK4SZMJt&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">hippocampus</span>
<span class="badge bg-primary">place cells</span>
<span class="badge bg-primary">contextual capacity</span>
<span class="badge bg-primary">remapping</span>
<span class="badge bg-primary">geometric modeling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates how the encoding of spatial context in the hippocampus, specifically through place cells, is influenced by the geometric properties of neural firing patterns. It introduces a model demonstrating that the hippocampal system’s capacity for storing distinct contexts grows exponentially with the number of place cells, while also revealing a fundamental trade-off between the resolution of spatial encoding and the number of contexts that can be distinguished. The findings suggest that wider place fields enhance contextual segregation but reduce spatial specificity, and that clustering of place cells near boundaries of environments can further improve context discrimination. This research provides insights into the mechanisms underlying spatial navigation and memory formation in the hippocampus, with implications for understanding episodic memory and cognitive mapping.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pC44UMwy2v&name=pdf" class="link-primary">https://openreview.net/attachment?id=pC44UMwy2v&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Chain-of-Thought</span>
<span class="badge bg-primary">Reasoning Boundary Framework</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Optimization Strategies</span>
<span class="badge bg-primary">Quantitative Metrics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces a novel Reasoning Boundary Framework (RBF) aimed at quantifying and optimizing Chain-of-Thought (CoT) reasoning for large language models (LLMs). The authors identify two significant challenges in existing CoT research: the absence of quantitative metrics to assess CoT capabilities and insufficient guidance for optimizing CoT performance. To address these issues, they define reasoning boundaries (RBs) to quantify the upper limits of task-specific reasoning complexity and propose a combination law for RBs that can be applied across various real-world tasks. Additionally, they categorize RBs into three intervals for optimization purposes and validate their framework through extensive experiments involving 27 models across five tasks, demonstrating its effectiveness in enhancing model performance. The work aims to provide a comprehensive understanding of reasoning boundaries and optimization strategies for LLMs, contributing to advancements in CoT methodologies.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=5zSCSE0k41&name=pdf" class="link-primary">https://openreview.net/attachment?id=5zSCSE0k41&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">lifelike talking faces</span>
<span class="badge bg-primary">audio-driven generation</span>
<span class="badge bg-primary">facial dynamics</span>
<span class="badge bg-primary">real-time video synthesis</span>
<span class="badge bg-primary">AI avatars</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents V ASA-1, a novel framework for generating lifelike talking faces in real-time using a single static image and a speech audio clip. This model excels in synchronizing lip movements with audio while also capturing a wide range of facial expressions and natural head movements, enhancing the sense of realism and liveliness. Central to its innovation is a diffusion-based model that operates in a uniquely expressive and disentangled facial latent space, allowing for comprehensive facial dynamics generation. Extensive experiments indicate that V ASA-1 significantly outperforms existing methods in various metrics, achieving high video quality and operational efficiency of up to 40 frames per second. This advancement holds promise for applications in digital communication, education, and healthcare, as it enables more natural interactions with AI avatars.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gojL67CfS8&name=pdf" class="link-primary">https://openreview.net/attachment?id=gojL67CfS8&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">image generation</span>
<span class="badge bg-primary">autoregressive modeling</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">visual transformers</span>
<span class="badge bg-primary">zero-shot generalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel approach called Visual AutoRegressive (V AR) modeling, which redefines autoregressive image generation as a next-scale prediction process, diverging from conventional raster-scan methods. This innovative methodology enables V AR to outperform existing image generation models, including diffusion transformers, in terms of image quality, data efficiency, and inference speed, achieving significant improvements on benchmarks like ImageNet. The authors demonstrate that V AR models exhibit strong scaling laws akin to those seen in large language models (LLMs) and highlight their capability for zero-shot generalization in various downstream tasks such as in-painting and editing. The findings suggest that integrating LLM techniques into computer vision could advance the development of multi-modal intelligence. The authors provide open-source code and models to encourage further exploration of autoregressive methods in visual generation.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9O2sVnEHor&name=pdf" class="link-primary">https://openreview.net/attachment?id=9O2sVnEHor&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Graph Neural Networks</span>
<span class="badge bg-primary">Weisfeiler-Leman Test</span>
<span class="badge bg-primary">Graph Isomorphism</span>
<span class="badge bg-primary">Cycle Counting</span>
<span class="badge bg-primary">Homomorphism Counting</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces the r-loopy Weisfeiler-Leman (r-`WL) test, a new hierarchy for graph isomorphism testing, and its associated graph neural network (GNN) architecture, r-`MPNN, which enhances the ability to count cycles up to length r+2. The authors demonstrate that r-`WL can count homomorphisms of cactus graphs, surpassing the capabilities of the existing 1-WL and k-WL tests while maintaining a more efficient computational complexity suitable for sparse graphs. Empirical evaluations on synthetic and real-world datasets validate the expressive power and scalability of r-`MPNN compared to traditional models, making it a significant advancement in graph representation learning. The code is made publicly available on GitHub for reproducibility.</p>
</div>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title">You Only Cache Once: Decoder-Decoder Architectures for Language Models</h3>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=25Ioxw576r&name=pdf" class="link-primary">https://openreview.net/attachment?id=25Ioxw576r&name=pdf</a></p>
<div class="mb-3">
<h3 class="h5">Topics</h3>
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">decoder-decoder architecture</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">long-context modeling</span>
<span class="badge bg-primary">GPU memory efficiency</span>
<span class="badge bg-primary">inference speedup</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces YOCO, a novel decoder-decoder architecture designed for large language models that only caches key-value pairs once, significantly reducing GPU memory usage while maintaining global attention capabilities. YOCO comprises a self-decoder that efficiently encodes global key-value caches reused by a cross-decoder through cross-attention, allowing for improved inference efficiency and reduced prefill latency. Experimental results show that YOCO outperforms traditional Transformer architectures across various model sizes and context lengths, achieving competitive performance while scaling up to 1M tokens and demonstrating substantial improvements in memory footprint, throughput, and serving capacity.</p>
</div>
</div>
</div>
</div>

        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
