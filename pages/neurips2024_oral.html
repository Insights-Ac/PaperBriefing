
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - NeurIPS 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">NeurIPS 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated on 2024-11-21 15:44:24 by <a href="https://github.com/Logan-Lin/PubSummarizer">PubSummarizer</a></em></p>
        <div class="row" data-masonry='{"percentPosition": true }'>
            <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">clustering</span>
<span class="badge bg-primary">Gaussian Mixture Models</span>
<span class="badge bg-primary">anisotropic covariance</span>
<span class="badge bg-primary">Lloyd's algorithm</span>
<span class="badge bg-primary">minimax bounds</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents new theoretical and algorithmic advancements in clustering using Gaussian Mixture Models with anisotropic covariance structures, achieving minimax optimality through an adjusted version of Lloyd's algorithm.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores clustering under anisotropic Gaussian Mixture Models (GMMs) where clusters have unknown and non-identical covariance matrices, addressing two scenarios: homogeneous and heterogeneous covariance. The authors derive minimax lower bounds for clustering performance that highlight the significant role of covariance structures compared to isotropic cases. They introduce a variant of Lloyd's algorithm that iteratively estimates both cluster centers and covariance matrices, demonstrating through theoretical proofs that this adjusted algorithm converges optimally within logarithmic iterations. The paper includes numerical comparisons to established methods, showing improved performance of the proposed algorithm under various settings, thus bridging theoretical optimality and practical computational efficiency in clustering tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ge8GZn8Gtu&name=pdf" class="link-primary">https://openreview.net/attachment?id=ge8GZn8Gtu&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Aligner: Efficient Alignment by Learning to Correct</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">alignment</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">efficiency</span>
<span class="badge bg-primary">residual learning</span>
<span class="badge bg-primary">human feedback</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents Aligner, a novel lightweight and model-agnostic alignment method that significantly improves the performance of large language models (LLMs) through efficient residual learning based on preferred and dispreferred responses.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Aligner, a new paradigm for aligning large language models (LLMs) with human intentions by learning correctional residuals between preferred and unpreferred answers without extensive computational resources or complex frameworks like reinforcement learning. Aligner is designed to be a plug-and-play module that can be easily integrated into various existing LLMs, improving metrics of helpfulness, harmlessness, and honesty across multiple models. Experimental results demonstrate significant performance enhancements, with Aligner -7B improving the helpfulness and harmlessness of LLMs such as GPT-4 by substantial percentages. The findings imply that Aligner can facilitate the rapid evolution of safety standards in language models, making advanced alignment methods more accessible and resource-efficient.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=kq166jACVP&name=pdf" class="link-primary">https://openreview.net/attachment?id=kq166jACVP&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Bayesian-guided Label Mapping for Visual Reprogramming</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Visual Reprogramming</span>
<span class="badge bg-primary">Label Mapping</span>
<span class="badge bg-primary">Bayesian Inference</span>
<span class="badge bg-primary">Pretrained Models</span>
<span class="badge bg-primary">Downstream Tasks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Bayesian-guided Label Mapping (BLM) for visual reprogramming, enhancing label correspondence between pretrained and downstream tasks to achieve superior classification performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of existing label mapping methods in visual reprogramming (VR), which transform outputs of pretrained models to align with distinct downstream label spaces. The authors propose a Bayesian-guided Label Mapping (BLM) method that creates a probabilistic label mapping matrix, capturing the complex relationships between pretrained and downstream labels through Bayesian conditional probabilities. This approach allows for flexible many-to-many mappings, as demonstrated through extensive experiments on various datasets using pretrained vision models and vision-language models. The results indicate that BLM and its extension, BLM+, significantly outperform traditional one-to-one mapping strategies, providing both improved accuracy and insightful interpretations of label connections, thus highlighting the effectiveness of the proposed methods in leveraging pretrained model knowledge for diverse tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=135eKqDoRR&name=pdf" class="link-primary">https://openreview.net/attachment?id=135eKqDoRR&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">multimodal LLMs</span>
<span class="badge bg-primary">vision-centric design</span>
<span class="badge bg-primary">visual representation learning</span>
<span class="badge bg-primary">instruction tuning</span>
<span class="badge bg-primary">benchmark evaluation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Cambrian-1, a family of multimodal large language models that achieve state-of-the-art performance through a vision-centric approach, incorporating innovative techniques for visual representation learning and a new vision-centric benchmark called CV-Bench.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Cambrian-1, a set of multimodal large language models (MLLMs) designed to address limitations in current visual representation methods by employing a vision-centric approach. The authors evaluate over 20 vision encoders, proposing the Spatial Vision Aggregator (SVA) to effectively integrate visual features with LLMs while reducing token counts. They also introduce a new benchmark, CV-Bench, to assess the visual grounding capabilities of MLLMs more accurately. The study finds that careful design of visual components and instruction-tuning data significantly enhances model performance, achieving superior results on various multimodal tasks. The authors provide comprehensive resources, including model weights, code, datasets, and tuning strategies, which aim to foster further research in multimodal systems and visual representation learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Vi8AepAXGy&name=pdf" class="link-primary">https://openreview.net/attachment?id=Vi8AepAXGy&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">CAT3D: Create Anything in 3D with Multi-View Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D Reconstruction</span>
<span class="badge bg-primary">Multi-View Diffusion Models</span>
<span class="badge bg-primary">Text-to-Image</span>
<span class="badge bg-primary">Novel View Synthesis</span>
<span class="badge bg-primary">Photogrammetry</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">CAT3D is a novel method that enables efficient 3D scene creation from one or more images using a multi-view diffusion model, significantly reducing the time and input requirements compared to existing techniques.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents CAT3D, a method for creating 3D scenes from various input images by simulating the image capture process through a multi-view diffusion model. CAT3D transforms the key challenge of insufficient observations into a generation problem by creating consistent novel views of a scene from any number of input images, which can then be processed by robust 3D reconstruction techniques. The model showcases rapid production of high-quality 3D representations, achieving results in as little as one minute while outperforming current benchmarks for single-image and few-view reconstructions. The authors validate CAT3D across a range of input scenarios, highlighting its advantages in efficiency and visualization quality despite certain limitations in handling diverse camera intrinsics and achieving consistent view generation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=TFZlFRl9Ks&name=pdf" class="link-primary">https://openreview.net/attachment?id=TFZlFRl9Ks&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Convolutional Differentiable Logic Gate Networks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">differentiable logic gates</span>
<span class="badge bg-primary">convolutional networks</span>
<span class="badge bg-primary">efficient inference</span>
<span class="badge bg-primary">CIFAR-10</span>
<span class="badge bg-primary">hardware optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces convolutional differentiable logic gate networks that enhance efficiency in inference while achieving superior accuracy on the CIFAR-10 dataset with significantly fewer logic gates than state-of-the-art approaches.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents convolutional differentiable logic gate networks (LogicTreeNet) as a novel approach to fast and efficient machine learning inference, building upon previous work in differentiable logic gate networks to incorporate deep logic gate tree convolutions, logical OR pooling, and residual initializations. The proposed architecture demonstrates the ability to learn effective representations that capture spatial patterns in image data while minimizing the number of logic gates required for achieving competitive accuracy, reporting an accuracy of 86.29% on the CIFAR-10 dataset using only 61 million gates. This improves upon the state-of-the-art results, reducing the gate count by a factor of 29. The efficiency gains are further validated through hardware implementation, achieving drastically faster inference times on FPGA, highlighting the practical applicability of this method for real-time and embedded applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4bKEFyUHT4&name=pdf" class="link-primary">https://openreview.net/attachment?id=4bKEFyUHT4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">humor</span>
<span class="badge bg-primary">comics</span>
<span class="badge bg-primary">juxtaposition</span>
<span class="badge bg-primary">AI comprehension</span>
<span class="badge bg-primary">Y ESBUT benchmark</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Y ESBUT benchmark to assess AI models' understanding of humor in comics through juxtaposed contradictory narratives, revealing significant gaps in their comprehension compared to human performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The research investigates the challenges that large multimodal language models face in understanding human humor expressed through comics featuring juxtaposed narratives that create humorous contradictions. It introduces the Y ESBUT benchmark, comprising tasks that range from basic literal comprehension to deeper narrative reasoning, enabling a comprehensive evaluation of AI's capability to interpret these comics. Experiments conducted on various state-of-the-art vision-language models indicate that while commercial models generally outperform open-source alternatives, they still lag significantly behind human performance in grasping the nuances of comic narratives. The findings highlight the limitations of current AI models and suggest areas for improving their semantic understanding of human creative expressions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bCMpdaQCNW&name=pdf" class="link-primary">https://openreview.net/attachment?id=bCMpdaQCNW&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Federated Learning</span>
<span class="badge bg-primary">Domain Adaptation</span>
<span class="badge bg-primary">Model Pruning</span>
<span class="badge bg-primary">Edge Devices</span>
<span class="badge bg-primary">System Heterogeneity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents DapperFL, a federated learning framework that integrates Model Fusion Pruning and Domain Adaptive Regularization to enhance model performance and reduce resource consumption on heterogeneous edge devices facing domain shifts.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">DapperFL addresses the challenges of system heterogeneity and domain shifts in federated learning environments by introducing a Model Fusion Pruning (MFP) module, which optimizes local models for edge devices by incorporating both local and global knowledge, and a Domain Adaptive Regularization (DAR) module, which encourages robust representation learning across domains. The framework demonstrates superior performance, achieving improved accuracy on benchmark datasets compared to state-of-the-art federated learning frameworks, while also significantly reducing model size by up to 80%. Experimental results validate the efficacy of DapperFL in heterogeneous settings, making it a promising solution for real-world applications in edge computing.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Pezt0xttae&name=pdf" class="link-primary">https://openreview.net/attachment?id=Pezt0xttae&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reasoning framework</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">Decomposing</span>
<span class="badge bg-primary">logical coherence</span>
<span class="badge bg-primary">error correction.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces DeAR (Decompose-Analyze-Rethink), a framework that enhances reasoning capabilities in large language models by systematically decomposing questions, analyzing sub-questions, and iteratively rethinking rationales for greater accuracy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents DeAR, a novel reasoning framework designed for complex problem-solving in large language models (LLMs) such as GPT-3.5 and LLaMA2. DeAR utilizes a top-down reasoning process that decomposes intricate questions into simpler sub-questions, followed by generating and self-checking rationales, and revising previous rationales based on new insights. Experiments conducted on reasoning benchmarks like ScienceQA, StrategyQA, and GSM8K show that DeAR significantly outperforms existing methods, including Tree-of-Thoughts and Graph-of-Thoughts, in terms of logical coherence and accuracy while maintaining superior efficiency in reasoning tasks. The findings indicate that DeAR effectively reduces errors and enhances the interpretability of the reasoning process, suggesting substantial implications for advancing artificial intelligence in reasoning and cognitive tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=NPKZF1WDjZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=NPKZF1WDjZ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DenoiseRep: Denoising Model for Representation Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Denoising</span>
<span class="badge bg-primary">Representation Learning</span>
<span class="badge bg-primary">Discriminative Models</span>
<span class="badge bg-primary">Feature Extraction</span>
<span class="badge bg-primary">Person Re-Identification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces DenoiseRep, a novel model that integrates denoising processes into representation learning to enhance feature discrimination in various vision tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents DenoiseRep, a method that innovatively combines denoising techniques with representation learning, particularly in discriminative tasks such as image classification and person re-identification. By treating each embedding layer in neural network architectures as a denoising layer, DenoiseRep allows for recursive denoising of features while maintaining computational efficiency through a parameter fusion algorithm, eliminating additional latency. Extensive experiments across multiple datasets, including person re-identification and large-scale image classification, demonstrate that DenoiseRep significantly improves feature discrimination and model performance, proving effective in both label-free and label-augmented scenarios. The results indicate the strong generalization capability of DenoiseRep across various architectures, including CNNs and Transformers.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=OycU0bAus6&name=pdf" class="link-primary">https://openreview.net/attachment?id=OycU0bAus6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">code generation</span>
<span class="badge bg-primary">divide-and-conquer</span>
<span class="badge bg-primary">functional consensus</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">programming tasks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">FUNCODER is a novel code generation framework that combines a divide-and-conquer strategy with functional consensus, significantly improving the performance of programming tasks in complex requirements.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces FUNCODER, a code generation framework designed to enhance the capabilities of large language models (LLMs) in managing complex programming tasks. FUNCODER employs a divide-and-conquer strategy to break down problems into smaller sub-functions, facilitating iterative coding and reducing complexity. It incorporates a functional consensus mechanism that evaluates multiple function implementations to select the most accurate one, thereby minimizing error propagation. Experiments demonstrate that FUNCODER outperforms state-of-the-art methods across various benchmarks, yielding a notable average improvement in problem-solving tasks and showcasing its robustness even with smaller models. The findings suggest that FUNCODER effectively addresses challenges in code generation, making it a promising tool for future advancements in programming assistance.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cFqAANINgW&name=pdf" class="link-primary">https://openreview.net/attachment?id=cFqAANINgW&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Do Finetti: On Causal Effects for Exchangeable Data</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Causality</span>
<span class="badge bg-primary">Exchangeable Data</span>
<span class="badge bg-primary">ICM Generative Processes</span>
<span class="badge bg-primary">Causal Effect Estimation</span>
<span class="badge bg-primary">Do-Finetti Algorithm</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a framework for estimating causal effects in exchangeable data by developing a Do-Finetti algorithm that addresses causal discovery and effect estimation simultaneously.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores causal effect estimation in contexts where data is not independent and identically distributed (i.i.d.) but rather exchangeable, underpinned by independent causal mechanisms (ICM). It introduces a generalized framework for handling causal inference in ICM generative processes, providing a truncated factorization formula that enables the identification and estimation of causal effects. The authors illustrate their methods using a causal Plya urn model, demonstrating how interventional effects propagate in an exchangeable setting. The paper culminates in the development of the Do-Finetti algorithm, which allows for the simultaneous identification of causal graphs and estimation of causal effects from multi-environment data, demonstrating improved performance compared to traditional i.i.d. methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4rCZeCZAON&name=pdf" class="link-primary">https://openreview.net/attachment?id=4rCZeCZAON&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">outliers</span>
<span class="badge bg-primary">rotation</span>
<span class="badge bg-primary">permutation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces DuQuant, an innovative method for quantizing large language models that effectively redistributes both normal and massive activations outliers, resulting in better performance in low-bit quantization scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses significant challenges in quantizing large language models (LLMs) due to activation outliers that hinder efficient low-bit representation. It presents DuQuant, a novel approach that employs rotation and permutation transformations to redistribute outlier activations among channels, thus mitigating their impact. The method consists of a greedy algorithm to construct a rotation matrix for local outlier redistribution and a zigzag permutation to balance outlier distributions globally. Experimental results demonstrate that DuQuant significantly outperforms state-of-the-art quantization techniques across various LLMs and tasks, achieving improvements in perplexity and accuracy while reducing memory usage and improving inference speed. The findings highlight DuQuant's effectiveness in enhancing the deployment of quantized LLMs in resource-constrained environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mp8u2Pcmqz&name=pdf" class="link-primary">https://openreview.net/attachment?id=mp8u2Pcmqz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Multimodal Fusion</span>
<span class="badge bg-primary">Object Detection</span>
<span class="badge bg-primary">End-to-End Learning</span>
<span class="badge bg-primary">Gradient Matrix Task-Alignment</span>
<span class="badge bg-primary">Autonomous Driving</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">E2E-MFD introduces a novel end-to-end framework for multimodal fusion detection that efficiently integrates image fusion and object detection through synchronous optimization, achieving superior performance on public datasets.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents E2E-MFD, an innovative end-to-end algorithm aimed at advancing multimodal image fusion and object detection, predominantly for applications in autonomous driving. The method simplifies the traditionally complex training processes associated with multimodal fusion, allowing for synchronous joint optimization that enhances results across both tasks. E2E-MFD employs an Object-Region-Pixel Phylogenetic Tree for effective feature extraction and a Coarse-to-Fine Diffusion Process for object detection, alongside a novel Gradient Matrix Task-Alignment to ensure an optimal parameter balance. Extensive experiments on various public datasets demonstrate that E2E-MFD significantly outperforms state-of-the-art approaches, showcasing improved image fusion quality and detection accuracy, thus highlighting its potential for practical applications in challenging environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=47loYmzxep&name=pdf" class="link-primary">https://openreview.net/attachment?id=47loYmzxep&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Enhancing Preference-based Linear Bandits via Human Response Time</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Preference learning</span>
<span class="badge bg-primary">linear bandits</span>
<span class="badge bg-primary">human response time</span>
<span class="badge bg-primary">utility estimation</span>
<span class="badge bg-primary">EZ-diffusion model</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper proposes a method to enhance preference-based linear bandit algorithms by incorporating human response times to improve utility estimations from binary choice data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of traditional interactive preference learning systems that rely solely on human binary choices, which do not provide enough information on preference strength. It introduces an efficient method utilizing human response times, which correlate inversely with preference strength, alongside choices to derive a more informative linear utility estimator based on the EZ-diffusion model. Through theoretical and empirical comparisons, the authors demonstrate that incorporating response times significantly accelerates preference learning and reduces identification errors in fixed-budget best-arm identification scenarios. The results, validated through simulations on real-world datasets, highlight the potential for improved user preference elicitation in applications such as recommender systems and assistive technologies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aIPwlkdOut&name=pdf" class="link-primary">https://openreview.net/attachment?id=aIPwlkdOut&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Graph Contrastive Learning</span>
<span class="badge bg-primary">Representation Scattering</span>
<span class="badge bg-primary">Scattering Graph Representation Learning</span>
<span class="badge bg-primary">Topology-based Constraint Mechanism</span>
<span class="badge bg-primary">GNN</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Scattering Graph Representation Learning (SGRL), a novel framework for graph contrastive learning that leverages a representation scattering mechanism to enhance the performance of GNNs without the need for manual annotations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the common mechanism of representation scattering among three established frameworks in Graph Contrastive Learning (GCL): node discrimination, group discrimination, and bootstrapping schemes. It presents Scattering Graph Representation Learning (SGRL), which utilizes a Representation Scattering Mechanism (RSM) to encourage diverse node representations and a Topology-based Constraint Mechanism (TCM) to account for the interconnected nature of graphs. Extensive experiments demonstrate that SGRL outperforms existing methods across multiple downstream tasks on various benchmark datasets, confirming the significance of representation scattering for enhancing generalization and robustness in GNN training. The findings provide a structured framework for advancing graph representation learning, with implications for broader applications in fields requiring efficient processing of graph data.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=R8SolCx62K&name=pdf" class="link-primary">https://openreview.net/attachment?id=R8SolCx62K&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Generalized Category Discovery</span>
<span class="badge bg-primary">Teacher-Student Framework</span>
<span class="badge bg-primary">Attention Alignment</span>
<span class="badge bg-primary">Semi-Supervised Learning</span>
<span class="badge bg-primary">Energy Perspective</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces FlipClass, an innovative method for improving learning synchronization in generalized category discovery by dynamically aligning teacher and student attention.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of traditional teacher-student frameworks in open-world generalized category discovery (GCD), particularly the issues arising from inconsistent learning between teacher and student due to the introduction of new classes. It proposes FlipClass, a method that dynamically adapts teacher attention based on student feedback to ensure synchronized learning, thus enhancing representation consistency across both old and new classes. Experimental results demonstrate that FlipClass significantly outperforms state-of-the-art methods on various benchmarks, establishing new standards in the field and highlighting its potential for improving model adaptability in real-world settings.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=C4NbtYnyQg&name=pdf" class="link-primary">https://openreview.net/attachment?id=C4NbtYnyQg&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">recommender systems</span>
<span class="badge bg-primary">generalization error</span>
<span class="badge bg-primary">tree structure</span>
<span class="badge bg-primary">theoretical analysis</span>
<span class="badge bg-primary">empirical validation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper analyzes the generalization error bounds of two-stage recommender systems with tree structures, demonstrating that increased branches in the retriever and harmonized distributions between training and inference can enhance generalization performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the generalization error of two-stage recommender systems that incorporate tree structures, consisting of an efficient retriever and a more precise ranker. Using an error decomposition framework and Rademacher complexity, the authors derive upper bounds for the generalization error of various tree-based retrievers and ranker models under shifted training distributions. They demonstrate that increasing the number of branches in the tree structure improves generalization capabilities and that aligning training and inference distributions enhances model performance. Theoretical insights are supported by empirical experiments on real-world datasets, reinforcing the findings and highlighting strategies for optimizing two-stage recommender systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=m1a4CrRJR7&name=pdf" class="link-primary">https://openreview.net/attachment?id=m1a4CrRJR7&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Spatiotemporal learning</span>
<span class="badge bg-primary">Multi-task learning</span>
<span class="badge bg-primary">Urban intelligence</span>
<span class="badge bg-primary">Continuous learning</span>
<span class="badge bg-primary">Data integration</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a Continuous Multi-task Spatio-Temporal learning framework (CMuST) that enhances urban intelligence by integrating multiple tasks and data sources to overcome limitations of traditional task-specific models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents CMuST, a framework designed to address the shortcomings of traditional spatiotemporal learning models that focus on isolated tasks, which often fail to generalize across dynamic urban environments. By utilizing a Multi-dimensional Spatio-Temporal Interaction Network (MSTI) that facilitates interactions between various data types and a Rolling Adaptation (RoAda) training scheme, CMuST effectively models relationships across multiple urban tasks, ensuring continuous learning and efficient adaptation. The authors empirically validate the effectiveness of CMuST through extensive experiments on datasets from three cities, showing significant performance improvements in prediction accuracy on both mainstream and new tasks. The findings highlight the framework's potential in fostering collective intelligence within urban systems, making it a significant contribution to the field of spatiotemporal learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tnh4LK72yj&name=pdf" class="link-primary">https://openreview.net/attachment?id=tnh4LK72yj&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Gaussian representation</span>
<span class="badge bg-primary">physical property estimation</span>
<span class="badge bg-primary">dynamic reconstruction</span>
<span class="badge bg-primary">simulation</span>
<span class="badge bg-primary">object identification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel hybrid framework leveraging 3D Gaussian representations for accurate estimation of physical properties through visual observations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces a hybrid framework called the Gaussian-Informed Continuum (GIC) that addresses the challenge of estimating physical properties of objects using visual observations. The proposed method incorporates a dynamic 3D Gaussian network for precise scene reconstruction and utilizes a coarse-to-fine filling strategy to generate density fields representing object shapes. By integrating 2D shape masks during simulations, the GIC effectively enhances the physical property estimation process. Extensive experiments demonstrate that the GIC outperforms existing methods across various benchmarks and applications, including real-world scenarios like robotic manipulation, thereby showcasing its practical utility and robustness in system identification tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=SSCtCq2MH2&name=pdf" class="link-primary">https://openreview.net/attachment?id=SSCtCq2MH2&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Graph Diffusion Transformers for Multi-Conditional Molecular Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">molecular generation</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">multi-conditional design</span>
<span class="badge bg-primary">Graph Diffusion Transformer</span>
<span class="badge bg-primary">polymer design</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Graph Diffusion Transformer (Graph DiT), an innovative model for multi-conditional molecular generation that effectively integrates numerous property constraints and demonstrates superior performance in generating polymers and small molecules.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the Graph Diffusion Transformer (Graph DiT), designed to enhance inverse molecular design through multi-conditional constraints, which is particularly useful in drug and material discovery. Graph DiT employs a Transformer-based denoising approach that combines a condition encoder for learning numerical and categorical property representations with a unique graph-dependent noise model to accurately estimate noise in molecular graphs. Extensive validation demonstrates Graph DiT's superiority over existing methods across various metrics for molecular property control and generative accuracy. The model's effectiveness in polymer design for gas separation tasks, supported by domain expert feedback, underscores its practical applicability and promise in advancing molecular design strategies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cfrDLD1wfO&name=pdf" class="link-primary">https://openreview.net/attachment?id=cfrDLD1wfO&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Guiding a Diffusion Model with a Bad Version of Itself</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">image generation</span>
<span class="badge bg-primary">classifier-free guidance</span>
<span class="badge bg-primary">autoguidance</span>
<span class="badge bg-primary">conditional sampling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel method called autoguidance that effectively utilizes a smaller, less-trained version of a diffusion model to enhance image quality while maintaining diversity in generated images.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses challenges in image-generating diffusion models, particularly in achieving optimal image quality and alignment with given conditions while retaining variation in outputs. The authors critique the standard classifier-free guidance (CFG) method, which often leads to reduced variation at the cost of quality. They propose autoguidance, a technique that employs a less capable version of the original model for guidance during image generation. This approach results in improved image quality (achieving state-of-the-art FID scores) without sacrificing diversity. The study validates autoguidance using various experiments on ImageNet datasets, illustrating its effectiveness over existing methods and making it applicable to both conditional and unconditional image synthesis tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bg6fVPVs3s&name=pdf" class="link-primary">https://openreview.net/attachment?id=bg6fVPVs3s&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Human Expertise in Algorithmic Prediction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">human-AI collaboration</span>
<span class="badge bg-primary">algorithmic prediction</span>
<span class="badge bg-primary">human expertise</span>
<span class="badge bg-primary">medical diagnostics</span>
<span class="badge bg-primary">multicalibration</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel framework for incorporating human expertise into algorithmic predictions, demonstrating that expert feedback can significantly enhance predictive performance in specific instances where algorithms falter.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of integrating human judgment into algorithmic predictions, particularly in high-stakes scenarios such as medical diagnostics. It proposes a framework that identifies "algorithmically indistinguishable" inputs—instances where algorithms lack predictive power but experts can still provide valuable insights. By employing multicalibration techniques, the authors demonstrate how to effectively incorporate human feedback into predictive algorithms, leading to significant performance improvements on specific subsets of data. Empirical studies, including a chest X-ray classification task, show that while algorithms may generally outperform human experts, there exists a considerable population subgroup (approximately 30%) where human judgment enhances predictions. The findings advocate for improved human-AI collaboration to leverage both human insight and algorithmic efficiency, suggesting implications for healthcare and decision-making technologies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=wpGJ2AX6SZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=wpGJ2AX6SZ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">HydraLoRA</span>
<span class="badge bg-primary">Parameter-Efficient Fine-Tuning</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Asymmetric Architecture</span>
<span class="badge bg-primary">Mixture-of-Experts</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces HydraLoRA, a novel asymmetric fine-tuning architecture for Large Language Models that improves task adaptation efficiency by mitigating parameter interference through shared and distinct matrices.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents HydraLoRA, an innovative framework aimed at enhancing the efficiency of fine-tuning Large Language Models (LLMs) through an asymmetric architecture. Traditional Parameter-Efficient Fine-Tuning (PEFT) methods, like LoRA, often suffer from performance deterioration in complex datasets due to task interference. To tackle this, HydraLoRA employs a shared matrix for common knowledge and multiple distinct matrices for specific tasks, automatically identifying intrinsic components during training using a Mixture-of-Experts (MoE) setup. Experimental results demonstrate that HydraLoRA outperforms existing PEFT techniques across various domains, achieving better performance with fewer parameters and without relying on domain expertise, ultimately providing a more efficient means of adapting LLMs to diverse tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qEpi8uWX3N&name=pdf" class="link-primary">https://openreview.net/attachment?id=qEpi8uWX3N&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Mendelian randomization</span>
<span class="badge bg-primary">bi-directional causality</span>
<span class="badge bg-primary">instrumental variables</span>
<span class="badge bg-primary">causal inference</span>
<span class="badge bg-primary">observational data</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel method to identify valid instrumental variables and estimate causal effects in bi-directional Mendelian randomization models despite the presence of invalid instruments and unmeasured confounding.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the complex problem of estimating causal effects in bi-directional Mendelian randomization (MR) using observational data, where some instrumental variables may be invalid, and unmeasured confounding exists. The authors present necessary and sufficient conditions for the identification of valid instrumental variables (IVs) in bi-directional MR models and develop a cluster fusion-like algorithm, termed PReBiM, to discover these valid sets and estimate causal effects. The theoretical correctness of the proposed method is established, and extensive experiments demonstrate its efficacy in accurately estimating causal effects compared to traditional methods in both one-directional and bi-directional contexts. The findings highlight the potential for deeper insights into causal relationships in various fields, including genetics and epidemiology.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=S2P6KPLtm8&name=pdf" class="link-primary">https://openreview.net/attachment?id=S2P6KPLtm8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improved Distribution Matching Distillation for Fast Image Synthesis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">distillation</span>
<span class="badge bg-primary">image synthesis</span>
<span class="badge bg-primary">GAN</span>
<span class="badge bg-primary">text-to-image</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces DMD2, an advanced method for fast image synthesis that improves upon Distribution Matching Distillation (DMD) by eliminating costly regression losses and integrating GAN approaches, achieving state-of-the-art results in one-step image generation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents DMD2, an enhanced technique for converting complex diffusion models into efficient, few-step image generators. By removing the regression loss required in previous DMD approaches, the authors mitigate the heavy computational burden associated with dataset construction while maintaining training stability through a Two Time-scale Update Rule and a GAN-based loss for better sample quality. DMD2 supports both one-step and multi-step generation, and experiments demonstrate its effectiveness in producing high-quality images, surpassing the performance of prior methods, including the original teacher models, with substantially lower inference costs. The findings suggest that DMD2 not only advances the field of fast image synthesis but also holds promise for broader applications in creative industries.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tQukGCDaNT&name=pdf" class="link-primary">https://openreview.net/attachment?id=tQukGCDaNT&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Unsupervised Environment Design</span>
<span class="badge bg-primary">Novelty Quantification</span>
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Curriculum Learning</span>
<span class="badge bg-primary">Gaussian Mixture Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents the Coverage-based Evaluation of Novelty In Environment (CENIE) framework, which enhances Unsupervised Environment Design (UED) by effectively quantifying environment novelty alongside traditional regret metrics to improve generalization in reinforcement learning agents.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenges of generalization in reinforcement learning agents by introducing the CENIE framework, which quantifies environment novelty through the lens of state-action space coverage, allowing for more effective curriculum design in UED. CENIE integrates Gaussian Mixture Models to assess novelty dynamically based on the student's accumulated experiences, ultimately complementing existing regret-based methods. Experimental evaluations demonstrate that incorporating CENIE into leading UED algorithms significantly improves zero-shot generalization performance across various benchmarks, indicating the importance of novelty in training robust reinforcement learning agents. This work highlights the potential of combining novelty-driven exploration with traditional regret-based exploitation in developing more capable agents.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=UdxpjKO2F9&name=pdf" class="link-primary">https://openreview.net/attachment?id=UdxpjKO2F9&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning diffusion at lightspeed</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion processes</span>
<span class="badge bg-primary">JKOnet</span>
<span class="badge bg-primary">energy functional</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">potential energy</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">JKOnet is a novel method that efficiently learns the energy functionals governing diffusion processes from observational data, significantly outperforming existing models in computational complexity and accuracy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces JKOnet, a simplified model designed to learn the energy functionals that drive diffusion processes, such as biological system dynamics and generative model behavior. Unlike existing approaches that rely on complex bilevel optimization, JKOnet leverages first-order necessary optimality conditions, leading to a single-level optimization problem that minimizes a quadratic loss. The authors demonstrate that JKOnet can accurately recover potential, interaction, and internal energy components from population data, achieving state-of-the-art performance with reduced computational costs. Through extensive experiments, including applications to cellular process predictions, the authors show that their method is highly efficient and scalable, thus providing significant insights for modeling complex diffusion dynamics with real-world implications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=y10avdRFNK&name=pdf" class="link-primary">https://openreview.net/attachment?id=y10avdRFNK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning Formal Mathematics From Intrinsic Motivation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Intrinsic Motivation</span>
<span class="badge bg-primary">Mathematical Reasoning</span>
<span class="badge bg-primary">Axiomatic Systems</span>
<span class="badge bg-primary">Theorem Proving</span>
<span class="badge bg-primary">Learning Agents</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents MINIMO, an agent that learns to generate and prove mathematical conjectures independently from only the axioms of a formal domain by leveraging intrinsic motivation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces MINIMO (Mathematics from Intrinsic Motivation), an artificial agent that collaborates self-improvement in generating challenging yet provable mathematical conjectures and in theorem proving, beginning solely with the axioms of formal mathematical domains. By integrating methods from constrained decoding and type-directed synthesis, MINIMO can generate valid conjectures while simultaneously improving its proof search capabilities through experiences derived from its own failures and successes, a process enhanced via a technique known as hindsight relabeling. Experiments conducted in three axiomatic domains—propositional logic, arithmetic, and group theory—demonstrate that the agent self-improves in both conjecture generation and theorem proving, and that it can effectively tackle previously unused human-generated mathematical theorems. The findings suggest promising avenues for developing autonomous mathematical reasoning systems capable of contributing to formal mathematics.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uNKlTQ8mBD&name=pdf" class="link-primary">https://openreview.net/attachment?id=uNKlTQ8mBD&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning rigid-body simulators over implicit shapes for large-scale scenes and vision</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">learned simulators</span>
<span class="badge bg-primary">rigid-body dynamics</span>
<span class="badge bg-primary">signed-distance functions</span>
<span class="badge bg-primary">graph networks</span>
<span class="badge bg-primary">large-scale scenes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents SDF-Sim, a learned rigid-body simulator that utilizes signed-distance functions (SDFs) to effectively scale simulations to large scenes with hundreds of objects while maintaining performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces SDF-Sim, a novel approach to simulating rigid-body dynamics in large-scale scenarios by employing learned signed-distance functions (SDFs) for object shape representation. Unlike traditional mesh-based simulators, SDF-Sim overcomes limitations related to collision detection and memory consumption, enabling it to handle scenes with up to 1.1 million nodes. The simulator leverages graph networks to predict object dynamics and demonstrates the ability to extract 3D shapes from multi-view images, thus generalizing to real-world applications. Results indicate SDF-Sim significantly reduces runtime and memory usage compared to state-of-the-art methods, while still achieving realistic simulations of complex interactions among objects. The findings imply strong potential for applications in robotics, animation, and gaming, specifically where efficient real-time simulations are essential.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=QDYts5dYgq&name=pdf" class="link-primary">https://openreview.net/attachment?id=QDYts5dYgq&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">in-context learning</span>
<span class="badge bg-primary">modular arithmetic</span>
<span class="badge bg-primary">skill composition</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">transformers</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper investigates the emergence of in-context learning and skill composition in transformer models through a series of modular arithmetic tasks, revealing a transition from memorization to generalization as the number of training tasks increases.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study explores how large language models, specifically transformer-based architectures, develop the capability of in-context learning and skill composition when tasked with linear modular functions. By utilizing a collection of modular arithmetic tasks for pre-training while testing the models' generalization abilities out-of-distribution, the authors demonstrate that as the number of training tasks increases, the models transition from merely memorizing previous examples to leveraging learned skills for generalization. They identify four distinct phases of model performance based on training task diversity and few-shot examples, highlighting a transient nature of generalization in deeper models. The research provides insight into the structured representations formed within the models, contributing to the understanding of emergent capabilities in deep learning systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aVh9KRZdRk&name=pdf" class="link-primary">https://openreview.net/attachment?id=aVh9KRZdRk&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LLM Evaluators Recognize and Favor Their Own Generations</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">self-preference</span>
<span class="badge bg-primary">self-recognition</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">evaluation bias</span>
<span class="badge bg-primary">AI safety</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper investigates the self-preference bias exhibited by large language models (LLMs), revealing that their ability to recognize their own outputs correlates with a tendency to favor their own generations over those from others.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This research explores the phenomenon of self-preference in large language models (LLMs), where these models rate their outputs more favorably than those created by other models or humans. The authors assess whether LLMs can recognize their own texts and how this ability influences their self-preference bias. Through controlled experiments, they demonstrate that models like GPT-4 and Llama 2 show a significant capacity for self-recognition without fine-tuning and that fine-tuning enhances this ability. The results highlight a linear correlation between self-recognition capability and the strength of self-preference bias. The findings have critical implications for AI safety and evaluation methods, particularly for practices relying on self-evaluation, as they could lead to inflated performance assessments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4NJBV6Wp0h&name=pdf" class="link-primary">https://openreview.net/attachment?id=4NJBV6Wp0h&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">maximum entropy</span>
<span class="badge bg-primary">inverse reinforcement learning</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">energy-based models</span>
<span class="badge bg-primary">generative modeling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI) framework to enhance the sample quality of diffusion generative models using energy-based models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors propose a novel framework called Diffusion by Maximum Entropy Inverse Reinforcement Learning (DxMI), aimed at improving the sample quality of diffusion models, particularly when generating samples in fewer steps. By utilizing an energy-based model (EBM) to represent the log probability density as a reward signal, they establish a joint training procedure for the diffusion model and the EBM. Their approach reformulates the training as a minimax problem, where maximizing entropy facilitates exploration and stabilizes training dynamics. Furthermore, they introduce the Diffusion by Dynamic Programming (DxDP) algorithm, which efficiently updates the diffusion model without backpropagation through time. Empirical results show that diffusion models trained with DxMI generate high-quality samples in as few as 4 to 10 steps while also providing effective training of EBMs for anomaly detection.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=V0oJaLqY4E&name=pdf" class="link-primary">https://openreview.net/attachment?id=V0oJaLqY4E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Medical Decision-Making</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Multi-Agent Collaboration</span>
<span class="badge bg-primary">Complexity Assessment</span>
<span class="badge bg-primary">Adaptive Framework</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents MDAgents, an adaptive framework utilizing multiple large language models (LLMs) to enhance medical decision-making by dynamically assigning roles based on task complexity, achieving superior performance on diverse medical benchmarks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This research introduces MDAgents, a novel framework designed to leverage collaborations among large language models (LLMs) for effective medical decision-making. The framework mimics real-world clinical processes by automatically assessing the complexity of medical queries and recruiting the appropriate medical expertise as needed, whether that involves a single clinician or a multi-disciplinary team. Through a series of experiments, MDAgents demonstrated superior accuracy compared to traditional solo and group methods across ten medical benchmarks, highlighting significant improvements in handling complex medical tasks requiring multi-modal reasoning. The findings suggest that adaptive collaboration structures can enhance the efficiency and effectiveness of AI-assisted medical diagnoses, encouraging further exploration into integrating LLM capabilities into clinical settings.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=EKdk4vxKO4&name=pdf" class="link-primary">https://openreview.net/attachment?id=EKdk4vxKO4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">mesh generation</span>
<span class="badge bg-primary">3D reconstruction</span>
<span class="badge bg-primary">sparse-view</span>
<span class="badge bg-primary">transformers</span>
<span class="badge bg-primary">input guidance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents MeshFormer, a novel model for high-quality 3D mesh generation from sparse-view RGB images and normal maps, achieving efficient training and improved detail through a unified single-stage approach with explicit 3D guidance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces MeshFormer, an innovative open-world sparse-view reconstruction model that generates high-quality 3D textured meshes in just seconds from a small number of multi-view RGB images and their corresponding normal maps. The model leverages a unique architecture combining 3D voxel representations with transformers, allowing for explicit projective bias and improved learning efficiency. MeshFormer employs a unified single-stage training process by incorporating surface rendering with signed distance function (SDF) supervision, enabling direct high-quality mesh generation. Experimental results demonstrate that MeshFormer outperforms existing methods in terms of mesh quality, requiring significantly less computational resources, and can easily adapt to various 2D diffusion models to facilitate tasks such as single-image-to-3D and text-to-3D generation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=x7pjdDod6Z&name=pdf" class="link-primary">https://openreview.net/attachment?id=x7pjdDod6Z&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">linear attention</span>
<span class="badge bg-primary">softmax approximation</span>
<span class="badge bg-primary">MetaLA</span>
<span class="badge bg-primary">transformer models</span>
<span class="badge bg-primary">optimal design</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces MetaLA, a new linear attention mechanism that provides an optimal approximation of softmax attention while meeting criteria for memory efficiency, parameter usage, and model performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the limitations of existing linear complexity models like LinFormer, State Space Model (SSM), and Linear RNN (LinRNN) in approximating softmax attention within transformer architectures. The authors unify these models under a common framework and derive three necessary conditions for optimal linear attention: dynamic memory capability, static approximation ability, and minimal parameter usage. They propose the Meta Linear Attention (MetaLA) model, which meets these criteria by eliminating unnecessary key matrices, employing self-augmentation techniques, and incorporating short convolutions for enhanced local interactions. Experimental results demonstrate that MetaLA outperforms existing linear attention models across various tasks, including language modeling, associative recall, and image classification, indicating its effectiveness and efficiency as an alternative to traditional softmax attention mechanisms.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Y8YVCOMEpz&name=pdf" class="link-primary">https://openreview.net/attachment?id=Y8YVCOMEpz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Neural Pfaffians: Solving Many Many-Electron Schrdinger Equations</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Neural Networks</span>
<span class="badge bg-primary">Quantum Chemistry</span>
<span class="badge bg-primary">Pfafans</span>
<span class="badge bg-primary">Many-Electron Systems</span>
<span class="badge bg-primary">Variational Monte Carlo</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Neural Pfafans, a new class of fully learnable neural wave functions that improve the accuracy and efficiency of approximating the ground states of many-electron systems by utilizing Pfafans instead of traditional Slater determinants.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Neural Pfafans (NeurPf), a novel framework that addresses the challenges in approximating the ground states of many-electron systems in quantum chemistry. By leveraging Pfafans, the authors circumvent the limitations of Slater determinants, enabling overparametrization without the strict orbital selection constraints related to spin configurations and molecular structures. The empirical evaluation showcases that NeurPf achieves chemical accuracy in calculating ground and ionization energies across various systems, outperforming existing wave function models and traditional reference methods such as CCSD(T) on the TinyMol dataset. Furthermore, the research highlights the model's capability to generalize across diverse chemical systems without substantial loss in accuracy, making it a significant advancement in the computational tools available for quantum chemistry.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HRkniCWM3E&name=pdf" class="link-primary">https://openreview.net/attachment?id=HRkniCWM3E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">fMRI</span>
<span class="badge bg-primary">video reconstruction</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">NeuroClips</span>
<span class="badge bg-primary">semantic perception</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">NeuroClips is a novel framework that successfully reconstructs high-fidelity and smooth videos from fMRI data by employing distinct semantics and perception reconstructors to capture both high-level semantics and low-level perceptual details.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents NeuroClips, an innovative framework aimed at reconstructing continuous video sequences from functional magnetic resonance imaging (fMRI) data, which traditionally poses challenges in relating the low temporal resolution of fMRI to the high resolution of video frames. The framework integrates two core components: a Semantics Reconstructor that focuses on accurately capturing high-level semantic features, and a Perception Reconstructor that decodes low-level perceptual details to ensure video smoothness. By employing a pre-trained text-to-video diffusion model, NeuroClips demonstrated significant improvements over existing methods, achieving a 6-second video reconstruction at 8 frames per second with notable enhancements in structural similarity and spatiotemporal metrics. This work bridges the gap between brain activity and video representation, offering new insights into cognitive neuroscience and advancing methodologies in visual stimulus decoding.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8qu52Fl1Dt&name=pdf" class="link-primary">https://openreview.net/attachment?id=8qu52Fl1Dt&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Not All Tokens Are What You Need for Pretraining</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Selective Language Modeling</span>
<span class="badge bg-primary">Token Selection</span>
<span class="badge bg-primary">Language Model Pretraining</span>
<span class="badge bg-primary">Data Efficiency</span>
<span class="badge bg-primary">Mathematical Reasoning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces RHO-1, a language model that employs Selective Language Modeling (SLM) to improve training efficiency by focusing on high-utility tokens, leading to significant performance gains in mathematical reasoning tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper challenges the conventional approach to language model pretraining that applies uniform loss to all tokens, proposing that not all tokens are equally important for effective model training. It introduces RHO-1, which utilizes Selective Language Modeling (SLM) to selectively train on more useful tokens identified through a reference model. By scoring tokens based on their alignment with desired distributions, RHO-1 demonstrates a remarkable improvement in few-shot accuracy on mathematical tasks after continual pretraining on a reduced dataset. Experimental results show that RHO-1 achieves state-of-the-art performance while leveraging only a fraction of the training data used by previous models, indicating enhanced data efficiency and robustness in the modeling approach for mathematical reasoning tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0NMzBwqaAJ&name=pdf" class="link-primary">https://openreview.net/attachment?id=0NMzBwqaAJ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Optimal Parallelization of Boosting</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Boosting</span>
<span class="badge bg-primary">Parallelization</span>
<span class="badge bg-primary">Weak-to-Strong Learning</span>
<span class="badge bg-primary">Complexity</span>
<span class="badge bg-primary">Algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper closes the gap between theoretical lower bounds and practical algorithms for parallelizing boosting methods by introducing an improved algorithm with optimal trade-offs in complexity parameters.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the parallelization of boosting algorithms, addressing a significant gap between theoretical lower bounds and the performance of existing algorithms. It establishes stronger lower bounds on the parallel complexity of weak-to-strong learners and presents a new algorithm that efficiently uses parallelism to optimize the number of training rounds and parallel work per round. The authors show that their algorithm achieves a trade-off in complexity parameters that matches the established lower bounds, thereby demonstrating nearly sample-optimal performance in the context of parallel boosting. The implications of this research advance the understanding of the inherent trade-offs in boosting and could lead to more efficient implementations in machine learning systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=rtz4df9IF1&name=pdf" class="link-primary">https://openreview.net/attachment?id=rtz4df9IF1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Policy Learning</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Tutorial Books</span>
<span class="badge bg-primary">Decision-making</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a novel approach called Policy Learning from Tutorial Books (PLfB), which leverages Large Language Models to derive policy networks from written tutorial content, demonstrating significant performance improvements in game environments without real-world interactions.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study introduces the concept of Policy Learning from Tutorial Books (PLfB), which aims to teach agents decision-making skills by extracting knowledge from written materials, notably tutorial books. The authors propose a three-stage framework: Understanding, Rehearsing, and Introspecting (URI) that utilizes Large Language Models (LLMs) to convert text into a structured knowledge database, simulate decision-making processes, and refine policies through introspection of generated datasets. Experiments conducted on Tic-Tac-Toe and Football environments reveal that the URI method consistently outperforms baseline approaches, achieving notable win rates against competing agents without any real data interactions. The findings suggest that leveraging textual knowledge can effectively enhance policy learning for AI agents, opening new avenues for research in offline reinforcement learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Ddak3nSqQM&name=pdf" class="link-primary">https://openreview.net/attachment?id=Ddak3nSqQM&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">PV-Tuning</span>
<span class="badge bg-primary">model compression</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents PV-Tuning, a novel framework that improves fine-tuning strategies for extreme quantization of large language models (LLMs) to achieve better accuracy and efficiency at 1-2 bits per parameter.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenges of extreme compression of large language models (LLMs) by proposing PV-Tuning, a framework that enhances the performance of quantization-aware fine-tuning strategies. It critiques existing methods that rely heavily on straight-through estimators for updating compressed weights, revealing their limitations in optimization. By introducing an alternative optimization algorithm that systematically refines both discrete and continuous parameters, PV-Tuning is shown to outperform previous state-of-the-art techniques for quantized models, achieving Pareto-optimal quantization at 2 bits per parameter for Llama-2 models. The experimental results validate the framework's effectiveness using leading LLMs, suggesting significant implications for deploying high-performance models in resource-constrained environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=YvA8UF0I37&name=pdf" class="link-primary">https://openreview.net/attachment?id=YvA8UF0I37&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Questioning the Survey Responses of Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">survey responses</span>
<span class="badge bg-primary">biases</span>
<span class="badge bg-primary">American Community Survey</span>
<span class="badge bg-primary">alignment metrics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper critically analyzes the use of survey methodologies to evaluate large language models, revealing significant biases in their responses that undermine their resemblance to human populations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the efficacy of utilizing survey responses from large language models (LLMs) to understand their demographic and alignment characteristics, using questions from the American Community Survey (ACS) as a basis. Through systematic prompting of 43 different language models, the authors identify substantial ordering and labeling biases affecting the models' responses, which often appear closer to uniform distributions regardless of the size or pre-training data of the models. The study concludes that even after adjusting for these biases, LLMs do not meaningfully align with U.S. census data, which raises concerns about the validity of conclusions drawn from survey-derived alignment metrics. These findings suggest that current methodologies may yield misleading insights regarding the representation of various demographic subgroups by LLMs, calling for caution in applying survey results to model evaluation and a need for further research on effective assessment strategies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Oo7dlLgqQX&name=pdf" class="link-primary">https://openreview.net/attachment?id=Oo7dlLgqQX&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">latent dynamics</span>
<span class="badge bg-primary">statistical modularity</span>
<span class="badge bg-primary">algorithmic modularity</span>
<span class="badge bg-primary">representation learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the challenges and solutions of reinforcement learning in environments with complex observations but simple latent dynamics, establishing a framework for statistical and algorithmic modularity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the complexities associated with reinforcement learning (RL) in environments characterized by rich, high-dimensional observations while underlying dynamics are governed by a simpler, unobserved latent state space. It introduces a theoretical framework that explores two major aspects: statistical modularity, which relates to the tractability of learning under latent dynamics based on existing RL settings, and algorithmic modularity, which enables the efficient adaptation of latent RL algorithms to observable settings. The authors present a negative result showing that many well-studied RL settings do not maintain statistical modularity under latent dynamics and complement this with positive results demonstrating that specific structural conditions, like pushforward coverability, can enable practical learning. Additionally, the paper develops algorithms that transform existing RL methods for latent dynamics into observable settings, showcasing the foundational work towards a unified theory in this domain.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qf2uZAdy1N&name=pdf" class="link-primary">https://openreview.net/attachment?id=qf2uZAdy1N&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Return of Unconditional Generation: A Self-supervised Representation Generation Method</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Unconditional Generation</span>
<span class="badge bg-primary">Self-Supervised Learning</span>
<span class="badge bg-primary">Representation Conditioned Generation</span>
<span class="badge bg-primary">Image Synthesis</span>
<span class="badge bg-primary">Generative Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel framework called Representation-Conditioned Generation (RCG) that significantly enhances the quality of unconditional image generation by leveraging self-supervised representations instead of human labels.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the long-standing challenge of unconditional image generation using a new framework, Representation-Conditioned Generation (RCG). By mapping images to a low-dimensional representation space via a self-supervised encoder, RCG generates abstract representations that serve as conditioning inputs for an image generator, thus bridging the performance gap with conditional generation methods. Comprehensive experiments demonstrate that RCG achieves a state-of-the-art Frechet Inception Distance (FID) of 2.15 on the ImageNet dataset, greatly outperforming previous unconditional methods and approaching the quality of leading conditional methods. This framework effectively utilizes the vast amounts of available unlabeled data, offering a promising direction for future research in generative models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=clTa4JFBML&name=pdf" class="link-primary">https://openreview.net/attachment?id=clTa4JFBML&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D Referring Expression Segmentation</span>
<span class="badge bg-primary">Spatial Awareness</span>
<span class="badge bg-primary">Rule-Guided Weak Supervision</span>
<span class="badge bg-primary">Text-driven Localization Module</span>
<span class="badge bg-primary">Point Clouds</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces RG-SAN, a Rule-Guided Spatial Awareness Network that enhances 3D referring expression segmentation by effectively modeling spatial relationships and using weak supervision for improved accuracy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the Rule-Guided Spatial Awareness Network (RG-SAN) to address the challenges of 3D referring expression segmentation (3D-RES). Traditional methods struggle with over-segmentation and mis-segmentation due to a lack of focus on spatial information among instances. RG-SAN comprises a Text-driven Localization Module (TLM) that accurately locates mentioned instances and iteratively refines their positions, alongside a Rule-guided Weak Supervision (RWS) strategy that leverages dependency tree rules for positional guidance based solely on the target instance. Experimental results on the ScanRefer benchmark demonstrate that RG-SAN achieves a mean Intersection over Union (mIoU) improvement of 5.1 points compared to prior models and shows enhanced robustness against spatial ambiguities in descriptions, emphasizing the significance of incorporating spatial context in visual grounding tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=r5spnrY6H3&name=pdf" class="link-primary">https://openreview.net/attachment?id=r5spnrY6H3&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Reinforcement Learning</span>
<span class="badge bg-primary">Code-as-Policy</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Hierarchical Framework</span>
<span class="badge bg-primary">Minecraft</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces RL-GPT, a novel framework that integrates reinforcement learning and large language models to enhance task learning in complex environments, achieving superior performance in the Minecraft game.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents RL-GPT, a two-level hierarchical framework designed to merge reinforcement learning (RL) with large language models (LLMs) for effective task learning in open-world environments like Minecraft. The approach features a slow agent that decomposes tasks into manageable sub-actions and decides which can be implemented through coding, while a fast agent executes these coded actions and fine-tunes them through RL. Experimental results demonstrate that RL-GPT significantly outperforms traditional RL methods and existing LLM-based agents, achieving high success rates in various tasks within Minecraft, including complex long-horizon challenges like obtaining diamonds. The integration of high-level planning and low-level action control is posited to improve sample efficiency and adaptability in task execution.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LEzx6QRkRH&name=pdf" class="link-primary">https://openreview.net/attachment?id=LEzx6QRkRH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Scale Equivariant Graph Metanetworks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Scaling symmetries</span>
<span class="badge bg-primary">MetaNetworks</span>
<span class="badge bg-primary">Neural networks</span>
<span class="badge bg-primary">Equivariance</span>
<span class="badge bg-primary">Graph structures</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes Scale Equivariant Graph MetaNetworks (ScaleGMNs), which integrate scaling symmetries into the representation and processing of neural networks for improved performance across various architectures and datasets.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces Scale Equivariant Graph MetaNetworks (ScaleGMNs), which extend existing Graph Metanetworks by incorporating scaling symmetries present in neural networks, particularly those arising from typical activation functions. The authors establish the design principles of ScaleGMNs, demonstrating their ability to maintain equivariance to both permutations and scaling operations during the processing of feedforward neural networks. They provide theoretical guarantees on the expressivity of ScaleGMNs, showing that they can simulate both the forward and backward passes of any input feedforward neural network. Experimental results indicate that ScaleGMNs outperform state-of-the-art methods across multiple datasets and activation functions, highlighting the significance of scaling symmetries as a vital inductive bias in neural network processing.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8Fxqn1tZM1&name=pdf" class="link-primary">https://openreview.net/attachment?id=8Fxqn1tZM1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">A* search</span>
<span class="badge bg-primary">Monte-Carlo tree search</span>
<span class="badge bg-primary">heuristic search</span>
<span class="badge bg-primary">selective sampling</span>
<span class="badge bg-primary">exploration</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces SeeA, an enhanced A* search algorithm that leverages selective sampling to improve exploration and efficiency when heuristic accuracy is insufficient.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents SeeA (Sampling-exploration enhanced A* search), a novel search algorithm aimed at improving the efficiency of classical A* search by incorporating a selective sampling method to construct a dynamic candidate subset of open nodes. Unlike traditional A*, which consistently expands the node with the best heuristic value, SeeA diversifies exploration by sometimes selecting nodes that may not have the best heuristic estimates, thus enabling the discovery of more promising branches in the search space. The research also investigates three sampling techniques—uniform, clustering, and UCT-like sampling—demonstrating SeeA's theoretical superiority over standard A* when heuristic functions are inaccurate. Empirical results from applications in retrosynthetic planning, logic synthesis, and the Sokoban game indicate that SeeA significantly outperforms leading heuristic search algorithms in problem-solving success rates and solution quality while minimizing node expansions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mSaqxZVZW8&name=pdf" class="link-primary">https://openreview.net/attachment?id=mSaqxZVZW8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">sample complexity</span>
<span class="badge bg-primary">average-reward MDPs</span>
<span class="badge bg-primary">discounting</span>
<span class="badge bg-primary">weakly communicating</span>
<span class="badge bg-primary">generative models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper establishes optimal sample complexity bounds for learning near-optimal policies in weakly communicating and general average-reward Markov decision processes (MDPs).</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the sample complexity required to learn ε-optimal policies in average-reward MDPs using a generative model. The authors derive sample complexity bounds for weakly communicating MDPs, demonstrating that eO(SAH²/ε) samples suffice, where H represents the span of the bias function of the optimal policy. They also introduce a transient time parameter B for general average-reward MDPs, establishing a sample complexity of eO(SAB + H²/ε) and proving a matching lower bound. The results highlight the necessity of considering both span and transient time parameters for optimal sample complexity in reinforcement learning scenarios, improving upon previously known bounds by minimizing dependence on the effective horizon for both specific and general MDP instances.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pGEY8JQ3qx&name=pdf" class="link-primary">https://openreview.net/attachment?id=pGEY8JQ3qx&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Statistical Efficiency of Distributional Temporal Difference Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Distributional reinforcement learning</span>
<span class="badge bg-primary">distributional temporal difference learning</span>
<span class="badge bg-primary">statistical efficiency</span>
<span class="badge bg-primary">convergence bounds</span>
<span class="badge bg-primary">Wasserstein distance.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper analyzes the finite-sample performance of distributional temporal difference learning algorithms, establishing minimax optimal sample complexity bounds for both non-parametric and categorical instances.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the statistical efficiency of distributional temporal difference (TD) learning in the context of distributional reinforcement learning (DRL), primarily focusing on finite-sample performance. It introduces non-parametric distributional TD (NTD) and revisits categorical TD (CTD), demonstrating that both require approximately \(O(\frac{1}{2p(1)^{2p+1}})\) iterations to achieve -optimal estimators in the \(p\)-Wasserstein distance. The results indicate that these bounds are minimax optimal, contributing significantly to our understanding of distributional policy evaluation in reinforcement learning. Additionally, the authors establish a novel Freedman's inequality in Hilbert spaces, which provides a theoretical tool of independent interest for future research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=eWUM5hRYgH&name=pdf" class="link-primary">https://openreview.net/attachment?id=eWUM5hRYgH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Stochastic Taylor Derivative Estimator</span>
<span class="badge bg-primary">High-Dimensional PDEs</span>
<span class="badge bg-primary">Physics-Informed Neural Networks</span>
<span class="badge bg-primary">Automatic Differentiation</span>
<span class="badge bg-primary">Computational Efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents the Stochastic Taylor Derivative Estimator (STDE), which significantly improves the computation of arbitrary high-order differential operators in neural networks, achieving over 1000x speed-up and 30% memory reduction over traditional methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces the Stochastic Taylor Derivative Estimator (STDE) to efficiently compute high-dimensional and high-order derivatives in neural networks, particularly within the context of Physics-Informed Neural Networks (PINNs). The authors address the computational challenges associated with the exponential growth of derivative tensor size and complexity by utilizing high-order automatic differentiation combined with randomization techniques. Their method demonstrates remarkable performance improvements, including substantial reductions in evaluation time and memory requirements, allowing for the solution of million-dimensional partial differential equations within minutes on standard GPUs. STDE not only generalizes previous approaches but also enhances the feasibility of employing complex differential operators in large-scale machine learning tasks, thereby advancing the capabilities of PINNs in practical applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=J2wI2rCG2u&name=pdf" class="link-primary">https://openreview.net/attachment?id=J2wI2rCG2u&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Stylus: Automatic Adapter Selection for Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adapter selection</span>
<span class="badge bg-primary">image generation</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">Stable Diffusion</span>
<span class="badge bg-primary">visual fidelity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Stylus, a system that automatically selects and composes highly relevant adapters for diffusion models, enhancing image generation quality and diversity based on user prompts.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Stylus, a novel method for improving image generation using diffusion models by automatically selecting and composing low-rank adaptation (LoRA) adapters based on user-provided prompts. Stylus employs a three-stage framework consisting of a refiner that generates better descriptions of adapter tasks, a retriever that fetches relevant adapters, and a composer that segments prompts into distinct tasks and assigns appropriate adapters to them. The authors developed StylusDocs, a curated dataset containing 75,000 adapters with enhanced documentation, to evaluate Stylus's effectiveness. Results indicate that Stylus achieves superior visual fidelity, textual alignment, and image diversity over existing models, demonstrating its potential to enhance generative AI art.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3Odq2tGSpp&name=pdf" class="link-primary">https://openreview.net/attachment?id=3Odq2tGSpp&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">The Road Less Scheduled</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Schedule-Free Optimization</span>
<span class="badge bg-primary">Learning Rate Schedules</span>
<span class="badge bg-primary">Gradient Descent</span>
<span class="badge bg-primary">Stochastic Optimization</span>
<span class="badge bg-primary">Empirical Evaluation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel Schedule-Free optimization method that outperforms existing learning rate schedules in various machine learning tasks without the need for specifying a stopping time.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper proposes a Schedule-Free optimization approach that eliminates the necessity for learning rate schedules, which are typically dependent on a predetermined optimization stopping time. The method unifies theoretical insights from scheduling and iterate averaging, resulting in state-of-the-art performance across different optimization problems, including deep learning and convex settings. Utilizing a modified momentum approach, it circumvents the need for additional hyper-parameters, demonstrating superior or comparable results to optimized cosine schedules in extensive empirical evaluations. The success of Schedule-Free AdamW is validated through its performance in the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge, showcasing its practical applicability and effectiveness in real-world tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0XeNkkENuI&name=pdf" class="link-primary">https://openreview.net/attachment?id=0XeNkkENuI&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">The Sample-Communication Complexity Trade-off in Federated Q-Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Federated Q-learning</span>
<span class="badge bg-primary">communication complexity</span>
<span class="badge bg-primary">sample complexity</span>
<span class="badge bg-primary">algorithm design</span>
<span class="badge bg-primary">reinforcement learning.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a comprehensive study of the sample-communication complexity trade-off in Federated Q-learning, introducing a novel algorithm, Fed-DVR-Q, that achieves optimal performance in both metrics simultaneously.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the collaborative learning of optimal Q-functions in an infinite horizon Markov Decision Process through Federated Q-learning, focusing on the trade-off between sample complexity and communication complexity. It establishes lower bounds on communication costs necessary for achieving collaborative benefits, revealing that any algorithm offering linear sample efficiency must incur significant communication costs. The authors then introduce Fed-DVR-Q, the first Federated Q-learning algorithm that achieves both order-optimal sample complexity and minimal communication costs, thereby fully characterizing the sample-communication complexity landscape for this domain. Through theoretical results and numerical experiments, the findings highlight the effectiveness and efficiency of the developed approach, which provides valuable insights for the design of practical federated learning systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6YIpvnkjUK&name=pdf" class="link-primary">https://openreview.net/attachment?id=6YIpvnkjUK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">hippocampus</span>
<span class="badge bg-primary">place cells</span>
<span class="badge bg-primary">contextual capacity</span>
<span class="badge bg-primary">memory encoding</span>
<span class="badge bg-primary">neural geometry</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that the contextual capacity of hippocampal place cell codes grows exponentially with the number of neurons, revealing a trade-off between spatial resolution and context discrimination.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates how hippocampal place cells, which are key to cognitive mapping in animals, encode contextual information by analyzing their firing properties through a geometric framework. The authors present a model showing that the number of distinct contexts the hippocampus can store increases exponentially with the number of place cells, while identifying a fundamental trade-off between high spatial resolution and the ability to distinguish multiple contexts. Furthermore, they explore how the physical arrangement of place cells, particularly clustering near boundaries, enhances contextual discrimination. The findings suggest that variability in place cell widths across the hippocampus may be an adaptive feature, optimizing the encoding of spatial and contextual information according to different behavioral needs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=REIK4SZMJt&name=pdf" class="link-primary">https://openreview.net/attachment?id=REIK4SZMJt&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Chain-of-Thought (CoT)</span>
<span class="badge bg-primary">Reasoning Boundary Framework</span>
<span class="badge bg-primary">Large Language Models (LLMs)</span>
<span class="badge bg-primary">Optimization Strategies</span>
<span class="badge bg-primary">Quantification Metrics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel Reasoning Boundary Framework (RBF) to quantify and optimize Chain-of-Thought reasoning in Large Language Models, establishing a systematic approach to enhance model performance across various complex tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a Reasoning Boundary Framework (RBF) to address the challenges of quantifying and optimizing Chain-of-Thought (CoT) capabilities in Large Language Models (LLMs). It defines the concept of a Reasoning Boundary (RB) to measure the upper limits of task-specific reasoning complexity and formulates a combination law for RB to enable practical quantification across diverse CoT tasks. The authors categorize RBs into three types—Completely Feasible, Partially Feasible, and Completely Infeasible—while proposing optimization techniques based on reasoning path adjustments and RB promotion strategies. Through extensive experiments involving 27 models across five complex reasoning tasks, the framework's effectiveness is validated, providing valuable insights into CoT strategies and laying the groundwork for future advancements in LLM optimization.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pC44UMwy2v&name=pdf" class="link-primary">https://openreview.net/attachment?id=pC44UMwy2v&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">audio-driven generation</span>
<span class="badge bg-primary">talking faces</span>
<span class="badge bg-primary">facial dynamics</span>
<span class="badge bg-primary">real-time rendering</span>
<span class="badge bg-primary">artificial intelligence</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents V ASA-1, a framework that generates lifelike talking faces in real-time from a single image and audio, achieving high realism and expressiveness in facial dynamics.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces V ASA-1, an advanced framework designed to create realistic talking face videos from a static image and a speech audio clip. The model leverages a diffusion-based approach to generate facial movements and head dynamics in a holistic latent space, significantly improving synchronization with audio and the expressiveness of facial nuances compared to prior methods. Extensive experiments demonstrate that V ASA-1 outperforms existing approaches across various metrics, achieving high video quality and real-time performance. The implications of this work extend to enhancing virtual human interactions across different applications, including education and healthcare, while addressing the challenges of latency and realism in digital communication.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=5zSCSE0k41&name=pdf" class="link-primary">https://openreview.net/attachment?id=5zSCSE0k41&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Visual Autoregressive Modeling</span>
<span class="badge bg-primary">Image Generation</span>
<span class="badge bg-primary">Scaling Laws</span>
<span class="badge bg-primary">Transformer Architecture</span>
<span class="badge bg-primary">Zero-Shot Generalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Visual AutoRegressive (V AR) modeling, a new image generation framework that significantly outperforms existing autoregressive and diffusion models by utilizing a coarse-to-fine next-scale prediction method.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Visual AutoRegressive (V AR) modeling, which redefines autoregressive learning in image generation to focus on next-scale prediction rather than traditional next-token prediction. This method enables fast learning of visual distributions and improves performance on the ImageNet benchmark, achieving a Fréchet Inception Distance (FID) of 1.73 and an Inception Score (IS) of 350.2, along with twenty-fold faster inference speeds compared to baseline autoregressive models. V AR exhibits strong scalability, demonstrated by power-law relationships between model size, training compute, and performance metrics, alongside promising zero-shot generalization capabilities in image inpainting and editing tasks. The findings suggest V AR effectively harnesses strengths from large language models and enhances generative capabilities in vision.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gojL67CfS8&name=pdf" class="link-primary">https://openreview.net/attachment?id=gojL67CfS8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">graph neural networks</span>
<span class="badge bg-primary">Weisfeiler-Leman</span>
<span class="badge bg-primary">expressive power</span>
<span class="badge bg-primary">cycles</span>
<span class="badge bg-primary">homomorphism counting</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the r-loopy Weisfeiler-Leman (r-`WL) algorithm and the corresponding graph neural network framework (r-`MPNN), which effectively counts cycles of varying lengths and enables distinguishing complex graph structures beyond the limitations of existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents r-loopy Weisfeiler-Leman (r-`WL), a novel hierarchy of graph isomorphism tests aimed at improving the expressive power of graph neural networks (GNNs). The authors propose the r-`MPNN framework, capable of counting cycles up to length r+2, thus overcoming limitations faced by traditional GNNs, specifically those that rely on the Weisfeiler-Leman (WL) algorithm. They demonstrate that r-`WL can count homomorphisms of cactus graphs, which is a significant enhancement over existing approaches like k-WL. Empirical validation on both synthetic and real-world datasets shows that r-`MPNN scales efficiently and outperforms current state-of-the-art models, especially on sparse graphs, highlighting its potential for applications in various fields that require intricate graph structure analysis.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9O2sVnEHor&name=pdf" class="link-primary">https://openreview.net/attachment?id=9O2sVnEHor&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">You Only Cache Once: Decoder-Decoder Architectures for Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">decoder-decoder architecture</span>
<span class="badge bg-primary">YOCO</span>
<span class="badge bg-primary">key-value caching</span>
<span class="badge bg-primary">long-context modeling</span>
<span class="badge bg-primary">efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents YOCO, a novel decoder-decoder architecture for large language models that significantly reduces GPU memory usage and prefill latency while maintaining strong performance for long-context tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces YOCO, a decoder-decoder architecture designed for large language models that optimizes memory usage by only caching key-value (KV) pairs once, employing a self-decoder to generate global KV caches and a cross-decoder to reuse them. This approach allows YOCO to effectively handle long-context input, achieving scalability up to 1 million tokens and demonstrating improved inference performance compared to traditional transformers. Experimental evaluations show YOCO's competitive language modeling capabilities, with remarkable enhancements in prefill latency and GPU memory consumption, positioning it as a robust solution for future large language models built for long-sequence tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=25Ioxw576r&name=pdf" class="link-primary">https://openreview.net/attachment?id=25Ioxw576r&name=pdf</a></p>
</div>
</div>
</div>

        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
