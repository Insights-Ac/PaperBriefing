
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - ICLR 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">ICLR 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated on 2024-11-21 15:58:47 by <a href="https://github.com/Logan-Lin/PubSummarizer">PubSummarizer</a></em></p>
        <div class="row" data-masonry='{"percentPosition": true }'>
            <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">"What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">influence functions</span>
<span class="badge bg-primary">data selection</span>
<span class="badge bg-primary">model performance</span>
<span class="badge bg-primary">fairness</span>
<span class="badge bg-primary">robustness</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces an influence-based data selection approach to enhance the performance and interpretability of classifiers in terms of utility, fairness, and robustness, validated across various application scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the critical yet often overlooked issue of data selection to improve classification model performance by utilizing influence functions to evaluate the impact of training samples on utility, fairness, and robustness. It proposes influence estimation models, particularly tree-based methods, to interpret which features contribute positively or negatively to model outcomes, and presents data trimming strategies to enhance performance within specific budgets. Through extensive experiments on both synthetic and real-world datasets, the authors demonstrate that their approach not only improves accuracy but also enhances fairness and robustness across challenging scenarios, such as distribution shifts and adversarial attacks, underscoring its practical implications for machine learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HE9eUQlAvo&name=pdf" class="link-primary">https://openreview.net/attachment?id=HE9eUQlAvo&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">web automation</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">HTML understanding</span>
<span class="badge bg-primary">program synthesis</span>
<span class="badge bg-primary">self-experience supervision</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents WebAgent, an LLM-driven web automation system that improves task success on real websites using HTML-T5 for planning and summarization and Flan-U-PaLM for code generation, achieving over 50% higher success rates compared to previous methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces WebAgent, a novel autonomous agent designed to complete tasks on real websites by employing large language models (LLMs) with specialized capabilities. It addresses challenges in web automation, such as long HTML documents and open-ended action spaces, by integrating HTML-T5 for planning and summarization and the Flan-U-PaLM model for generating executable Python programs. Through self-experience supervision, which leverages data generated from scripted interactions, WebAgent significantly enhances task completion success rates by over 50% on various websites, outperforming existing methods. The findings underscore the importance of domain-specific LLMs in improving web automation performance, with implications for advancing autonomous web agents in practical applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9JQtrumvg8&name=pdf" class="link-primary">https://openreview.net/attachment?id=9JQtrumvg8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Self-Repellent Random Walks</span>
<span class="badge bg-primary">Distributed Stochastic Optimization</span>
<span class="badge bg-primary">Stochastic Approximation</span>
<span class="badge bg-primary">Convergence Analysis</span>
<span class="badge bg-primary">Asymptotic Covariance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Self-Repellent Random Walk (SRRW) to enhance distributed stochastic optimization algorithms, demonstrating superior asymptotic convergence properties and lower variance compared to traditional Markovian approaches.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The study investigates a novel family of distributed stochastic optimization algorithms using Self-Repellent Random Walk (SRRW), which employs a non-linear Markov chain to improve the sampling efficiency of gradients in decentralized learning settings. By replacing conventional Markov chains with the SRRW, parameterized by a scalar that controls self-repellence, the authors derive a generalized algorithm termed SA-SRRW. The paper proves that the optimization errors of SA-SRRW converge to zero almost surely and establishes a central limit theorem, revealing that the asymptotic covariance matrices of this method exhibit decreased variance compared to traditional approaches, specifically achieving a rate of O(1/2). Empirical evaluations affirm the theoretical predictions, highlighting the practical benefits of using SRRW in real-world distributed optimization tasks, thus advocating for its broader adoption in stochastic optimization contexts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=BV1PHbTJzd&name=pdf" class="link-primary">https://openreview.net/attachment?id=BV1PHbTJzd&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Amortizing intractable inference in large language models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">GFlowNets</span>
<span class="badge bg-primary">Amortized Inference</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Chain-of-Thought Reasoning</span>
<span class="badge bg-primary">Intractable Posterior Sampling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel approach, leveraging GFlowNets for amortized Bayesian inference, that enables large language models to sample from intractable posteriors effectively, enhancing their performance in diverse tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the limitations of autoregressive large language models (LLMs) in sampling from intractable posterior distributions crucial for tasks like sequence continuation and reasoning. By fine-tuning LLMs using GFlowNets—diversity-seeking reinforcement learning algorithms—the authors propose an innovative amortized inference method that allows LLMs to sample from complex distributions effectively. Empirical results demonstrate significant improvements in both sample diversity and task performance across various applications, including story infilling, subjectivity classification, and arithmetic reasoning. The findings suggest that GFlowNet fine-tuning not only provides better fidelity and diversity trade-offs but also improves sample efficiency and generalization in scenarios with limited data.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Ouj6p4ca60&name=pdf" class="link-primary">https://openreview.net/attachment?id=Ouj6p4ca60&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Direct image alignment</span>
<span class="badge bg-primary">Gauss-Newton loss</span>
<span class="badge bg-primary">camera pose estimation</span>
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">feature matching.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents an analytical solution to the Gauss-Newton loss for direct image alignment, enhancing robustness to pose initialization and achieving high alignment accuracy with self-supervised feature descriptors.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of direct image alignment, particularly the reliance on initial pose accuracy for effective camera pose estimation. The authors derive a closed-form solution to the Gauss-Newton loss, allowing for dynamic adjustment of the convergence basin during optimization, which improves robustness to imprecise initializations. They demonstrate that their approach, which leverages self-supervised feature descriptors, achieves competitive performance compared to state-of-the-art end-to-end supervised methods across various datasets. The findings highlight the limitations of traditional end-to-end learning frameworks that utilize the Gauss-Newton loss, suggesting a deep connection between direct image alignment and feature matching, with implications for future research in robust feature representation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mE52zURNGc&name=pdf" class="link-primary">https://openreview.net/attachment?id=mE52zURNGc&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Nash Equilibrium</span>
<span class="badge bg-primary">Stochastic Optimization</span>
<span class="badge bg-primary">Monte Carlo Estimation</span>
<span class="badge bg-primary">Normal-Form Games</span>
<span class="badge bg-primary">Machine Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel loss function for approximating Nash equilibria in normal-form games that enables unbiased Monte Carlo estimation and leverages stochastic optimization techniques, providing efficient algorithms with promising empirical performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors propose a new loss function aimed at approximating Nash equilibria in normal-form games, which is designed for unbiased Monte Carlo estimation. This innovative approach reformulates the problem as a stochastic non-convex optimization task, allowing the application of powerful optimization methods like stochastic gradient descent (SGD). The paper includes rigorous theoretical analysis demonstrating the effectiveness of the proposed loss function and its favorable properties, alongside experimental results showing that SGD can outperform existing state-of-the-art methods in approximating Nash equilibria in larger games. The findings suggest that this new methodology can significantly advance computational game theory and its applications in multi-agent systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cc8h3I3V4E&name=pdf" class="link-primary">https://openreview.net/attachment?id=cc8h3I3V4E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ASID: Active Exploration for System Identification in Robotic Manipulation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">active exploration</span>
<span class="badge bg-primary">system identification</span>
<span class="badge bg-primary">robotic manipulation</span>
<span class="badge bg-primary">simulation-to-reality transfer</span>
<span class="badge bg-primary">reinforcement learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents ASID, a novel framework that employs active exploration for system identification in robotic manipulation, allowing effective sim-to-real transfer with minimal real-world data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces ASID (Active Exploration for System IDentification), a learning system designed to enhance robotic manipulation by autonomously refining simulation models through targeted exploration in the real world. The approach involves three main steps: performing exploration to gather informative data, updating simulator parameters based on real-world interactions, and training effective control policies in the refined simulation for zero-shot transfer to real-world tasks. Experimental results demonstrate that ASID effectively identifies physical parameters across various challenging tasks, requiring only a small amount of real-world data, thus bridging the sim-to-real gap commonly faced in robotics. The findings underscore the importance of carefully directed exploration in optimizing performance outcomes and practical applicability in real environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jNR6s6OSBT&name=pdf" class="link-primary">https://openreview.net/attachment?id=jNR6s6OSBT&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Batched Low-Rank Adaptation of Foundation Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Low-Rank Adaptation</span>
<span class="badge bg-primary">FLORA</span>
<span class="badge bg-primary">Foundation Models</span>
<span class="badge bg-primary">Batching</span>
<span class="badge bg-primary">Throughput</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Fast Low-Rank Adaptation (FLORA), a framework that enhances the batching capabilities of Low-Rank Adaptation (LORA) for foundation models, improving throughput and reducing latency without sacrificing accuracy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Fast Low-Rank Adaptation (FLORA), an extension of the Low-Rank Adaptation (LORA) method, designed to overcome the limitations of LORA in real-time serving scenarios by enabling each example in a minibatch to utilize unique low-rank adaptation weights. This modification allows for efficient batching of diverse user requests, significantly improving throughput and reducing latency compared to traditional LORA. Empirical evaluations on multilingual code generation and speech recognition tasks demonstrate that FLORA maintains LORA's accuracy while achieving up to 2X throughput improvements and halving latency, underscoring its practicality for serving large-scale foundation models in diverse applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=w4abltTZ2f&name=pdf" class="link-primary">https://openreview.net/attachment?id=w4abltTZ2f&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">expressiveness</span>
<span class="badge bg-primary">Graph Neural Networks</span>
<span class="badge bg-primary">homomorphism</span>
<span class="badge bg-primary">Weisfeiler-Lehman</span>
<span class="badge bg-primary">subgraph counting</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel quantitative framework based on homomorphism expressivity to assess and compare the expressive power of various Graph Neural Network (GNN) architectures.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of the Weisfeiler-Lehman hierarchy in evaluating the expressiveness of Graph Neural Networks (GNNs) by proposing a new framework centered on homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. By characterizing the expressivity of several GNN classes—specifically, MPNN, Subgraph GNN, Local GNN, and Folklore-type GNN—the authors demonstrate how this measure allows for complete and practical comparisons of different architectures. Through extensive theoretical analysis and empirical experiments, they establish a hierarchy of expressiveness among these GNNs, reveal links to subgraph counting capabilities, and show that theoretical expressivity correlates with practical performance in various tasks, paving the way for more effective GNN designs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HSKaGOi7Ar&name=pdf" class="link-primary">https://openreview.net/attachment?id=HSKaGOi7Ar&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">BooookScore: A systematic exploration of book-length summarization in the era of LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">book-length summarization</span>
<span class="badge bg-primary">coherence errors</span>
<span class="badge bg-primary">LLM evaluation</span>
<span class="badge bg-primary">automatic metrics</span>
<span class="badge bg-primary">human annotations</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces BOOOOK SCORE, a novel automatic metric for evaluating coherence in book-length summaries generated by large language models (LLMs), and conducts a systematic exploration of summarization methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of summarizing book-length documents using large language models (LLMs), specifically investigating coherence errors in generated summaries through human evaluations and an automatic metric named BOOOOK SCORE. It categorizes eight types of coherence errors observed in LLM-generated summaries of recently published books, employing two summarization strategies: hierarchical merging and incremental updating. The findings reveal differences in coherence between LLMs like GPT-4, Claude 2, and others, with hierarchical merging generally producing more coherent but less detailed summaries than incremental updating. BOOOOK SCORE, validated against human annotations, provides a scalable evaluation method and facilitates further research into the summarization capabilities of various LLMs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=7Ttk3RzDeu&name=pdf" class="link-primary">https://openreview.net/attachment?id=7Ttk3RzDeu&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Cameras as Rays: Pose Estimation via Ray Diffusion</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">pose estimation</span>
<span class="badge bg-primary">ray representation</span>
<span class="badge bg-primary">camera extrinsics</span>
<span class="badge bg-primary">denoising diffusion</span>
<span class="badge bg-primary">sparse views</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a distributed ray representation for camera pose estimation, utilizing regression and denoising diffusion methods to achieve state-of-the-art performance on the CO3D dataset.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel approach to camera pose estimation that addresses the challenges posed by sparse input images (fewer than 10). Instead of predicting global camera parameters like rotation and translation, the authors propose a distributed representation where each camera is treated as a bundle of rays associated with image patches. This representation enhances the coupling with spatial image features and allows for improved pose accuracy. The authors develop a regression-based model that maps image patches to rays and extend this method using a denoising diffusion model to account for uncertainties in sparse-view scenarios. The results demonstrate that both the regression and diffusion models not only surpass existing state-of-the-art methods but also generalize to unseen object categories and real-world captures, thereby providing a promising direction for future research in 3D reconstruction.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=EanCFCwAjM&name=pdf" class="link-primary">https://openreview.net/attachment?id=EanCFCwAjM&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">candidate label pruning</span>
<span class="badge bg-primary">partial-label learning</span>
<span class="badge bg-primary">data-centric approach</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">representation quality</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel approach called candidate label set pruning (CLSP) for partial-label learning that effectively filters out potential false candidate labels to enhance the performance of deep learning models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a new task known as candidate label set pruning (CLSP), which aims to improve the performance of deep partial-label learning (PLL) by reducing the size of candidate label sets associated with training instances. It proposes a training-free method that utilizes the inconsistency between the representation space of instances and their candidate label sets. The method identifies and eliminates high-probability false candidate labels by analyzing the labels from the k-nearest neighbors of an instance. Theoretical analyses are provided to establish an upper bound on the pruning error and to examine how representation quality affects the proposed method. Extensive experiments conducted on both synthetic and real-world datasets demonstrate significant improvements in deep PLL methods when applying the CLSP technique, advocating for a shift from traditional learning-centric approaches to a more data-centric perspective in partial-label learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Fk5IzauJ7F&name=pdf" class="link-primary">https://openreview.net/attachment?id=Fk5IzauJ7F&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">climate modeling</span>
<span class="badge bg-primary">weather forecasting</span>
<span class="badge bg-primary">neural ODE</span>
<span class="badge bg-primary">physics-informed</span>
<span class="badge bg-primary">uncertainty quantification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces ClimODE, a physics-informed neural model that enhances climate and weather forecasting by incorporating continuous-time dynamics and uncertainty quantification, achieving state-of-the-art performance with significantly fewer parameters than existing data-driven methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents ClimODE, a novel approach to climate and weather forecasting that integrates principles from statistical mechanics and utilizes continuous-time neural ODEs to model weather dynamics accurately. ClimODE captures the spatial movement of weather quantities through a value-conserving advection mechanism, allowing for more stable long-term predictions. It employs a hybrid model that combines local convolutions and global attention to account for both regional and global effects, alongside a probabilistic emission model to quantify uncertainty. The authors demonstrate that ClimODE outperforms existing deep learning methods in global and regional forecasting tasks while employing a more efficient parameterization, with open-source implementation made available for further research and application.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=xuY33XhEGR&name=pdf" class="link-primary">https://openreview.net/attachment?id=xuY33XhEGR&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Detecting, Explaining, and Mitigating Memorization in Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">memorization detection</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">text-conditional predictions</span>
<span class="badge bg-primary">mitigation strategies</span>
<span class="badge bg-primary">model privacy.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents an effective approach for detecting and mitigating memorization in diffusion models, utilizing the magnitude of text-conditional predictions to identify and explain memorization while proposing both inference-time and training-time strategies to counteract it.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the issue of unintentional memorization in diffusion models, where generated content closely replicates training data, posing legal risks. It introduces a method for detecting memorized prompts by analyzing the magnitude of text-conditional predictions, achieving high detection accuracy even with a single generation per prompt. Additionally, it offers an explainable framework that highlights the contribution of specific tokens to memorization, allowing for user prompt adjustments. Two mitigation strategies are proposed: one for inference that minimizes prediction magnitudes to reduce memorization effects without degrading output quality, and another for training that screens out potentially memorized pairs based on prediction metrics. The results demonstrate the effectiveness of these strategies in maintaining high generative quality while addressing memorization concerns, thus protecting the model's intellectual property and data privacy.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=84n3UwkH7b&name=pdf" class="link-primary">https://openreview.net/attachment?id=84n3UwkH7b&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Diffusion Model for Dense Matching</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Dense correspondence</span>
<span class="badge bg-primary">Diffusion model</span>
<span class="badge bg-primary">Generative prior</span>
<span class="badge bg-primary">Image matching</span>
<span class="badge bg-primary">Neural networks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents DiffMatch, a novel diffusion-based framework that effectively models both data and prior terms for dense image correspondence, outperforming existing methods in handling matching ambiguities.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper proposes DiffMatch, a conditional diffusion-based framework aimed at improving dense correspondence between image pairs by explicitly modeling both the data and prior terms, addressing limitations of prior methods that primarily focus on likelihood maximization. Leveraging a conditional denoising diffusion model, the approach incorporates initial correspondence and local matching costs, achieving a more robust representation of the matching field manifold. The authors employ a cascaded architecture to enhance input resolution, starting with a low-resolution model and transitioning to a super-resolution model for flow upsampling. Experimental results demonstrate significant performance improvements on standard benchmarks, particularly in challenging scenarios involving textureless regions and noise, highlighting the effectiveness of the generative prior established by DiffMatch over traditional discriminative methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Zsfiqpft6K&name=pdf" class="link-primary">https://openreview.net/attachment?id=Zsfiqpft6K&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D content creation</span>
<span class="badge bg-primary">Gaussian splatting</span>
<span class="badge bg-primary">optimization efficiency</span>
<span class="badge bg-primary">texture refinement</span>
<span class="badge bg-primary">generative models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents DreamGaussian, a framework that efficiently generates high-quality 3D content from images or text by using generative Gaussian splatting combined with novel mesh extraction and texture refinement techniques, achieving significant time savings over existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces DreamGaussian, a new framework designed to enhance the efficiency of 3D content creation, particularly for image-to-3D and text-to-3D tasks. Building on recent advancements in generative modeling, the authors propose a two-stage process that first utilizes 3D Gaussian splatting for rapid initialization of geometry and appearance, followed by an effective method for mesh extraction and UV-space texture refinement. Extensive experiments demonstrate that DreamGaussian can produce high-quality, textured 3D meshes from single images or text prompts in just a few minutes, offering around ten times the efficiency compared to previous optimization-based approaches. This work not only addresses challenges related to optimization speed and output quality but also opens new avenues for real-world applications in digital content creation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=UyNXMqnN3c&name=pdf" class="link-primary">https://openreview.net/attachment?id=UyNXMqnN3c&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">cooperative multi-agent reinforcement learning</span>
<span class="badge bg-primary">episodic memory</span>
<span class="badge bg-primary">semantic embeddings</span>
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">exploration-exploitation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Efficient episodic Memory Utilization (EMU) for cooperative multi-agent reinforcement learning, enhancing learning speed and policy optimization by utilizing trainable semantic embeddings and an episodic incentive structure.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Efficient episodic Memory Utilization (EMU) to enhance cooperative multi-agent reinforcement learning (MARL) by addressing the challenges of long convergence times and local optima in complex tasks. EMU incorporates a trainable encoder/decoder structure to create coherent memory embeddings that facilitate better exploration of episodic memory, and introduces an episodic incentive based on the desirability of states to promote desirable transitions. Evaluated in StarCraft II and Google Research Football, EMU demonstrates significant performance improvements over traditional methods, effectively expediting learning and achieving optimal policies. Overall, the contributions of EMU improve memory utilization and reduce the risk of local convergence without requiring extensive hyperparameter tuning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LjivA1SLZ6&name=pdf" class="link-primary">https://openreview.net/attachment?id=LjivA1SLZ6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">compositional generalization</span>
<span class="badge bg-primary">neural program synthesis</span>
<span class="badge bg-primary">ExeDec</span>
<span class="badge bg-primary">execution decomposition</span>
<span class="badge bg-primary">program synthesis tasks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces ExeDec, a novel execution decomposition strategy that improves compositional generalization in neural program synthesis by predicting execution subgoals, significantly enhancing performance compared to baseline methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of compositional generalization in neural program synthesis, proposing ExeDec, a strategy that breaks down complex programming tasks into smaller, manageable subtasks informed by execution predictions. The authors establish a meta-benchmark defining five forms of compositional generalization, which they utilize to create tasks for two datasets: RobustFill and DeepCoder. Experimental results demonstrate that ExeDec, when implemented with Transformer models trained from scratch, achieves higher synthesis performance and improved generalization abilities compared to traditional baselines. Furthermore, the paper shows that while large language models (LLMs) struggle with compositional generalization, an adapted ExeDec approach can enhance their performance in few-shot programming tasks. Overall, these findings highlight the importance of task decomposition in advancing neural program synthesis capabilities.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=oTRwljRgiv&name=pdf" class="link-primary">https://openreview.net/attachment?id=oTRwljRgiv&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">safety alignment</span>
<span class="badge bg-primary">adversarial attacks</span>
<span class="badge bg-primary">customization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the safety risks associated with fine-tuning aligned language models, revealing that both malicious and benign fine-tuning can substantially degrade their safety alignment.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores how fine-tuning large language models (LLMs) like GPT-3.5 Turbo and Llama-2 can introduce significant safety risks, even when performed by users with benign intentions. Through red teaming studies, the researchers demonstrate that fine-tuning with a small number of malicious examples can easily compromise the models' safety guardrails, allowing them to produce harmful outputs. Moreover, the study reveals that even fine-tuning with harmless datasets can inadvertently degrade safety alignment due to issues like catastrophic forgetting. The findings underscore a critical gap in existing safety measures that focus only on inference-time defenses, advocating for further research and development of robust safety protocols for the fine-tuning phase. The authors also propose various mitigation strategies to address these emerging risks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=hTEGyKf0dZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=hTEGyKf0dZ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Finetuning Text-to-Image Diffusion Models for Fairness</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">fairness</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">debiasing</span>
<span class="badge bg-primary">text-to-image</span>
<span class="badge bg-primary">distributional alignment</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This work presents a novel approach to mitigate biases in text-to-image diffusion models by framing fairness as a distributional alignment problem, achieving significant reductions in gender and racial biases across varied prompts.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the pressing issue of biases in text-to-image (T2I) diffusion models, arguing that these biases can distort societal views and limit opportunities for marginalized groups. The authors propose a dual-method approach: a distributional alignment loss that adjusts the characteristics of generated images towards user-defined target distributions and an adjusted direct finetuning technique that effectively optimizes the image generation process. Empirical results demonstrate that their method significantly reduces gender, racial, and intersectional biases while allowing for flexible control over the representation of various attributes, such as age distribution. The study underscores the importance of social alignment in multimedia generative AI and offers practical tools for achieving fairness in AI-generated content.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=hnrB5YHoYu&name=pdf" class="link-primary">https://openreview.net/attachment?id=hnrB5YHoYu&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Flow Matching on General Geometries</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Riemannian Flow Matching</span>
<span class="badge bg-primary">Continuous Normalizing Flows</span>
<span class="badge bg-primary">Manifolds</span>
<span class="badge bg-primary">Pre-metrics</span>
<span class="badge bg-primary">Generative Modeling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Riemannian Flow Matching (RFM), a novel framework for training continuous normalizing flows on general geometries that is simulation-free, avoids divergence computations, and effectively employs premetrics for training on diverse datasets.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Riemannian Flow Matching (RFM), a framework designed to improve the training of continuous normalizing flows on Riemannian manifolds. By directly employing a premetric to define target vector fields, RFM circumvents common challenges in generative modeling, such as the need for simulation and costly divergence computations. Notably, RFM operates efficiently on both simple and complex geometries, leveraging exact geodesic computations for straightforward cases and spectral distances for more intricate scenarios. The authors demonstrate RFM's state-of-the-art performance across various real-world non-Euclidean datasets, showcasing its scalability to high dimensions and applicability to complex geometries, including those with boundaries, thus pushing forward the capabilities of deep generative models on manifolds.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=g7ohDlTITL&name=pdf" class="link-primary">https://openreview.net/attachment?id=g7ohDlTITL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">gene regulatory networks</span>
<span class="badge bg-primary">dropout mechanisms</span>
<span class="badge bg-primary">causal inference</span>
<span class="badge bg-primary">RNA sequencing</span>
<span class="badge bg-primary">conditional independence</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a causal graphical model to address dropout issues in gene regulatory network inference, demonstrating that conditional independence relations can be reliably estimated by deleting zero-value samples in scRNA-seq data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper tackles the challenge of gene regulatory network inference (GRNI) in single-cell RNA sequencing data, which are often affected by dropout errors that produce zeros in gene expression measurements. To address this, the authors introduce a Causal Dropout Model which characterizes the dropout mechanism and establishes that the conditional independence relations derived from the data after excluding samples with zero values are asymptotically equivalent to those from the unaltered dataset. This deletion-based approach can be integrated into existing causal discovery frameworks and has been empirically validated through synthetic, curated, and real-world transcriptomic datasets, showcasing its effectiveness over traditional methods, particularly in preserving accurate causal relationships while mitigating biases introduced by dropouts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gFR4QwK53h&name=pdf" class="link-primary">https://openreview.net/attachment?id=gFR4QwK53h&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Generalization in diffusion models arises from geometry-adaptive harmonic representations</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">deep neural networks</span>
<span class="badge bg-primary">generalization</span>
<span class="badge bg-primary">inductive biases</span>
<span class="badge bg-primary">image denoising</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that deep neural networks (DNNs) trained for image denoising exhibit strong generalization properties and converge to a common score function when trained on large datasets, aided by geometry-adaptive harmonic representations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the generalization capabilities of deep neural networks (DNNs) trained for image denoising, especially in the context of diffusion models. It identifies that when trained on sufficiently large and diverse datasets, DNNs learn nearly identical score functions, leading to the generation of distinct and high-quality samples that are independent of the specific training images. The authors analyze the learned denoising functions to reveal that the DNNs perform a shrinkage operation in bases that adapt to the geometry of the images, termed geometry-adaptive harmonic bases (GAHBs). This suggests that DNNs have strong inductive biases that align well with the true distribution of photographic images, facilitating effective generalization. The study illustrates that while DNN denoisers achieve near-optimal performance for certain classes of images, they may struggle with datasets defined on low-dimensional manifolds where their biases are not fully aligned, leading to suboptimal results.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ANvmVS2Yr0&name=pdf" class="link-primary">https://openreview.net/attachment?id=ANvmVS2Yr0&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Generative Modeling with Phase Stochastic Bridge</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">generative modeling</span>
<span class="badge bg-primary">phase space dynamics</span>
<span class="badge bg-primary">stochastic optimal control</span>
<span class="badge bg-primary">sampling efficiency</span>
<span class="badge bg-primary">Accelerated Generative Model</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents the Accelerated Generative Model (AGM), which employs phase space dynamics and stochastic optimal control to enhance sampling efficiency in generative modeling.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces a new framework for generative modeling called the Accelerated Generative Model (AGM), which is based on the principles of phase space dynamics and stochastic optimal control. It aims to improve sampling efficiency by producing straighter trajectories in momentum systems and by incorporating velocity information to enable early estimation of data points—a technique termed "sampling-hop." The experimental results show that AGM achieves competitive performance in image generation tasks, outperforming existing models, particularly under low function evaluation settings. This indicates its potential as a robust alternative to traditional generative models like diffusion models while highlighting possible avenues for further enhancement in scenarios with more abundant function evaluations.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tUtGjQEDd4&name=pdf" class="link-primary">https://openreview.net/attachment?id=tUtGjQEDd4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Ghost on the Shell: An Expressive Representation of General 3D Shapes</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D shapes</span>
<span class="badge bg-primary">mesh representation</span>
<span class="badge bg-primary">manifold signed distance field</span>
<span class="badge bg-primary">generative modeling</span>
<span class="badge bg-primary">reconstruction</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Ghost-on-the-Shell (G-S HELL), a novel representation for both watertight and non-watertight 3D meshes using manifold signed distance fields, which improves mesh reconstruction and generative capabilities.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Ghost-on-the-Shell (G-S HELL), a differentiable representation designed to effectively model a wide range of 3D shapes, including both watertight and non-watertight meshes. By defining a manifold signed distance field (mSDF) that characterizes the boundaries of open surfaces on watertight templates, G-S HELL allows for efficient mesh reconstruction from multi-view images and enables generative modeling of 3D shapes. The authors demonstrate that G-S HELL outperforms existing methods in tasks involving reconstruction and unconditional generation of meshes. Their approach facilitates the joint optimization of topology, material, and lighting, advancing the capabilities of 3D shape representation in practical applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Ad87VjRqUw&name=pdf" class="link-primary">https://openreview.net/attachment?id=Ad87VjRqUw&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">graph neural networks</span>
<span class="badge bg-primary">adversarial robustness</span>
<span class="badge bg-primary">certified defense</span>
<span class="badge bg-primary">graph classification</span>
<span class="badge bg-primary">perturbation resilience</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">GNNCert is a certified defense method for graph neural networks that provides deterministic robustness guarantees against both graph structure and node feature perturbations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces GNNCert, a novel certified defense for graph classification that addresses vulnerabilities to adversarial perturbations affecting graph structures and node features. The method involves dividing a testing graph into sub-graphs using a hash function and then employing a base graph classifier to predict labels for these sub-graphs, ultimately using majority voting for the final prediction. Unlike existing approaches, GNNCert offers deterministic robustness guarantees, demonstrating significantly improved certified accuracy across multiple benchmark datasets. The authors validate the effectiveness and efficiency of GNNCert, asserting its potential for broader applications in safety-critical contexts where adversarial resilience is essential.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=IGzaH538fz&name=pdf" class="link-primary">https://openreview.net/attachment?id=IGzaH538fz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Graph Neural Networks for Learning Equivariant Representations of Neural Networks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">graph neural networks</span>
<span class="badge bg-primary">neural networks</span>
<span class="badge bg-primary">permutation symmetry</span>
<span class="badge bg-primary">heterogeneous architectures</span>
<span class="badge bg-primary">generalization performance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a neural graph representation for neural networks that preserves permutation symmetry, enabling effective learning and generalization across diverse architectures.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of designing neural networks that process the parameters of other neural networks by proposing a novel neural graph representation, which captures both the architecture and parameters of the original network while preserving permutation symmetry. This representation allows existing graph neural networks (GNNs) and transformers to process heterogeneous architectures, making it feasible to analyze a variety of neural networks with differing structures. The authors demonstrate the effectiveness of their approach on multiple tasks, including classification of implicit neural representations and predicting generalization performance, achieving substantial improvements over state-of-the-art methods. The results highlight the potential of their model for various applications in the field of geometric deep learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=oO6FsMyDBt&name=pdf" class="link-primary">https://openreview.net/attachment?id=oO6FsMyDBt&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">noise prior</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">temporal coherence</span>
<span class="badge bg-primary">video generation</span>
<span class="badge bg-primary">noise transport</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel noise representation called R-noise that preserves temporal correlations in diffusion models for video editing and generation, improving quality by reducing flickering and texture-sticking artifacts.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of existing diffusion-based video processing methods, which struggle to maintain temporal coherence due to ineffective noise sampling techniques. It introduces a new noise representation, R-noise, which interprets noise samples as a continuously integrated noise field, thereby allowing for accurate transport of noise samples across frames while preserving their correlations and properties. The authors derive a noise transport equation and implement a practical algorithm to apply this representation in various video tasks, such as video restoration and conditional generation. Experimental results demonstrate substantial improvements over traditional noise priors, showcasing the effectiveness of R-noise in achieving temporally coherent and visually high-quality video outputs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pzElnMrgSD&name=pdf" class="link-primary">https://openreview.net/attachment?id=pzElnMrgSD&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D medical imaging</span>
<span class="badge bg-primary">transfer learning</span>
<span class="badge bg-primary">supervised pre-training</span>
<span class="badge bg-primary">AbdomenAtlas 1.1</span>
<span class="badge bg-primary">segmentation performance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents AbdomenAtlas 1.1, a large, annotated dataset for 3D medical imaging, and demonstrates that models pre-trained on this dataset significantly outperform existing pre-trained models in transfer learning for image segmentation tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study investigates the effectiveness of supervised pre-training for 3D image segmentation by creating AbdomenAtlas 1.1, a dataset comprising 9,262 CT volumes with detailed voxel-level annotations for 25 anatomical structures and pseudo annotations for tumor types. The authors develop a suite of supervised pre-trained models (SuPreM) that leverage this extensive dataset and showcase significant efficiency benefits in transfer learning compared to self-supervised methods; for example, models trained with only 21 CT volumes can achieve similar performance as those trained on much larger datasets. Their findings indicate that the supervised pre-training approach enhances model transferability across different medical imaging tasks, and the released pre-trained models are expected to boost advancements in 3D segmentation capabilities in the medical domain.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=AhizIPytk4&name=pdf" class="link-primary">https://openreview.net/attachment?id=AhizIPytk4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improved Active Learning via Dependent Leverage Score Sampling</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">active learning</span>
<span class="badge bg-primary">leverage score sampling</span>
<span class="badge bg-primary">pivotal sampling</span>
<span class="badge bg-primary">polynomial regression</span>
<span class="badge bg-primary">agnostic learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a pivotal sampling method that integrates spatial awareness with leverage score sampling, achieving improved sample efficiency in active learning problems, particularly in polynomial regression settings.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This research addresses the active linear regression problem within the agnostic learning setting, proposing a novel pivotal sampling technique that enhances traditional leverage score sampling by ensuring spatial coverage of samples. The authors demonstrate that this method can reduce the number of samples required to achieve a specified accuracy by up to 50% compared to independent sampling methods. The approach is theoretically supported by two main results: one that matches the sampling complexity of independent leverage score sampling and another that provides improved bounds for polynomial regression. Through empirical tests on various problems, including those motivated by parametric PDEs, the pivotal sampling method outperformed standard Bernoulli leverage score sampling, showcasing its practical effectiveness in approximating complex functions. This work suggests a promising direction for combining theoretical rigor with practical sampling strategies in active learning contexts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=IYxDy2jDFL&name=pdf" class="link-primary">https://openreview.net/attachment?id=IYxDy2jDFL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improved Techniques for Training Consistency Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">consistency models</span>
<span class="badge bg-primary">training techniques</span>
<span class="badge bg-primary">generative models</span>
<span class="badge bg-primary">Pseudo-Huber loss</span>
<span class="badge bg-primary">sample quality</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents significant improvements in training consistency models for generative tasks, surpassing prior methods and achieving state-of-the-art performance without relying on adversarial training or learned metrics.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper focuses on enhancing consistency training (CT) techniques for generative models, aiming to match or exceed the performance of consistency distillation (CD) while avoiding its computational overhead and quality limitations. The authors identify and address a flaw in previous theoretical analyses by eliminating Exponential Moving Average from the teacher model and introducing robust Pseudo-Huber loss functions. They propose an innovative curriculum for total discretization steps and a lognormal noise schedule, resulting in improved sample quality. Through extensive experiments, the models achieve Fréchet Inception Distance (FID) scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64x64, respectively, significantly outperforming prior methods. These results illustrate the potential of consistency models as a promising independent family of generative models, rivaling other state-of-the-art approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=WNzy9bRDvG&name=pdf" class="link-primary">https://openreview.net/attachment?id=WNzy9bRDvG&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improving Convergence and Generalization Using Parameter Symmetries</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">teleportation</span>
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">parameter symmetries</span>
<span class="badge bg-primary">generalization</span>
<span class="badge bg-primary">neural networks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that incorporating parameter space symmetries through a method called "teleportation" enhances the convergence speed of optimization algorithms and improves the generalization of neural networks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The research investigates the role of parameter space symmetries in deep neural networks, specifically through a technique known as teleportation, which accelerates optimization by moving parameters to steeper points in loss landscapes. The authors provide theoretical guarantees that teleportation boosts convergence rates in stochastic gradient descent (SGD) and can lead to optimal trajectories for certain loss functions. Additionally, the paper explores the relationship between the curvature of minima and generalization, revealing that teleporting to regions with different curvature can enhance a model's ability to generalize. Various optimization algorithms are shown to benefit from integrating teleportation, highlighting its versatility and potential applications in improving convergence and generalization in neural network training.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=L0r0GphlIL&name=pdf" class="link-primary">https://openreview.net/attachment?id=L0r0GphlIL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">data pruning</span>
<span class="badge bg-primary">lossless performance</span>
<span class="badge bg-primary">training acceleration</span>
<span class="badge bg-primary">gradient expectation</span>
<span class="badge bg-primary">unbiased framework</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces InfoBatch, a framework that leverages unbiased dynamic data pruning to achieve lossless training acceleration across various deep learning tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents InfoBatch, a novel data pruning method designed to accelerate training without sacrificing performance. It addresses the issue of gradient expectation bias associated with traditional static and dynamic pruning approaches by employing a soft pruning strategy that randomly discards less informative samples while rescaling the gradients of the remaining data. This approach allows InfoBatch to maintain a similar gradient expectation to that of training on the full dataset. The framework is versatile and achieves lossless performance with 20% to 40% less overall cost across multiple tasks, including classification and semantic segmentation, as demonstrated through experiments on datasets like CIFAR and ImageNet. The authors conclude that InfoBatch significantly enhances training efficiency while minimizing computational overhead, making it suitable for real-world applications in deep learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=C61sk5LsK6&name=pdf" class="link-primary">https://openreview.net/attachment?id=C61sk5LsK6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Interpreting CLIP's Image Representation via Text-Based Decomposition</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">CLIP</span>
<span class="badge bg-primary">image representation</span>
<span class="badge bg-primary">attention heads</span>
<span class="badge bg-primary">text-based decomposition</span>
<span class="badge bg-primary">zero-shot segmentation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper analyzes CLIP's image representation by decomposing it into contributions from individual layers, attention heads, and image patches, revealing specialized roles and enabling performance improvements in zero-shot image segmentation and spurious feature reduction.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the internal mechanisms of CLIP's image encoder by dissecting its representation into components influenced by specific layers, attention heads, and image locations. Utilizing a method called TEXTSPAN, the authors label and interpret attention heads based on their output characteristics, identifying property-specific functions like color and shape recognition. The study reveals that focusing on certain layers and heads significantly affects classification accuracy, leading to enhanced performance in zero-shot segmentation tasks and mitigation of spurious correlations in outputs. The findings suggest that a deeper understanding of transformer models like CLIP can facilitate improvements in their effectiveness for various downstream applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=5Ca9sSzuDp&name=pdf" class="link-primary">https://openreview.net/attachment?id=5Ca9sSzuDp&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">image encoding</span>
<span class="badge bg-primary">video data</span>
<span class="badge bg-primary">DORA</span>
<span class="badge bg-primary">Walking Tours dataset</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Walking Tours dataset and a novel self-supervised learning method called DORA, demonstrating that training on a single lengthy, unlabeled video can yield performance competitive with traditional ImageNet pretraining for various image and video tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the efficiency of data usage in self-supervised learning by introducing a new dataset, Walking Tours, comprising high-resolution, continuously captured first-person videos. It presents a self-supervised image pretraining method named DORA, which focuses on tracking and recognizing objects across video frames using transformer cross-attention mechanisms. The results show that the DORA method enables competitive performance with ImageNet-pretrained models by using just a single Walking Tours video for training. The findings suggest a shift towards more effective use of video data in representation learning, potentially accelerating the training process compared to conventional methods reliant on large-scale image datasets.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Yen1lGns2o&name=pdf" class="link-primary">https://openreview.net/attachment?id=Yen1lGns2o&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">knowledge cards</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">modular frameworks</span>
<span class="badge bg-primary">knowledge integration</span>
<span class="badge bg-primary">factuality</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces KNOWLEDGE CARD, a framework that enhances large language models (LLMs) by integrating specialized, modular knowledge cards, enabling dynamic knowledge synthesis and updates.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the KNOWLEDGE CARD framework, which addresses the limitations of static large language models (LLMs) in knowledge-intensive tasks by incorporating modular knowledge cards—specialized language models trained on specific domains. The framework enables flexible knowledge integration through two main approaches: bottom-up and top-down—allowing LLMs to dynamically access relevant information while controlling for relevance, brevity, and factuality via three implemented selectors. Extensive experiments demonstrate KNOWLEDGE CARD's superior performance over standard LLMs and existing retrieval-augmented models across multiple tasks, significantly improving responses, especially in areas requiring current knowledge updates. The approach fosters community-driven contributions to LLM knowledge enhancement and aims to lower the carbon footprint associated with retraining large models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=WbWtOYIzIK&name=pdf" class="link-primary">https://openreview.net/attachment?id=WbWtOYIzIK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Evolving Domain Generalization</span>
<span class="badge bg-primary">Stochastic Differential Equations</span>
<span class="badge bg-primary">Infinitely Fined-Grid Evolving Trajectory</span>
<span class="badge bg-primary">Distribution Shift</span>
<span class="badge bg-primary">Machine Learning.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes SDE-EDG, a novel approach for Evolving Domain Generalization that utilizes Stochastic Differential Equations and continuous interpolations to better model evolving patterns in data under distribution shifts over time.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study addresses the challenges of Evolving Domain Generalization (EDG) in machine learning, particularly in scenarios with limited timestamps leading to overfitting and poor generalization. The authors introduce a new method, SDE-EDG, which constructs an Infinitely Fined-Grid Evolving Trajectory (IFGET) using sample-to-sample correspondence and continuous interpolation techniques to bridge temporal gaps. By leveraging Stochastic Differential Equations, SDE-EDG models the evolving dynamics of latent representations, enhancing the model's ability to capture distribution shifts over time. Evaluations on benchmark datasets demonstrate that SDE-EDG significantly outperforms existing state-of-the-art methods, highlighting its effectiveness in generalizing to unseen domains. The approach is further supported by theoretical analysis showing its potential to mitigate generalization risks related to time-varying environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bTMMNT7IdW&name=pdf" class="link-primary">https://openreview.net/attachment?id=bTMMNT7IdW&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning Energy Decompositions for Partial Inference in GFlowNets</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">GFlowNets</span>
<span class="badge bg-primary">energy decomposition</span>
<span class="badge bg-primary">partial inference</span>
<span class="badge bg-primary">potential functions</span>
<span class="badge bg-primary">training algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Learning Energy Decompositions for GFlowNets (LED-GFN), enhancing the sampling efficiency from the Boltzmann distribution by employing learnable potential functions for improving credit assignment during training.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the use of Generative Flow Networks (GFlowNets) for sampling objects from a Boltzmann energy distribution through sequences of actions, with a focus on enhancing training via partial inference. The authors propose LED-GFN, which overcomes the limitations of previous methods by decomposing terminal state energy into learnable potential functions associated with state transitions, thus facilitating better local credit assignments. Empirical evaluations across various tasks, including bag generation, molecular discovery, and RNA sequence generation, demonstrate that LED-GFN consistently outperforms existing approaches, even when traditional methods have access to ideal local credits, highlighting its effectiveness and practicality in diverse sampling applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=P15CHILQlg&name=pdf" class="link-primary">https://openreview.net/attachment?id=P15CHILQlg&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning Interactive Real-World Simulators</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">universal simulator</span>
<span class="badge bg-primary">generative modeling</span>
<span class="badge bg-primary">real-world interactions</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">vision-language policy</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a universal simulator (UniSim) that integrates diverse datasets to simulate realistic experiences of real-world interactions, enabling the training of both vision-language and low-level control policies for deployment in actual scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the development of a universal simulator (UniSim) capable of generating realistic visual outcomes in response to various human and robotic actions, leveraging generative modeling techniques. By orchestrating a wide range of datasets, which include textual, visual, and behavioral information, the simulator is designed to facilitate interactive learning and task execution. The authors demonstrate UniSim's effectiveness in training high-level vision-language policies and low-level reinforcement learning agents, showing that these agents can generalize to real-world applications after training purely in simulation. The findings suggest broader implications for the use of such simulators in improving machine intelligence, making them applicable in fields ranging from robotics to content creation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=sFyTZEqmUY&name=pdf" class="link-primary">https://openreview.net/attachment?id=sFyTZEqmUY&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LEGO-Prover: Neural Theorem Proving with Growing Libraries</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">neural theorem proving</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">skill library</span>
<span class="badge bg-primary">modular proof construction</span>
<span class="badge bg-primary">formalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents LEGO-Prover, a novel approach to neural theorem proving that utilizes a growing skill library to enhance the capability of large language models in formalizing mathematical proofs.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces LEGO-Prover, an innovative method for neural theorem proving that addresses the challenges faced by existing models due to their reliance on static libraries. By employing a growing library of verified lemmas as modular skills, LEGO-Prover allows for the construction of proofs in a block-by-block manner, enabling the model to retrieve existing skills and create new ones during the proving process. The system demonstrated significant advancements in success rates on the miniF2F dataset, achieving a pass rate of 57.0% on the validation set and 50.0% on the test set, surpassing previous state-of-the-art approaches. The findings suggest that the dynamically expanding skill library enhances the ability of language models to tackle increasingly complex mathematical problems and bridge the gap between informal and formal proofs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3f5PALef5B&name=pdf" class="link-primary">https://openreview.net/attachment?id=3f5PALef5B&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Less is More: Fewer Interpretable Region via Submodular Subset Selection</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">image attribution</span>
<span class="badge bg-primary">submodular optimization</span>
<span class="badge bg-primary">interpretability</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">explanation methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel method for image attribution based on submodular subset selection, improving interpretability with fewer, more accurate regions and addressing both correct and incorrect model predictions.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses challenges in existing image attribution methods, specifically the tendency to generate imprecise attribution regions and difficulties in interpreting incorrect model predictions. It reformulates the image attribution problem as a submodular subset selection task, employing a new submodular function that considers prediction confidence, effectiveness, consistency, and collaboration of sub-regions. Extensive experiments on two face datasets (Celeb-A and VGG-Face2) and a fine-grained dataset (CUB-200-2011) demonstrate significant performance improvements in Deletion and Insertion AUC scores, with the proposed method outperforming state-of-the-art approaches, and effectively identifying causes behind misclassifications. This work enhances the interpretability of machine learning models while illustrating the importance of local region analysis in attribution tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jKTUlxo5zy&name=pdf" class="link-primary">https://openreview.net/attachment?id=jKTUlxo5zy&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Lipschitz Singularities in Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Diffusion models</span>
<span class="badge bg-primary">Lipschitz singularities</span>
<span class="badge bg-primary">E-TSDM</span>
<span class="badge bg-primary">stability</span>
<span class="badge bg-primary">image synthesis</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper addresses the issue of infinite Lipschitz constants near the zero point in diffusion models and proposes a novel method, E-TSDM, to mitigate these singularities, leading to significant performance improvements in image synthesis tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the problem of infinite Lipschitz constants in diffusion models, particularly near the zero time point, which negatively impacts the models' stability and accuracy during training and inference. The authors provide both theoretical proofs and empirical evidence for the presence of Lipschitz singularities and propose the Early Timestep-shared Diffusion Model (E-TSDM), which alleviates these issues by sharing timestep conditions in intervals with high Lipschitz constants. Extensive experiments across various datasets demonstrate that E-TSDM significantly enhances performance, reduces Fréchet Inception Distance in image generation tasks by over 33%, and maintains adaptability across different noise schedules and fast sampling methods. The findings advance understanding of diffusion processes and highlight potential pathways for further model improvements.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=WNkW0cOwiz&name=pdf" class="link-primary">https://openreview.net/attachment?id=WNkW0cOwiz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">carbon footprint</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">LLMCarbon</span>
<span class="badge bg-primary">operational emissions</span>
<span class="badge bg-primary">embodied emissions</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces LLMCarbon, a comprehensive model for accurately predicting the carbon footprint of large language models (LLMs) during their training, inference, experimentation, and storage phases, addressing gaps in existing tools.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents LLMCarbon, an end-to-end carbon footprint projection model specifically designed for large language models (LLMs), both dense and mixture-of-experts (MoE) architectures. The model incorporates essential LLM, hardware, and data center parameters to estimate both operational and embodied carbon emissions throughout the LLM's lifecycle. Compared to existing tools like mlco2, LLMCarbon demonstrates improved accuracy in carbon footprint estimations, with validation results showing discrepancies of only 8.2% from actual data. The findings emphasize the significance of considering both operational efficiency and embodied carbon in the assessment of LLMs' environmental impacts, thereby supporting more sustainable AI practices moving forward.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aIok3ZD9to&name=pdf" class="link-primary">https://openreview.net/attachment?id=aIok3ZD9to&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">LoRA</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">performance improvement</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents LoftQ, a novel quantization framework that improves the performance of quantized large language models through LoRA fine-tuning by simultaneously mitigating the discrepancies introduced by quantization.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors introduce LoftQ, a novel quantization framework designed to effectively combine quantization and LoRA fine-tuning in large language models (LLMs). Recognizing the performance gaps often observed when using low-bit quantization methods in conjunction with LoRA, LoftQ aims to provide a more suitable low-rank initialization that reduces discrepancies between quantized and full-precision models. The technique involves alternating between quantization and low-rank approximation to better approximate high-precision weights. Extensive experiments demonstrate that LoftQ significantly outperforms existing quantization approaches like QLoRA across various natural language understanding, question answering, summarization, and generation tasks, showcasing robust performance particularly in low-bit quantization scenarios such as 2-bit precision.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LzPWWPAdY4&name=pdf" class="link-primary">https://openreview.net/attachment?id=LzPWWPAdY4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LongLoRA</span>
<span class="badge bg-primary">context extension</span>
<span class="badge bg-primary">sparse attention</span>
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">large language models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">LongLoRA is an efficient fine-tuning method that enables large language models to extend their context lengths significantly with reduced computational costs and minimal accuracy loss.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents LongLoRA, a novel approach designed to efficiently extend the context lengths of pre-trained large language models (LLMs) while minimizing computational expenses. The authors introduce Shifted Sparse Attention (S2-Attn) to enhance context extension using sparse local attention during fine-tuning without altering the global dense attention required during inference. They also improve the parameter-efficient fine-tuning technique Low-Rank Adaptation (LoRA) by making embedding and normalization layers trainable, which effectively bridges the performance gap between conventional LoRA and full fine-tuning. Empirical results demonstrate that LongLoRA successfully fine-tunes LLaMA models, extending the context from shorter lengths to up to 100k tokens for LLaMA 7B models on a single 8 A100 machine, while retaining significant performance. The work also highlights compatibility with existing optimization techniques and outlines future research directions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6PmJoRfdaK&name=pdf" class="link-primary">https://openreview.net/attachment?id=6PmJoRfdaK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LRM: Large Reconstruction Model for Single Image to 3D</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">3D Reconstruction</span>
<span class="badge bg-primary">Neural Radiance Fields</span>
<span class="badge bg-primary">Transformer Architecture</span>
<span class="badge bg-primary">Image-to-3D</span>
<span class="badge bg-primary">Large-Scale Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces the Large Reconstruction Model (LRM), a transformer-based framework that efficiently generates high-quality 3D shapes from single images within five seconds, leveraging a large-scale dataset of nearly one million images and 3D objects.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents LRM, the first large-scale system for reconstructing 3D models from single images using a scalable transformer-based architecture with 500 million parameters. Unlike preceding methods that required category-specific training on smaller datasets, LRM is trained end-to-end on a diverse dataset combining synthetic and real images to create a generalizable 3D prior. It employs a visual transformer to encode image features and a decoder that maps these features to a triplane representation, which is then used to render 3D shapes. The model demonstrates high fidelity in reconstructing complex geometries and textures from various input types, including real-world images and those generated by other models, thus offering significant potential for applications in design and augmented reality.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=sllU8vvsFF&name=pdf" class="link-primary">https://openreview.net/attachment?id=sllU8vvsFF&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Mastering Memory Tasks with World Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">model-based reinforcement learning</span>
<span class="badge bg-primary">long-term memory</span>
<span class="badge bg-primary">state space models</span>
<span class="badge bg-primary">credit assignment</span>
<span class="badge bg-primary">world models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Recall to Imagine (R2I), a novel approach that integrates a modified state space model into model-based reinforcement learning to enhance long-term memory and improve credit assignment in various complex tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents R2I, a new method that enhances model-based reinforcement learning (MBRL) agents' ability to handle long-term dependencies by integrating a variant of state space models (SSMs) within the world models of these agents. R2I demonstrates superior performance in memory-intensive tasks like BSuite and POPGym, achieving state-of-the-art results, including surpassing human performance in the Memory Maze benchmark. The method maintains competitive performance in conventional RL environments like Atari and DMC while significantly improving computational speed, allowing faster convergence than previous state-of-the-art methods. By systematically demonstrating R2I's capabilities across various domains, the authors emphasize its potential for improved long-term memory and credit assignment in reinforcement learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=1vDArHJ68h&name=pdf" class="link-primary">https://openreview.net/attachment?id=1vDArHJ68h&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">mathematical reasoning</span>
<span class="badge bg-primary">foundation models</span>
<span class="badge bg-primary">visual contexts</span>
<span class="badge bg-primary">benchmark evaluation</span>
<span class="badge bg-primary">MATHVISTA</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces MATHVISTA, a benchmark for evaluating the mathematical reasoning capabilities of foundation models in visual contexts, revealing significant performance gaps compared to humans and highlighting the strengths of the GPT-4V model.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents MATHVISTA, a benchmark created to systematically assess the mathematical reasoning abilities of large language models (LLMs) and large multimodal models (LMMs) in visual contexts. The dataset comprises 6,141 examples derived from 28 existing multimodal datasets and three newly created datasets, covering various mathematical reasoning types and visual contexts. A comprehensive evaluation of 12 foundation models, including GPT-4V and Bard, reveals that while GPT-4V achieved the highest performance at 49.9%, it still falls short of human performance by 10.4%. The findings underscore the ongoing challenges in enabling AI models to effectively integrate visual comprehension with rigorous mathematical reasoning and highlight the potential for further advancements in this domain.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=KUNzEQMWU7&name=pdf" class="link-primary">https://openreview.net/attachment?id=KUNzEQMWU7&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">continual learning</span>
<span class="badge bg-primary">Meta-Continual Learning</span>
<span class="badge bg-primary">Hessian approximation</span>
<span class="badge bg-primary">variance reduction</span>
<span class="badge bg-primary">online learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Variance-Reduced Meta-CL (VR-MCL), a new method that combines the principles of Meta-Continual Learning and regularization-based approaches to enhance Hessian approximation while reducing variance, significantly improving performance in continual learning tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper revisits the field of continual learning (CL) by bridging regularization-based methods with Meta-Continual Learning (Meta-CL). It highlights the limitations of existing Hessian approximations used in regularization methods, which remain fixed during training, and discusses how Meta-CL leverages hypergradient information for more timely Hessian updates but suffers from high variance due to sampling inconsistencies. To address these challenges, the authors propose Variance-Reduced Meta-CL (VR-MCL), which effectively reduces hypergradient variance through a momentum-based technique, concurrently providing a more accurate approximation of the Hessian. Comprehensive experiments demonstrate that VR-MCL outperforms state-of-the-art CL methods across various datasets and settings, while theoretical analysis reinforces its efficacy and provides a regret bound, confirming VR-MCL's superiority in maintaining knowledge over sequential tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=TpD2aG1h0D&name=pdf" class="link-primary">https://openreview.net/attachment?id=TpD2aG1h0D&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">meta-programming</span>
<span class="badge bg-primary">multi-agent systems</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">software engineering</span>
<span class="badge bg-primary">standardized operating procedures</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces MetaGPT, a meta-programming framework that improves multi-agent collaboration and code generation quality through structured workflows and executable feedback mechanisms.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents MetaGPT, a meta-programming framework designed to enhance the capabilities of multi-agent systems utilizing large language models (LLMs) by integrating Standardized Operating Procedures (SOPs) into workflows. The framework assigns specialized roles to agents, allowing them to collaboratively tackle complex software engineering tasks more efficiently. MetaGPT employs structured communication protocols and incorporates an executable feedback mechanism that iteratively improves code quality. Experimental evaluations demonstrate that MetaGPT achieves state-of-the-art performance on various coding benchmarks, highlighting its robustness and suitability for automating complex problem-solving processes in software development.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=VtmBAGCN7o&name=pdf" class="link-primary">https://openreview.net/attachment?id=VtmBAGCN7o&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">METRA: Scalable Unsupervised RL with Metric-Aware Abstraction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Unsupervised Reinforcement Learning</span>
<span class="badge bg-primary">Metric-Aware Abstraction</span>
<span class="badge bg-primary">Skill Discovery</span>
<span class="badge bg-primary">High-Dimensional Environments</span>
<span class="badge bg-primary">Temporal Distances</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces METRA, a scalable unsupervised reinforcement learning method that achieves diverse skill discovery in complex, high-dimensional environments by utilizing a metric-aware objective based on temporal distances.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents METRA, an innovative unsupervised reinforcement learning (RL) framework designed to effectively explore and learn diverse behaviors in complex, high-dimensional environments without supervision. Traditional unsupervised RL approaches, such as pure exploration and mutual information skill discovery, struggle to scale in such settings due to the challenges of covering large state spaces or incentivizing effective exploration. METRA overcomes these limitations by focusing on a compact latent space connected to the state space through temporal distances, maximizing coverage and discovering a variety of useful skills. Experiments across five locomotion and manipulation environments demonstrate that METRA successfully identifies diverse behaviors, outperforming existing unsupervised RL methods and providing a foundation for efficient learning in downstream tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=c5pwL0Soay&name=pdf" class="link-primary">https://openreview.net/attachment?id=c5pwL0Soay&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">tabular data synthesis</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">variational autoencoder</span>
<span class="badge bg-primary">synthetic data quality</span>
<span class="badge bg-primary">mixed data types</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces TABSYN, an advanced methodology for synthesizing tabular data using a diffusion model within a variational autoencoder framework, achieving significant improvements in data quality and generation speed over existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents TABSYN, a novel approach for generating synthetic tabular data that effectively handles mixed data types by first transforming raw tabular data into a continuous embedding space and then applying a score-based diffusion model in this latent representation. Key innovations of TABSYN include the integration of a variational autoencoder with a transformer architecture to capture inter-column relationships and adaptive loss weighting to optimize the latent space distribution. Experiments across six datasets show that TABSYN outperforms state-of-the-art methods, reducing error rates by 86% for column-wise density estimation and 67% for pair-wise correlations, while also demonstrating efficient performance on downstream tasks like machine learning and missing value imputation. The results highlight the potential of TABSYN to generate high-quality synthetic data that captures the nuances of real-world tabular datasets.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4Ay23yeuz0&name=pdf" class="link-primary">https://openreview.net/attachment?id=4Ay23yeuz0&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adaptive KV cache compression</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">generative inference</span>
<span class="badge bg-primary">memory efficiency</span>
<span class="badge bg-primary">attention profiling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces FastGen, an adaptive method for compressing the KV cache in large language models that significantly reduces memory usage without sacrificing generation quality.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents FastGen, a novel approach for adaptive KV cache compression aimed at improving the memory efficiency during generative inference of large language models (LLMs). By leveraging lightweight attention profiling to identify the unique structures of various attention heads, FastGen selectively evicts less relevant tokens from the cache based on the recognized patterns. The proposed method demonstrates a substantial reduction in GPU memory consumption, achieving over 95% recovery of attention scores while compressing the cache by up to 50%, all without the need for resource-intensive fine-tuning or retraining. Experimental results across multiple tasks show that FastGen outperforms traditional fixed KV cache methods, highlighting its potential for practical implementations in LLMs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uNrFpDPMyo&name=pdf" class="link-primary">https://openreview.net/attachment?id=uNrFpDPMyo&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Bayesian inference</span>
<span class="badge bg-primary">linear inverse problems</span>
<span class="badge bg-primary">denoising diffusion models</span>
<span class="badge bg-primary">Sequential Monte Carlo</span>
<span class="badge bg-primary">uncertainty quantification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces MCGdiff, a novel Sequential Monte Carlo algorithm that samples from the Bayesian posterior of linear inverse problems with denoising diffusion model priors, demonstrating superior performance against competing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses ill-posed linear inverse problems commonly found in applications like medical imaging and computational photography by leveraging Bayesian inference with denoising diffusion models as priors. The authors propose MCGdiff, a new algorithm that sequentially estimates the posterior distribution of the unknowns by exploiting the specific structure of the prior and employing Sequential Monte Carlo methods. Theoretical foundations support the convergence of MCGdiff to the target posterior, and numerical experiments showcase its effectiveness over existing methods, confirming that MCGdiff consistently yields high-quality samples aligned with the target distribution. Overall, this work contributes to advancing approaches for robust solutions in Bayesian linear inverse problems, emphasizing the importance of accurate uncertainty quantification.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=nHESwXvxWK&name=pdf" class="link-primary">https://openreview.net/attachment?id=nHESwXvxWK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Multi-granularity Correspondence Learning from Long-term Noisy Videos</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">video-language</span>
<span class="badge bg-primary">temporal learning</span>
<span class="badge bg-primary">optimal transport</span>
<span class="badge bg-primary">noisy correspondence</span>
<span class="badge bg-primary">action segmentation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Norton, a method that utilizes noise-robust temporal optimal transport to effectively learn temporal correlations from long videos while addressing multi-granularity noisy correspondences.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of learning long-term temporal dependencies in video-language understanding, which is often overlooked due to computational limitations. The proposed method, Norton, leverages a unified optimal transport framework to tackle the multi-granularity noisy correspondence (MNC) problem, which encompasses both coarse-grained clip-caption misalignments and fine-grained frame-word misalignments. Norton employs contrastive losses, an alignable prompt bucket, and a soft-maximum operator to filter out irrelevant pairs and identify crucial words and frames. Comprehensive experiments on various video tasks, including video-paragraph retrieval and action segmentation, demonstrate that Norton outperforms existing approaches, indicating its effectiveness in capturing long-term temporal relations while mitigating alignment noise.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9Cu8MRmhq2&name=pdf" class="link-primary">https://openreview.net/attachment?id=9Cu8MRmhq2&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Multi-Source Diffusion Models for Simultaneous Music Generation and Separation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">music generation</span>
<span class="badge bg-primary">source separation</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">generative models</span>
<span class="badge bg-primary">audio processing</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a novel multi-source diffusion model that simultaneously performs music generation, source separation, and source imputation by learning the joint probability density of contextual audio sources.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This work introduces a diffusion-based generative model capable of handling multiple audio tasks, specifically music synthesis and source separation. By learning the joint distribution of instrumental sources within a musical context, the model enables various inference tasks such as total generation (creating a complete mixture), partial generation (generating missing sources given others), and source separation (isolating sources from a mixture). The proposed model, trained on the Slakh2100 dataset, utilizes an innovative Dirac likelihood-based method for source separation, achieving competitive results against state-of-the-art methods while allowing for flexible audio manipulation. This advancement highlights the potential for developing general audio models that facilitate complex music composition and source control.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=h922Qhkmx1&name=pdf" class="link-primary">https://openreview.net/attachment?id=h922Qhkmx1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Multisize Dataset Condensation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">dataset condensation</span>
<span class="badge bg-primary">on-device learning</span>
<span class="badge bg-primary">subset degradation</span>
<span class="badge bg-primary">adaptive subset loss</span>
<span class="badge bg-primary">computational efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a novel approach called Multisize Dataset Condensation (MDC) that condenses multiple dataset adjustment processes into a single one to facilitate flexible dataset sizes for on-device learning while overcoming the subset degradation problem.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of dataset condensation in on-device scenarios, where fluctuating computational resources require flexible dataset sizes and limit additional condensation operations. The authors propose the Multisize Dataset Condensation (MDC) method, which consolidates N condensation processes into one by introducing an adaptive subset loss to mitigate subset degradation—where selecting a subset from a condensed dataset often leads to lower performance than condensing directly from the full dataset. The results demonstrate that MDC outperforms traditional methods in various network architectures and datasets, achieving significant accuracy gains without the overhead of multiple condensation processes. Overall, MDC presents a more efficient and effective solution for dataset condensation in resource-constrained environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=FVhmnvqnsI&name=pdf" class="link-primary">https://openreview.net/attachment?id=FVhmnvqnsI&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Neural Fine-Tuning Search for Few-Shot Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Few-Shot Learning</span>
<span class="badge bg-primary">Neural Architecture Search</span>
<span class="badge bg-primary">Adaptation Strategies</span>
<span class="badge bg-primary">Meta-Dataset</span>
<span class="badge bg-primary">Fine-Tuning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Neural Fine-Tuning Search (NFTS), a method that utilizes neural architecture search to optimally determine which layers of pre-trained models to adapt in few-shot learning, achieving state-of-the-art performance on established benchmarks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of few-shot learning, where classifiers must rapidly adapt to new, disjoint class sets. It proposes a novel approach called Neural Fine-Tuning Search (NFTS) that employs neural architecture search (NAS) to systematically identify the most effective adaptation strategies—deciding which model layers to fine-tune and where to insert new parameters (adapters). By leveraging both residual networks and vision transformers, the authors show that NFTS outperforms existing methods on the Meta-Dataset and Meta-Album benchmarks. The study provides empirical evidence that diversely selected architectures enhance performance and offers insights into the optimal configuration for few-shot adaptation across different domains.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=T7YV5UZKBc&name=pdf" class="link-primary">https://openreview.net/attachment?id=T7YV5UZKBc&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">long-range dependencies</span>
<span class="badge bg-primary">self-pretraining</span>
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">state space models</span>
<span class="badge bg-primary">performance evaluation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that self-pretraining significantly improves the performance of Transformers and state space models on long-sequence tasks, challenging the practice of training architectures from scratch.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the overestimation of performance differences among various sequence models, notably Transformers and state space models (SSMs), when trained from random initialization. It highlights the effectiveness of self-pretraining (SPT) using denoising objectives, which allows models to leverage only downstream task data for initialization. Through comprehensive evaluations on long sequence benchmarks, particularly the Long Range Arena (LRA), the authors show that proper pretraining leads to substantial performance gains for both Transformers and SSMs, narrowing the performance gaps and even achieving state-of-the-art results for certain tasks. The study emphasizes that the inclusion of data-driven priors through pretraining is essential for reliable performance assessments and suggests that manual architectural modifications may become redundant in light of effective self-pretraining.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=PdaPky8MUn&name=pdf" class="link-primary">https://openreview.net/attachment?id=PdaPky8MUn&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Psychological Evaluation</span>
<span class="badge bg-primary">PsychoBench</span>
<span class="badge bg-primary">Psychometrics</span>
<span class="badge bg-primary">Emotional Intelligence</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces PsychoBench, a framework for evaluating the psychological portrayal of Large Language Models (LLMs) across various psychological domains using established psychometric scales.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper proposes PsychoBench, a comprehensive framework designed to assess the psychological aspects of Large Language Models (LLMs) through thirteen widely used psychometric scales categorized into personality traits, interpersonal relationships, motivational tests, and emotional abilities. The authors evaluate five prominent LLMs, including text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b, and employ a jailbreaking technique to reveal the models' intrinsic characteristics. Results indicate significant differences in psychological profiles based on model size, updates, and the application of jailbreak methods, showing LLMs generally display higher openness, conscientiousness, and emotional intelligence than average humans. This study concludes with implications for developing more empathetic and engaging AI systems and emphasizes the importance of understanding the psychological attributes of LLMs to ensure ethical and responsible deployment in society.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=H3UayAQWoE&name=pdf" class="link-primary">https://openreview.net/attachment?id=H3UayAQWoE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">On the Joint Interaction of Models, Data, and Features</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">feature learning</span>
<span class="badge bg-primary">interaction tensor</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">Generalization Disagreement Equality</span>
<span class="badge bg-primary">model data interactions</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the interaction tensor as a novel tool for analyzing feature learning in deep learning models, revealing important insights about features, their distribution in data, and the implications for generalization and model performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the theoretical understanding of feature learning in deep learning, a field that lacks comprehensive formalizations despite its empirical successes. It introduces the interaction tensor to empirically analyze how features are learned from data across different models. Through this framework, the authors observe that features follow a long-tailed distribution, with models exhibiting different behaviors based on the features they learn, and they demonstrate the connection to the Generalization Disagreement Equality (GDE) without assuming model calibration. The proposed framework successfully captures various phenomena in feature learning, offering insights into how model architectures, data distributions, and feature characteristics interact, while providing a tool for further research into feature learning and model behavior.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ze7DOLi394&name=pdf" class="link-primary">https://openreview.net/attachment?id=ze7DOLi394&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">One-shot Empirical Privacy Estimation for Federated Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">federated learning</span>
<span class="badge bg-primary">differential privacy</span>
<span class="badge bg-primary">privacy estimation</span>
<span class="badge bg-primary">Gaussian mechanism</span>
<span class="badge bg-primary">membership inference</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel one-shot empirical method for estimating privacy loss in federated learning models trained using differential privacy, demonstrating its effectiveness across various adversarial threat models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces a one-shot approach for estimating the privacy loss of deep learning models trained with differential privacy in federated learning contexts, aimed at addressing challenges related to existing auditing techniques that often require extensive retraining or specific knowledge of model and task characteristics. The proposed method involves inserting canary clients that contribute randomly generated updates during the training process, allowing the estimation of privacy loss without needing intermediate model iterates or detailed knowledge about the data. Experimental results on established federated learning benchmarks indicate that the method yields accurate estimates of privacy loss under several adversarial scenarios, showing that modest noise can significantly enhance privacy, thereby providing a practical tool for empirical privacy assessment in real-world federated learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0BqyZSWfzo&name=pdf" class="link-primary">https://openreview.net/attachment?id=0BqyZSWfzo&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">inductive reasoning</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">hypothesis refinement</span>
<span class="badge bg-primary">symbolic interpretation</span>
<span class="badge bg-primary">human cognition</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the inductive reasoning capabilities of language models (LMs) using an iterative hypothesis refinement approach, revealing LMs' strengths as hypothesis proposers but significant limitations in applying their own proposed rules.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper systematically examines the inductive reasoning capabilities of language models through a novel method called iterative hypothesis refinement, which involves generating, selecting, and refining hypotheses in a manner similar to human reasoning. The study demonstrates that while LMs can propose plausible hypotheses effectively when paired with symbolic interpreters, they often struggle to apply these rules correctly and exhibit brittleness when faced with perturbations in input examples. Through various experimental evaluations across multiple datasets, the authors highlight the paradox of LMs being proficient at proposing hypotheses but puzzlingly inadequate in their application, underscoring gaps between human and LM reasoning processes, and the need for further exploration of their behavior and robustness in inductive tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bNt7oajl2a&name=pdf" class="link-primary">https://openreview.net/attachment?id=bNt7oajl2a&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">sample efficiency</span>
<span class="badge bg-primary">goal-conditioned policy</span>
<span class="badge bg-primary">hierarchical reinforcement learning</span>
<span class="badge bg-primary">clustering</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents PTGM, a pre-training method for goal-based models that enhances sample efficiency and performance in reinforcement learning by utilizing temporal abstractions and behavior regularization.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces PTGM, a novel approach designed to improve sample efficiency in reinforcement learning (RL) by leveraging pre-training on large, task-agnostic datasets. PTGM pre-trains a goal-conditioned policy and a high-level policy that generates discrete goals to guide the low-level policy, addressing challenges associated with high-dimensional action spaces through a clustering approach. Experimental results in both robotic manipulation (Kitchen) and complex open-world settings (Minecraft) demonstrate PTGM's superior performance compared to various baselines, showcasing its effectiveness in enhancing interpretability and generalization of learned skills while contributing to more efficient exploration in downstream tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=o2IEmeLL9r&name=pdf" class="link-primary">https://openreview.net/attachment?id=o2IEmeLL9r&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Predictive auxiliary objectives in deep RL mimic learning in the brain</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">predictive auxiliary objectives</span>
<span class="badge bg-primary">deep reinforcement learning</span>
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">brain function</span>
<span class="badge bg-primary">transfer learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper explores how predictive auxiliary objectives in deep reinforcement learning enhance representation learning and stability in resource-limited environments, mirroring representational changes observed in the brain.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study investigates the effects of predictive auxiliary objectives on representation learning within deep reinforcement learning (RL) systems and their parallels with neural activity in the brain. The authors implemented a deep Q-learning framework that includes a predictive model, allowing them to evaluate how these objectives prevent representational collapse and improve task performance, particularly in resource-constrained architectures. Their findings demonstrate that longer predictive horizons facilitate better representational transfer and show that the representational changes in their RL model closely resemble those seen in various brain regions, particularly the hippocampus and visual cortex. This work suggests a new perspective on the role of the hippocampus in prediction and representation learning, indicating that such systems can benefit other regions even without direct planning functions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=agPpmEgf8C&name=pdf" class="link-primary">https://openreview.net/attachment?id=agPpmEgf8C&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Protein Discovery with Discrete Walk-Jump Sampling</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">discrete generative models</span>
<span class="badge bg-primary">protein discovery</span>
<span class="badge bg-primary">antibodies</span>
<span class="badge bg-primary">energy-based models</span>
<span class="badge bg-primary">Walk-Jump Sampling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Discrete Walk-Jump Sampling (dWJS), a novel method for generating diverse and functional antibody proteins by effectively combining energy-based modeling and Langevin Markov chain Monte Carlo sampling.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors develop a new approach for generative modeling of antibody proteins named Discrete Walk-Jump Sampling (dWJS), which addresses the challenges associated with discrete sequence generation. They utilize a smoothed energy function to improve training and sampling efficiency, allowing for the effective generation of high-quality samples from a sparse and complex data distribution. The method achieves impressive results in both in silico and in vitro experiments, with 97-100% of generated antibody samples successfully expressed and a 70% binding affinity rate for functional designs. Additionally, the study introduces a Distributional Conformity Score (DCS) to benchmark protein generative models, confirming the robustness and effectiveness of the proposed sampling technique for therapeutic molecule design.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=zMPHKOmQNb&name=pdf" class="link-primary">https://openreview.net/attachment?id=zMPHKOmQNb&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Provable Compositional Generalization for Object-Centric Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">compositional generalization</span>
<span class="badge bg-primary">object-centric learning</span>
<span class="badge bg-primary">autoencoders</span>
<span class="badge bg-primary">identifiability theory</span>
<span class="badge bg-primary">representation learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper establishes theoretical guarantees for compositional generalization in object-centric learning by leveraging identifiability theory, demonstrating that specific structural assumptions on autoencoders facilitate this capability.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates how to achieve compositional generalization— the ability to apply learned concepts to novel combinations— within object-centric representation learning. By employing identifiability theory, the authors show that autoencoders with a compositional and additive decoder, along with a compositional consistency regularizer, can reliably learn object-centric representations that generalize to out-of-distribution scenarios. Empirical experiments on synthetic image data validate the theoretical framework, indicating that satisfying these assumptions significantly enhances the model's performance in recognizing and reconstructing unseen object combinations. The findings contribute to a deeper understanding of how structured decoding can promote robust machine perception in comparison to human learning capabilities.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=7VPTUWkiDQ&name=pdf" class="link-primary">https://openreview.net/attachment?id=7VPTUWkiDQ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Proving Test Set Contamination in Black-Box Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">test set contamination</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">statistical testing</span>
<span class="badge bg-primary">exchangeability</span>
<span class="badge bg-primary">benchmark memorization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a statistical test for identifying test set contamination in language models based solely on log probability queries, providing provable guarantees of detection without access to training data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses concerns regarding dataset contamination in large language models (LLMs) by proposing a novel statistical test that leverages the concept of exchangeability to identify whether a model has been influenced by specific benchmark datasets. The authors demonstrate how to compare the log probability of a dataset's canonical ordering against the probabilities of its shuffled permutations, with significant discrepancies indicating potential contamination. Their test is validated through controlled experiments with intentionally contaminated models and is then applied to audit various publicly available LLMs, finding little evidence of pervasive contamination. The findings emphasize the practicality and effectiveness of the proposed test in auditing language model performance, suggesting that further developments could enhance detection capabilities for lower duplication counts in contaminated datasets.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=KS8mIvetg2&name=pdf" class="link-primary">https://openreview.net/attachment?id=KS8mIvetg2&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">transfer learning</span>
<span class="badge bg-primary">hyperparameter optimization</span>
<span class="badge bg-primary">meta-learning</span>
<span class="badge bg-primary">pretrained models</span>
<span class="badge bg-primary">Bayesian optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Quick-Tune, a methodology for efficiently selecting and optimizing pretrained models and their hyperparameters for image classification tasks through Bayesian optimization and meta-learning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of selecting the optimal pretrained model and finetuning hyperparameters from a large pool of options for various image classification tasks. The authors propose Quick-Tune, a Combined Algorithm Selection and Hyperparameter Optimization (CASH) approach that leverages a large-scale meta-dataset created by evaluating over 20k hyperparameter configurations across 24 pretrained models on 87 datasets. The methodology incorporates gray-box optimization, meta-learning for performance prediction, and cost-awareness to speed up the search process. Experimental results demonstrate that Quick-Tune outperforms traditional finetuning methods and state-of-the-art hyperparameter optimization techniques, highlighting its practical utility in enhancing performance while saving time and computational resources.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tqh1zdXIra&name=pdf" class="link-primary">https://openreview.net/attachment?id=tqh1zdXIra&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">ReLU</span>
<span class="badge bg-primary">activation sparsity</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">inference efficiency</span>
<span class="badge bg-primary">model optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper advocates for the use of ReLU activations in Large Language Models (LLMs) to significantly improve inference efficiency by leveraging activation sparsity with minimal impact on performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study investigates the computational challenges of deploying Large Language Models (LLMs) on resource-constrained devices, emphasizing the advantages of using the ReLU activation function over more complex alternatives like GELU or SiLU. The authors demonstrate that ReLU leads to high activation sparsity during inference, which reduces computational load and weight transfer, translating to substantial savings in Floating Point Operations Per Second (FLOPS). The paper outlines a process termed "relufication," where existing pretrained models are fine-tuned to incorporate ReLU activations, showing that this approach quickly restores original performance on various tasks. Additionally, the authors reveal patterns of aggregated sparsity and propose further strategies to enhance inference speed, while introducing the shifted ReLU activation to maximize sparsity without sacrificing performance. Overall, the findings highlight the potential benefits of architectural changes in LLMs for improved efficiency.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=osoWxY8q2E&name=pdf" class="link-primary">https://openreview.net/attachment?id=osoWxY8q2E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Robust agents learn causal world models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Causality</span>
<span class="badge bg-primary">Robustness</span>
<span class="badge bg-primary">Generalization</span>
<span class="badge bg-primary">Transfer Learning</span>
<span class="badge bg-primary">Decision Making</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper establishes that agents capable of robustly adapting to distributional shifts must learn an approximate causal model of the data-generating process to satisfy regret bounds, leading to significant implications for AI robustness and general intelligence.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the necessity of causal reasoning for general intelligence by proving that any agent that can achieve low regret in response to various distributional shifts must have learned an approximate causal model of the underlying data-generating process. The authors provide theoretical results demonstrating that robust policies can be derived from these causal models, and they explore implications for transfer learning, causal inference, and the emergence of agent capabilities. The findings highlight important connections between causal representation learning and the ability of agents to handle unseen domain shifts effectively, suggesting that learning causal structures is essential for achieving strong robustness in AI systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pOoKI3ouv1&name=pdf" class="link-primary">https://openreview.net/attachment?id=pOoKI3ouv1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Self-Alignment with Instruction Backtranslation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">instruction alignment</span>
<span class="badge bg-primary">self-training</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">data curation</span>
<span class="badge bg-primary">backtranslation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces "instruction backtranslation," an innovative self-training method that leverages unlabeled web data to enhance instruction-following capabilities in language models without relying on extensive human annotations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a scalable self-training approach called instruction backtranslation, designed to improve instruction-following language models by automatically generating and curating training examples from a web corpus. By initializing with a small set of human-annotated examples, the method iterates through self-augmentation to create candidate instruction-output pairs, followed by quality assessment and selection of high-quality examples. The resultant models, particularly Humpback, demonstrate superior performance against existing non-distilled models on the Alpaca leaderboard, proving that high-quality data curation significantly enhances model capabilities while reducing reliance on human annotations. The findings suggest potential for further improvements through larger unlabeled datasets.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=1oijHJBRsT&name=pdf" class="link-primary">https://openreview.net/attachment?id=1oijHJBRsT&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Self-Reflection</span>
<span class="badge bg-primary">Retrieval-Augmented Generation</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Factual Accuracy</span>
<span class="badge bg-primary">Critique Mechanism</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces SELF-RAG, a framework that enhances the quality and factuality of large language models by enabling on-demand retrieval, generation, and self-critique through innovative reflection tokens, leading to significant performance improvements over existing models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents SELF-RAG, a novel framework designed to improve the performance of large language models (LLMs) by incorporating a mechanism for on-demand retrieval and self-reflection. Unlike traditional retrieval-augmented generation methods that indiscriminately fetch and integrate external information, SELF-RAG uses special reflection tokens to enable the model to determine when retrieval is necessary, assess the relevance of retrieved passages, and critique its own outputs. The framework was evaluated against various state-of-the-art LLMs and retrieval-augmented models across multiple tasks, demonstrating substantial gains in factual accuracy, citation precision, and overall generation quality. Through detailed experiments, the authors illustrate the effectiveness of SELF-RAG in allowing the model to tailor its responses based on diverse task requirements, thereby addressing common limitations seen in prior approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=hSyW5go0v8&name=pdf" class="link-primary">https://openreview.net/attachment?id=hSyW5go0v8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Small-scale proxies for large-scale Transformer training instabilities</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">training instability</span>
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">learning rate sensitivity</span>
<span class="badge bg-primary">model scaling</span>
<span class="badge bg-primary">mitigation strategies</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper explores the reproduction and understanding of training instabilities in large-scale Transformer models through small-scale proxy experiments, revealing effective mitigation strategies and predictive scaling behaviors.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates training instabilities encountered during large-scale training of Transformer models, specifically focusing on two types of instabilities: attention logit growth and output logit divergence. By replicating these instabilities in smaller models, the authors demonstrate that high learning rates can lead to similar issues even at a reduced scale, allowing for the evaluation of existing mitigation strategies such as qk-layernorm and z-loss. The study introduces the concept of learning rate sensitivity to quantify model performance relative to learning rate across various scales. Additionally, the authors identify predictive behaviors in model characteristics that can signal potential instabilities before they emerge, providing a framework for future research on ensuring stable training in large Transformers. This work allows for the study of instability phenomena without requiring extensive computational resources.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=d8w0pmvXbZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=d8w0pmvXbZ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Statistically Optimal K -means Clustering via Nonnegative Low-rank Semidefinite Programming</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">K-means clustering</span>
<span class="badge bg-primary">semidefinite programming</span>
<span class="badge bg-primary">nonnegative matrix factorization</span>
<span class="badge bg-primary">low-rank factorization</span>
<span class="badge bg-primary">statistical optimality.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces an efficient nonnegative low-rank algorithm for K-means clustering that combines the scalability of matrix factorization with strong statistical guarantees typically reserved for semidefinite programming.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper proposes a novel clustering algorithm that addresses the challenges of K-means clustering by utilizing a nonnegative low-rank representation based on a semidefinite programming (SDP) relaxation. It highlights the limitations of current methods like semidefinite programming due to their computational inaccessibility for large datasets, and contrasts this with nonnegative matrix factorization (NMF), which lacks theoretical guarantees. The authors develop a nonnegative low-rank (NLR) approach that maintains the statistical benefits of SDP while achieving the computational efficiency of NMF. Their experiments demonstrate that the NLR algorithm achieves significantly lower mis-clustering errors compared to state-of-the-art methods while being scalable, thus providing a powerful alternative for large-scale clustering problems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=v7ZPwoHU1j&name=pdf" class="link-primary">https://openreview.net/attachment?id=v7ZPwoHU1j&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SWE-bench: Can Language Models Resolve Real-world Github Issues?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">software engineering</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">benchmarks</span>
<span class="badge bg-primary">GitHub issues</span>
<span class="badge bg-primary">code generation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces SWE-bench, an evaluation framework for language models that assesses their ability to resolve real-world GitHub issues, revealing that even state-of-the-art models struggle, with the top performer only solving 1.96% of tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents SWE-bench, a novel evaluation framework designed to test the capabilities of language models (LMs) in addressing real-world software engineering problems extracted from GitHub issues. The framework encompasses 2,294 software engineering tasks, each consisting of a codebase and a relevant issue description, requiring models to generate patches that resolve the described problems. To establish a comprehensive evaluation, the authors tested various state-of-the-art LMs, including their own fine-tuned models, and found that these models largely fail to address the complexities of software issues, with the best performing model, Claude 2, resolving only 1.96% of issues. The findings emphasize the need for more challenging benchmarks to guide future developments in LMs that can effectively tackle real-world coding problems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=VTF8yNQM66&name=pdf" class="link-primary">https://openreview.net/attachment?id=VTF8yNQM66&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">The mechanistic basis of data dependence and abrupt learning in an in-context classification task</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">in-context learning</span>
<span class="badge bg-primary">transformers</span>
<span class="badge bg-primary">induction head</span>
<span class="badge bg-primary">data distribution</span>
<span class="badge bg-primary">abrupt learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the mechanistic basis of in-context learning (ICL) in transformer models, demonstrating that the abrupt emergence of an induction head, influenced by specific data distribution features, differentiates ICL from in-weights learning (IWL).</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the mechanisms underlying in-context learning (ICL) in transformer models, contrasting it with traditional in-weights learning (IWL). By employing a minimal attention-only network trained on simplified datasets, the authors establish that ICL is driven by the sudden formation of an induction head, which competes with IWL. They analyze how distributional properties of the data, such as burstiness and rank-frequency distributions, affect the trade-off between these learning modalities. The results include the construction of a two-parameter model simulating induction head behavior and a phenomenological model that connects the abrupt transitions in learning to a sequential learning process involving nested logits. The findings suggest that intrinsic curricula could enhance ICL in larger language models (LLMs) by facilitating the emergence of more complex learning strategies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=aN4Jf6Cx69&name=pdf" class="link-primary">https://openreview.net/attachment?id=aN4Jf6Cx69&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Topological data analysis on noisy quantum computers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Topological Data Analysis</span>
<span class="badge bg-primary">Quantum Computing</span>
<span class="badge bg-primary">NISQ-TDA</span>
<span class="badge bg-primary">Betti Numbers</span>
<span class="badge bg-primary">Machine Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces NISQ-TDA, a quantum algorithm for estimating Betti numbers in topological data analysis, designed for noisy intermediate-scale quantum computers that demonstrates robustness against noise and significant potential for achieving speedups over classical methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents NISQ-TDA, a novel quantum machine learning algorithm aimed at estimating Betti numbers from high-dimensional datasets using topological data analysis (TDA) within the framework of Noisy Intermediate-Scale Quantum (NISQ) computing. Unlike traditional methods, NISQ-TDA operates with short circuit depths and avoids the limitations associated with data loading and fault tolerance. The algorithm utilizes an efficient representation of boundary operators, quantum rejection sampling, and stochastic rank estimation to produce estimates for normalized Betti numbers. Empirical results indicate that NISQ-TDA performs well under noise, suggesting strong potential for practical applications of TDA in machine learning and artificial intelligence on NISQ devices. The findings underscore the promise of quantum advantages in handling complex data analysis tasks that are computationally prohibitive for classical approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dLrhRIMVmB&name=pdf" class="link-primary">https://openreview.net/attachment?id=dLrhRIMVmB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Towards a statistical theory of data selection under weak supervision</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Data selection</span>
<span class="badge bg-primary">weak supervision</span>
<span class="badge bg-primary">biased sampling</span>
<span class="badge bg-primary">statistical learning</span>
<span class="badge bg-primary">empirical risk minimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates data selection methods under weak supervision, demonstrating that biased subsampling can outperform traditional unbiased methods in certain scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the effectiveness of data selection in machine learning, particularly when working with larger unlabeled datasets and imperfect surrogate models. It proposes a framework where a subset of unlabeled samples is selected for labeling and model training, focusing on the design of selection probabilities that can potentially improve learning outcomes. Through numerical experiments and theoretical analyses under both low- and high-dimensional settings, the authors find that biased data selection methods significantly outperformed traditional unbiased approaches, especially when considering their relationship with the selection fraction. The insights suggest that better utilization of available data, even with weaker surrogates, can lead to lower misclassification rates, challenging common assumptions about unbiased sampling in the field.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HhfcNgQn6p&name=pdf" class="link-primary">https://openreview.net/attachment?id=HhfcNgQn6p&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">in-context learning</span>
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">Boolean functions</span>
<span class="badge bg-primary">sample efficiency</span>
<span class="badge bg-primary">Large Language Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the in-context learning capabilities of Transformers and Large Language Models (LLMs) across various Boolean function tasks, revealing both their strengths in learning simpler functions and their limitations with more complex ones.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors explore the phenomenon of in-context learning in Transformers and LLMs by evaluating their performance in learning discrete Boolean functions within a stylized framework. They find that while Transformers can achieve near-optimal performance on simpler tasks, such as conjunctions and disjunctions, their efficacy deteriorates with more complex tasks, like parity functions. The paper also highlights the ability of Transformers to leverage teaching sequences for more sample-efficient learning and shows that certain attention-free models perform similarly to Transformers. Additionally, pretrained LLMs, including popular models like GPT-4 and LLaMA-2, demonstrate competitive performance on tasks outside their training set, indicating an intriguing capacity for in-context learning. The findings underscore both the capabilities and inherent limitations of Transformers and LLMs in implementing learning algorithms, raising questions for future research on improving sample efficiency and understanding pretraining influences.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ekeyCgeRfC&name=pdf" class="link-primary">https://openreview.net/attachment?id=ekeyCgeRfC&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">geometric generative models</span>
<span class="badge bg-primary">Bayesian Flow Networks</span>
<span class="badge bg-primary">molecular geometry</span>
<span class="badge bg-primary">noise sensitivity</span>
<span class="badge bg-primary">SE-(3) invariance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents Geometric Bayesian Flow Networks (GeoBFN), a novel generative model that effectively represents 3D molecular geometry while overcoming challenges related to noise sensitivity and multi-modality by utilizing SE-(3) invariant density modeling.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Geometric Bayesian Flow Networks (GeoBFN), a generative modeling framework specifically designed for 3D molecular geometry. GeoBFN addresses challenges such as noise sensitivity and multi-modality by employing a unified probabilistic modeling approach and incorporating rotational and translational invariance (SE-(3) invariance) in its density functions. The model operates in a differentiable parameter space that enhances compatibility with the inherent noise sensitivity of molecular coordinates. Experimental results demonstrate that GeoBFN outperforms state-of-the-art methods in various benchmarks for molecular generation, achieving high stability rates and enabling sampling with any number of steps, thereby optimizing efficiency and generation quality. The findings indicate GeoBFN's potential for advancing molecular design and discovery.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=NSVtmmzeRB&name=pdf" class="link-primary">https://openreview.net/attachment?id=NSVtmmzeRB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Unprocessing Seven Years of Algorithmic Fairness</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">algorithmic fairness</span>
<span class="badge bg-primary">postprocessing</span>
<span class="badge bg-primary">error rate parity</span>
<span class="badge bg-primary">unprocessing</span>
<span class="badge bg-primary">Pareto dominance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that the simple postprocessing method for achieving error rate parity dominates various advanced fairness methods across multiple datasets, suggesting that it can yield optimal fairness-accuracy trade-offs.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors critically evaluate the effectiveness of algorithmic fairness interventions over the past seven years, particularly focusing on the postprocessing method, which adjusts acceptance thresholds to equalize error rates across demographic groups. Through extensive empirical analysis involving over 11,000 model evaluations across multiple tabular datasets, they establish that postprocessing is Pareto-dominant, meaning it outperforms or matches the accuracy-fairness trade-offs of all other evaluated methods. They address common methodological errors in prior comparisons and introduce the concept of "unprocessing" to facilitate fair comparison among models with varying constraints. Their findings underscore the importance of rigorous evaluation standards in the field of algorithmic fairness and suggest that postprocessing remains a compellingly simple yet effective approach to achieving fairness goals.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jr03SfWsBS&name=pdf" class="link-primary">https://openreview.net/attachment?id=jr03SfWsBS&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">uncertainty estimation</span>
<span class="badge bg-primary">semantic segmentation</span>
<span class="badge bg-primary">validation framework</span>
<span class="badge bg-primary">aleatoric uncertainty</span>
<span class="badge bg-primary">epistemic uncertainty</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a systematic framework, ValUES, for the validation of uncertainty estimation methods in semantic segmentation, addressing existing gaps between theory and application by rigorously evaluating uncertainty types, method components, and downstream tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper identifies significant gaps in the validation of uncertainty estimation methods used in semantic segmentation, particularly regarding the separation and evaluation of aleatoric and epistemic uncertainties. It proposes a comprehensive evaluation framework called ValUES, which includes controlled environments for studying data ambiguities and distribution shifts, systematic ablations of method components, and test beds for various application scenarios. Empirical findings reveal that while uncertainty type separation is feasible in simulated settings, it does not consistently apply to real-world data; moreover, the effectiveness of individual method components, particularly aggregation strategies, varies widely depending on the dataset and task. The study concludes with practical recommendations for researchers and practitioners in the field to enhance the reliability and applicability of uncertainty estimation methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=yV6fD7LYkF&name=pdf" class="link-primary">https://openreview.net/attachment?id=yV6fD7LYkF&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Vision Transformers Need Registers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Vision Transformers</span>
<span class="badge bg-primary">Artifacts</span>
<span class="badge bg-primary">Feature Maps</span>
<span class="badge bg-primary">Register Tokens</span>
<span class="badge bg-primary">Self-Supervised Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper identifies and remediates artifacts in feature maps of Vision Transformers by introducing additional register tokens, improving performance on dense prediction tasks and object discovery methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates artifacts found in the feature maps of Vision Transformers (ViTs), primarily focusing on high-norm tokens that emerge during inference, particularly in low-information background areas of images. The authors propose a solution involving the addition of register tokens to the input sequence, which effectively mitigates these artifacts across various models, including supervised and self-supervised frameworks like DINOv2. Their empirical analysis demonstrates that incorporating register tokens leads to smoother feature maps, enhances performance on dense prediction tasks, and improves results in unsupervised object discovery. The findings indicate that this intervention not only resolves artifact issues but also preserves or even boosts the performance of the models involved.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=2dnO3LLiJ1&name=pdf" class="link-primary">https://openreview.net/attachment?id=2dnO3LLiJ1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Wrstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">text-to-image synthesis</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">architecture efficiency</span>
<span class="badge bg-primary">semantic representation</span>
<span class="badge bg-primary">computational cost reduction</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Wurstchen, a novel three-stage architecture for text-to-image synthesis that significantly reduces computational costs while achieving competitive image quality.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Wurstchen, a new architecture designed for large-scale text-to-image synthesis, which combines enhanced performance with a substantial reduction in computational resources required for training and inference. The model employs a three-stage process, using a highly compressed semantic representation to guide diffusion, which drastically lowers training costs to about 24,602 A100-GPU hours compared to 200,000 hours for existing models like Stable Diffusion 2.1. Results demonstrate that Wurstchen not only performs competitively in image quality but also improves inference speed and reduces environmental impact. The authors aim to promote more sustainable practices in generative AI by making their findings and models publicly accessible.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gU58d5QeGv&name=pdf" class="link-primary">https://openreview.net/attachment?id=gU58d5QeGv&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Zipformer: A faster and better encoder for automatic speech recognition</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Zipformer</span>
<span class="badge bg-primary">Automatic Speech Recognition</span>
<span class="badge bg-primary">Neural Networks</span>
<span class="badge bg-primary">Optimization</span>
<span class="badge bg-primary">Encoder Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Zipformer, a novel and efficient transformer model for automatic speech recognition that outperforms existing models while being faster and less memory-intensive.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Zipformer, a new encoder model designed for automatic speech recognition (ASR) that enhances both speed and performance compared to existing models, such as the Conformer. Zipformer incorporates a U-Net-like structure for temporal downsampling, a novel block design that reuses attention weights, and introduces BiasNorm for normalization, together with new activation functions (SwooshR and SwooshL). Additionally, a new optimizer called ScaledAdam is proposed to improve convergence by scaling updates according to parameter sizes. Comprehensive experiments on datasets like LibriSpeech, Aishell-1, and WenetSpeech demonstrate Zipformer's state-of-the-art performance, reduced computational requirements, and faster inference times, showcasing its effectiveness over previous models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9WD9KwssyT&name=pdf" class="link-primary">https://openreview.net/attachment?id=9WD9KwssyT&name=pdf</a></p>
</div>
</div>
</div>

        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
