
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>PubSummarizer - ICML 2024 Oral Papers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
</head>
<body>
    <div class="container py-4">
        <h1 class="mb-4">ICML 2024 Oral Papers</h1>
        <p class="text-muted"><em>Generated on 2024-11-21 15:58:49 by <a href="https://github.com/Logan-Lin/PubSummarizer">PubSummarizer</a></em></p>
        <div class="row" data-masonry='{"percentPosition": true }'>
            <div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">A Dynamic Algorithm for Weighted Submodular Cover Problem</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">dynamic algorithms</span>
<span class="badge bg-primary">weighted submodular cover</span>
<span class="badge bg-primary">approximation algorithms</span>
<span class="badge bg-primary">query complexity</span>
<span class="badge bg-primary">submodular optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a randomized dynamic algorithm for the weighted submodular cover problem, achieving a (1O(), O(1))-bicriteria approximation with low query complexity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the dynamic version of the weighted submodular cover problem, where elements of a ground set can be inserted and deleted over time, complicating the task of maintaining an optimal solution. The authors propose a randomized algorithm that maintains an approximately optimal solution while ensuring polylogarithmic update time per query. The algorithm leverages a hierarchical data structure with multiple levels and utilizes sampling techniques to balance the trade-off between the number of elements retained and the query complexity during updates. The main contribution is the demonstration that the algorithm can provide an expected (1O(), O(1))-bicriteria approximation, making it practical for real-world applications involving dynamic datasets.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uUeXaKLE1I&name=pdf" class="link-primary">https://openreview.net/attachment?id=uUeXaKLE1I&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">alignment algorithms</span>
<span class="badge bg-primary">DPO</span>
<span class="badge bg-primary">toxicity reduction</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">jailbreaks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the mechanisms of direct preference optimization (DPO) in mitigating toxicity in language models, revealing that toxic behaviors can be circumvented rather than eliminated, facilitating potential jailbreaks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The study examines how alignment algorithms, particularly direct preference optimization (DPO), affect language models' behaviors regarding toxicity. By analyzing the representation of toxicity in pre-trained models like GPT2 and Llama2, the authors identify specific vectors in the models that promote toxic responses. They implement a pairwise preference dataset to train the DPO algorithm, finding that while DPO reduces toxic outputs, it does so by learning a distributional offset that bypasses toxic regions without removing the underlying toxic vectors. Consequently, the study highlights the ease with which aligned models can be un-aligned or "jailbroken" by restoring the previously suppressed toxic behaviors. The findings provide a mechanistic understanding of DPO's workings and suggest directions for more robust alignment methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dBqHGZPGZI&name=pdf" class="link-primary">https://openreview.net/attachment?id=dBqHGZPGZI&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">A Touch, Vision, and Language Dataset for Multimodal Alignment</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">touch</span>
<span class="badge bg-primary">vision</span>
<span class="badge bg-primary">language</span>
<span class="badge bg-primary">dataset</span>
<span class="badge bg-primary">multimodal alignment</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Touch-Vision-Language (TVL) dataset, a novel dataset of 44K vision-touch pairs annotated with natural language labels and demonstrates significant improvements in tactile-vision-language alignment through the development of a multimodal learning model.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the integration of touch as a sensing modality in multimodal language models, which has been previously underexplored due to challenges in data labeling and alignment. It presents the TVL dataset, comprising 44,000 in-the-wild vision-touch pairs with a combination of human annotations and textual pseudo-labels generated by GPT-4V. The authors develop a vision-language-aligned tactile encoder and a touch-vision-language model (TVL model) to generate tactile descriptions, achieving notable enhancements in classification accuracy and alignment capabilities—showing a 29% improvement over prior models. The findings indicate that combining human labels with VLM-generated pseudo-labels effectively improves touch-vision-language understanding, setting a foundation for future research in this multimodal domain.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tFEOOH9eH0&name=pdf" class="link-primary">https://openreview.net/attachment?id=tFEOOH9eH0&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Accurate LoRA-Finetuning Quantization of LLMs via Information Retention</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LoRA</span>
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">information retention</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">accuracy</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces IR-QLoRA, a novel approach for accurate quantization of large language models (LLMs) through improved information retention during LoRA finetuning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of quantizing large language models (LLMs) while maintaining accuracy, a critical requirement for deployment on resource-constrained hardware. The authors propose IR-QLoRA, which leverages two key methods: Information Calibration Quantization (ICQ) to ensure that quantized LLM parameters retain essential original information, and Information Elastic Connection (IEC) to enhance LoRA’s capability to utilize this retained information effectively. Extensive experiments demonstrate that IR-QLoRA significantly outperforms existing state-of-the-art methods across various model sizes and quantization levels, notably achieving higher accuracy with minimal additional computational costs. The results indicate its potential for efficient and accurate deployment of quantized LLMs in practical applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jQ92egz5Ym&name=pdf" class="link-primary">https://openreview.net/attachment?id=jQ92egz5Ym&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">causality</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">entropy regularization</span>
<span class="badge bg-primary">exploration efficiency</span>
<span class="badge bg-primary">off-policy actor-critic</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces ACE, an off-policy actor-critic algorithm that utilizes causality-aware entropy regularization and a gradient-dormancy-guided reset mechanism to significantly enhance exploration efficiency and sample performance across diverse continuous control tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents ACE, a novel off-policy actor-critic algorithm designed to address high sample complexity in reinforcement learning by emphasizing the varying significance of different primitive behaviors throughout the policy learning process. The authors implement a causal policy-reward structural model to assess the impact of individual actions on rewards, introducing a causality-aware entropy term that prioritizes significant actions for efficient exploration. Additionally, they propose a gradient-dormancy-guided reset mechanism to counteract the risk of overfitting to specific behaviors. Extensive experiments demonstrate that ACE outperforms traditional model-free reinforcement learning baselines across 29 control tasks, suggesting its versatility and robust sample efficiency. The findings have implications for the development of more effective reinforcement learning algorithms that leverage causal relationships for improved decision-making and exploration strategies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=1puvYh729M&name=pdf" class="link-primary">https://openreview.net/attachment?id=1puvYh729M&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adaptive experimental design</span>
<span class="badge bg-primary">average treatment effects</span>
<span class="badge bg-primary">covariate choice</span>
<span class="badge bg-primary">semiparametric efficiency bound</span>
<span class="badge bg-primary">propensity score optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes an active adaptive experimental design for estimating average treatment effects (ATE) by optimizing both covariate density and propensity scores to minimize asymptotic variance, demonstrating improved efficiency over existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study presents an innovative framework for conducting adaptive experiments aimed at efficiently estimating average treatment effects (ATEs) through the sequential optimization of covariate density and propensity scores based on prior observations. The authors derive a semiparametric efficiency bound, which underpins the proposed design's ability to achieve lower asymptotic variance compared to traditional methods that focus solely on propensity score optimization. The method, referred to as the Active-Adaptive-Sampling (AAS)-AIPWIW experiment, employs the Augmented Inverse Probability Weighting (AIPW) estimator with importance weighting, ensuring that the final ATE estimator aligns with the derived efficiency bound. Simulation studies demonstrate the effectiveness of this approach, indicating substantial reductions in mean squared error (MSE) and highlighting applications across various fields, including healthcare and economics.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=K6HpbvkrwO&name=pdf" class="link-primary">https://openreview.net/attachment?id=K6HpbvkrwO&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Active Statistical Inference</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">active inference</span>
<span class="badge bg-primary">statistical inference</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">data collection</span>
<span class="badge bg-primary">uncertainty quantification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces active inference, a method that optimizes label collection through machine learning to enhance statistical inference while reducing sample size requirements.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents active inference, a novel methodology that combines machine learning with strategic data collection to improve statistical inference, particularly under labeling budgets. Active inference operates on the premise of prioritizing labels for data points where the associated machine learning model shows uncertainty, thus effectively directing data collection efforts. This approach constructs valid confidence intervals and hypothesis tests, outperforming traditional methods that rely on non-adaptive sampling by achieving similar inference accuracy with significantly fewer samples—up to 80% budget savings. The authors evaluate active inference across various domains, including public opinion research, census analysis, and proteomics, highlighting its potential to yield smaller interval widths and more powerful tests compared to classical inference methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=GKMcCtWC7H&name=pdf" class="link-primary">https://openreview.net/attachment?id=GKMcCtWC7H&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">AI Control: Improving Safety Despite Intentional Subversion</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">AI control</span>
<span class="badge bg-primary">safety protocols</span>
<span class="badge bg-primary">intentional subversion</span>
<span class="badge bg-primary">backdoor detection</span>
<span class="badge bg-primary">language models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents methodologies for improving the safety of large language models by developing protocols that prevent intentional subversion and submitting malicious code, thus ensuring systems operate reliably under adversarial conditions.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of ensuring safety in powerful language models (LLMs) against intentional subversion, focusing on a methodology called AI control. The authors develop and evaluate various safety protocols within a testbed known as the APPS backdooring setting, aimed at solving programming problems without submitting backdoored solutions. Using a combination of a powerful untrusted model (GPT-4), a less powerful trusted model (GPT-3.5), and limited trusted human labor, the study evaluates various methods to prevent backdoors while maintaining usefulness. Key findings reveal that protocols like trusted monitoring and trusted editing lead to significant improvements in safety without substantial loss in performance, achieving over 90% safety across numerous trials while simultaneously highlighting the mechanisms employed by adversarial models to subvert these safety measures. The implications of this work underscore the necessity for robust safety techniques as LLMs are increasingly integrated into influential roles in society.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=KviM5k8pcP&name=pdf" class="link-primary">https://openreview.net/attachment?id=KviM5k8pcP&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">All-in-one simulation-based inference</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Amortized Inference</span>
<span class="badge bg-primary">Simulation-based Inference</span>
<span class="badge bg-primary">Probabilistic Diffusion Models</span>
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">Bayesian Inference</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The Simformer introduces a novel method for amortized simulation-based inference using transformers and probabilistic diffusion models, achieving greater flexibility and efficiency in estimating conditional distributions.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the Simformer, an innovative approach to amortized Bayesian inference that leverages transformers and probabilistic diffusion models to enhance simulation-based inference capabilities. Unlike existing methods, the Simformer allows for the estimation of arbitrary conditional distributions, accommodating unstructured and missing data as well as function-valued parameters. The efficacy of the Simformer is demonstrated across various benchmark tasks, showing that it surpasses state-of-the-art methods in accuracy while requiring significantly fewer simulations. The study highlights its potential applications in diverse scientific fields, emphasizing its flexibility in handling different inference scenarios and data types without necessitating retraining or fixed parametric specifications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=DL79HYCFFq&name=pdf" class="link-primary">https://openreview.net/attachment?id=DL79HYCFFq&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">AlphaFold Meets Flow Matching for Generating Protein Ensembles</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">protein ensembles</span>
<span class="badge bg-primary">generative modeling</span>
<span class="badge bg-primary">AlphaFold</span>
<span class="badge bg-primary">flow matching</span>
<span class="badge bg-primary">molecular dynamics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Alpha FLOW and ESM FLOW, two flow-based generative models that enhance protein structure prediction by accurately capturing conformational diversity and flexibility, outperforming traditional methods like MSA subsampling.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces Alpha FLOW and ESM FLOW, innovative generative models that combine the precision of well-established protein structure prediction tools, AlphaFold and ESMFold, with flow matching techniques to effectively sample conformational landscapes of proteins. The authors repurpose these single-state predictors into sequence-conditioned generative models capable of learning from protein ensembles, including those derived from expensive molecular dynamics (MD) simulations. Through extensive evaluations on both the Protein Data Bank (PDB) and MD datasets, the proposed models demonstrate superior performance in terms of precision, diversity, and the ability to replicate structural flexibility and ensemble properties compared to existing methods. These advances show potential for providing more efficient and accurate tools in structural biology, particularly in drug discovery and protein design applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=rs8Sh2UASt&name=pdf" class="link-primary">https://openreview.net/attachment?id=rs8Sh2UASt&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">any-precision LLM</span>
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">deployment</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">inference efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces any-precision LLM, a method that significantly reduces deployment costs and memory usage by enabling low-cost serving of multiple LLMs of different sizes through efficient quantization.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the high costs and memory limitations associated with deploying multiple large language models (LLMs) of varying sizes, presenting "any-precision LLM" as a solution that extends the concept of any-precision deep neural networks to LLMs. The authors propose a lightweight approach for any-precision quantization using a post-training framework and introduce a specialized software engine to optimize inference throughput. Experimental results demonstrate that this method can effectively overlay LLMs quantized at different bit-widths into a memory footprint similar to a single model while achieving state-of-the-art quality and efficiency in inference processes. This advancement has significant implications for deploying LLMs in resource-constrained environments, enhancing their accessibility and usability across various applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=u09gadH3BU&name=pdf" class="link-primary">https://openreview.net/attachment?id=u09gadH3BU&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adaptive pruning</span>
<span class="badge bg-primary">tuning</span>
<span class="badge bg-primary">pretrained language models</span>
<span class="badge bg-primary">efficient training</span>
<span class="badge bg-primary">inference efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents APT, a method for adaptively pruning and tuning pretrained language models to enhance both training and inference efficiency while maintaining high task performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces APT (Adaptive Pruning and Tuning), a novel approach designed to improve the efficiency of fine-tuning and inference in large language models (LMs) by dynamically selecting parameters for pruning and tuning. APT prunes less important parameters early in the training process to accelerate convergence and reduce memory usage, while still being able to recover model performance through selective tuning of salient parameters. Experiments demonstrate that APT can maintain up to 98% task performance in models like RoBERTa and T5 while pruning 60% of their parameters, as well as achieving significant improvements in training speed (up to 8 times faster) and reducing memory footprints by up to 70%. The approach highlights the potential for more accessible usage of LMs in resource-constrained environments, paving the way for further research in efficient model adaptations.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=sb81Xl50JG&name=pdf" class="link-primary">https://openreview.net/attachment?id=sb81Xl50JG&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Arrows of Time for Large Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">autoregressive models</span>
<span class="badge bg-primary">Arrow of Time</span>
<span class="badge bg-primary">natural language processing</span>
<span class="badge bg-primary">perplexity</span>
<span class="badge bg-primary">computational complexity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that autoregressive Large Language Models (LLMs) exhibit a consistent time asymmetry, termed the Arrow of Time (AoT), in their ability to learn and predict natural language, showing that they perform better in forward token prediction compared to backward.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the probabilistic modeling capabilities of autoregressive Large Language Models (LLMs) through the lens of time directionality, revealing the existence of the Arrow of Time (AoT) phenomenon in LLMs where models trained to predict the next token (forward) consistently outperform those predicting the previous token (backward). By analyzing various languages, contexts, and model architectures, the authors establish a significant disparity in average log-perplexity between forward and backward predictions, suggesting that this asymmetry arises from factors related to sparsity and computational complexity. The findings indicate that the AoT is not merely a consequence of tokenization artifacts, and they open new avenues for further research in understanding the learning dynamics of language models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=UpSe7ag34v&name=pdf" class="link-primary">https://openreview.net/attachment?id=UpSe7ag34v&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Retrieval-Augmented Generation</span>
<span class="badge bg-primary">Evaluation Methodology</span>
<span class="badge bg-primary">Item Response Theory</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Automated Exam Generation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces an automated evaluation framework for Retrieval-Augmented Language Models (RAG) using task-specific synthetic exams generated via Item Response Theory, demonstrating its effectiveness across multiple domains.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper proposes a novel methodology to evaluate the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG) by scoring them based on automatically generated synthetic exams composed of multiple-choice questions derived from relevant document corpora. Utilizing Item Response Theory (IRT), this method iteratively refines exam quality and assesses RAG system performance, offering an interpretable and scalable evaluation strategy without the need for annotated datasets. The authors validate their framework across four new open-ended Question-Answering tasks, revealing critical insights into performance factors such as retrieval mechanisms and model sizes, ultimately finding that optimizing retrieval algorithms yields greater performance improvements than merely increasing model size. The work contributes a comprehensive and automated approach for evaluating RAG systems in real-world applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4jqOV6NlUz&name=pdf" class="link-primary">https://openreview.net/attachment?id=4jqOV6NlUz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Bottleneck-Minimal Indexing for Generative Document Retrieval</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Generative Document Retrieval</span>
<span class="badge bg-primary">Information Bottleneck</span>
<span class="badge bg-primary">Indexing Methods</span>
<span class="badge bg-primary">Mutual Information</span>
<span class="badge bg-primary">Neural Networks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a bottleneck-minimal indexing method for generative document retrieval (GDR) that incorporates query distributions, resulting in improved retrieval performance over existing indexing techniques.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper redefines generative document retrieval (GDR) using an information-theoretic framework, specifically applying the information bottleneck theory to optimize the indexing of documents. By treating indexing as a bottleneck that affects information transmission from documents to queries, the authors propose a new approach that emphasizes clustering queries rather than documents. They empirically evaluate their bottleneck-minimal indexing method against traditional methods using the NQ320K and MARCO datasets, finding significant improvements in recall and mean reciprocal rank. The findings indicate that the proposed indexing method not only consistently outperforms existing techniques based on traditional metrics but also aligns with theoretical insights from the information bottleneck theory, highlighting the importance of incorporating query distribution into the indexing process for better performance in GDR systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=MFPYCvWsNR&name=pdf" class="link-primary">https://openreview.net/attachment?id=MFPYCvWsNR&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Candidate Pseudolabel Learning</span>
<span class="badge bg-primary">Vision-Language Models</span>
<span class="badge bg-primary">Prompt Tuning</span>
<span class="badge bg-primary">Unlabeled Data</span>
<span class="badge bg-primary">Semi-Supervised Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Candidate Pseudolabel Learning (CPL), a novel method that enhances vision-language models by generating candidate pseudolabels from unlabeled data, addressing the challenges posed by inaccurate hard pseudolabels.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces Candidate Pseudolabel Learning (CPL) as a method to fine-tune vision-language models (VLMs) using abundant unlabeled data. The key innovation of CPL lies in its generation strategy for candidate pseudolabels, which involves both intra- and inter-instance label selections based on a confidence score matrix for all unlabeled instances. By selecting classes with the top-k highest prediction confidences, the method aims to include true labels while balancing class representations. Extensive experiments on multiple benchmark datasets demonstrate that CPL significantly outperforms traditional hard pseudolabeling methods, particularly in scenarios with low zero-shot performance. This research showcases the effectiveness of leveraging candidate pseudolabels to improve model adaptation with limited labeled data, providing a robust alternative for exploiting unlabeled datasets in various learning paradigms.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=sBJNokmYuV&name=pdf" class="link-primary">https://openreview.net/attachment?id=sBJNokmYuV&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Chain of Code: Reasoning with a Language Model-Augmented Code Emulator</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Code reasoning</span>
<span class="badge bg-primary">Language models</span>
<span class="badge bg-primary">Chain of Code</span>
<span class="badge bg-primary">LMulator</span>
<span class="badge bg-primary">Semantic tasks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Chain of Code (CoC), a method that enhances language model reasoning by leveraging a code emulator (LMulator) to mix executable code with semantic reasoning, resulting in improved performance on complex tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper proposes Chain of Code (CoC), an innovative approach that combines code generation with semantic reasoning in language models, facilitating complex problem-solving. CoC enables language models to write pseudocode and executable code, while an LMulator emulates code execution when necessary, allowing for flexible handling of undefined behaviors. Experimental results show that CoC significantly outperforms existing methods, such as Chain of Thought, across diverse reasoning tasks, achieving high performance on benchmarks like BIG-Bench Hard. The findings suggest CoC's potential to effectively blend the strengths of structured coding and semantic understanding, with implications for various applications, including robotics and general-purpose reasoning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=vKtomqlSxm&name=pdf" class="link-primary">https://openreview.net/attachment?id=vKtomqlSxm&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Challenges in Training PINNs: A Loss Landscape Perspective</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Physics-Informed Neural Networks</span>
<span class="badge bg-primary">Optimization</span>
<span class="badge bg-primary">Loss Landscape</span>
<span class="badge bg-primary">Ill-conditioning</span>
<span class="badge bg-primary">Second-order Methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the ill-conditioning of the loss landscape in training Physics-Informed Neural Networks (PINNs) and introduces improved optimization strategies, including a novel optimizer called NysNewton-CG (NNCG), which enhances training performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the challenges associated with training Physics-Informed Neural Networks (PINNs), specifically focusing on the ill-conditioning of the loss landscape caused by differential operators in the residual term. The authors compare the performance of various optimizers, including Adam, L-BFGS, and a combination of both (Adam+L-BFGS), demonstrating that the combination outperforms the individual methods. They introduce a new second-order optimizer, NysNewton-CG (NNCG), which significantly improves the optimization process after Adam+L-BFGS. Theoretical insights regarding the relationship between the ill-conditioning of differential operators and the PINN loss function are provided, along with empirical evidence showing that lower loss values correspond to better solution accuracy. The findings present valuable insights for developing more robust training methodologies for PINNs, which are crucial for solving complex partial differential equations.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mJGiFr8jLa&name=pdf" class="link-primary">https://openreview.net/attachment?id=mJGiFr8jLa&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">competition dynamics</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">agent-based modeling</span>
<span class="badge bg-primary">market strategies</span>
<span class="badge bg-primary">sociological theories</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a framework and simulation environment for studying competition dynamics among large language model-based agents, revealing significant insights into market strategies and social theories.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the competition dynamics of large language model (LLM)-based agents through a developed framework called CompeteAI, which is implemented in a simulated environment featuring restaurant and customer agents. By employing GPT-4, the authors simulate a virtual town where restaurant agents compete for customers, revealing behaviors such as differentiation, imitation, and social learning that align with established sociological and economic theories. The study finds that competition drives restaurants to innovate and improve product quality while also uncovering the "Matthew Effect," where initially successful agents continue to gain advantages, and that grouping of customers mitigates the winner-take-all phenomenon. The insights from this research contribute to a better understanding of competitive interactions and suggest implications for the design and deployment of LLM-based agents in various contexts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=wGtzp4ZT1n&name=pdf" class="link-primary">https://openreview.net/attachment?id=wGtzp4ZT1n&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">overparameterization</span>
<span class="badge bg-primary">low-rank learning</span>
<span class="badge bg-primary">model fine-tuning</span>
<span class="badge bg-primary">matrix completion</span>
<span class="badge bg-primary">compression methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that leveraging low-dimensional structures and compressible dynamics in overparameterized deep learning models can significantly reduce computational costs while preserving the benefits of overparameterization, particularly in low-rank matrix completion and language model fine-tuning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This work explores the concept of compressible dynamics within deep overparameterized low-rank learning frameworks, revealing that the learning dynamics of weight matrices remain confined to invariant low-dimensional subspaces. By classifying this behavior theoretically, the authors present efficient compression methods that allow for significant reductions in the number of training parameters. These methods are effectively applied to deep low-rank matrix completion and lead to a new performance-enhancing adaptation technique for language models, termed Deep LoRA, which reduces overfitting while simplifying hyperparameter tuning. Empirical results indicate improved training efficiency and performance, particularly in scenarios with limited data, reinforcing the practicality of the proposed approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uDkXoZMzBv&name=pdf" class="link-primary">https://openreview.net/attachment?id=uDkXoZMzBv&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Contrasting Multiple Representations with the Multi-Marginal Matching Gap</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">multi-marginal optimal transport</span>
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">contrastive loss</span>
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">multimodal embeddings</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces the multi-marginal matching gap (M3G), a novel loss function that effectively learns representations from multiple views or modalities using multi-marginal optimal transport principles.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of learning meaningful representations from complex objects observed through multiple views or modalities, which is crucial in machine learning. Existing methods primarily extend pairwise loss functions but fail to leverage the simultaneous coherence of multiple embeddings. The authors propose the M3G loss, which utilizes multi-marginal optimal transport theory to compare the ground-truth polymatching of k-tuples of embeddings against the optimal cost of arrangements derived from these embeddings. Experimental results demonstrate that M3G outperforms traditional pairwise loss extensions in both self-supervised learning and multimodal tasks across various datasets, suggesting its potential for enhancing representation learning through a holistic view of all views simultaneously.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dV9B9qFeGi&name=pdf" class="link-primary">https://openreview.net/attachment?id=dV9B9qFeGi&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Data-free Neural Representation Compression with Riemannian Neural Dynamics</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Riemannian metrics</span>
<span class="badge bg-primary">Neural compression</span>
<span class="badge bg-primary">Data-free learning</span>
<span class="badge bg-primary">Neural networks</span>
<span class="badge bg-primary">Dynamic systems</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel data-free neural representation compression method using Riemannian metrics to enhance inference accuracy and reduce model size without requiring fine-tuning on real data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a new approach to neural model compression that leverages Riemannian geometry to interpret neural weights as interactions between neurons in a dynamic system. By defining a Riemannian metric (RieM), the authors model these interactions, facilitating efficient data-free compression of neural networks such as ResNet and Vision Transformers. The experimental results indicate that models compressed with RieM outperform existing data-free methods in accuracy and compression ratios on various datasets, including MNIST, CIFAR-100, ImageNet, and COCO. The proposed method represents a significant advancement in the field of neural compression, emphasizing the potential of integrating Riemannian dynamics into neural architectures for enhanced performance and efficiency.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LTifAl5bKb&name=pdf" class="link-primary">https://openreview.net/attachment?id=LTifAl5bKb&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Debating with More Persuasive LLMs Leads to More Truthful Answers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">debate</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">truthfulness</span>
<span class="badge bg-primary">supervision</span>
<span class="badge bg-primary">LLMs</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that weaker language models can effectively judge the correctness of stronger models through a debate format, leading to more truthful outcomes in various tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the use of debate as a method to align stronger large language models (LLMs) with truthful responses, leveraging weaker models or human judges to evaluate their outputs. The authors conducted experiments with both human and LLM judges engaging in debates between expert models to assess their ability to discern correct answers based on arguments presented. The findings showed that debate could significantly enhance the accuracy of both non-expert models and human judges compared to naive baselines. Moreover, by optimizing for persuasive arguments, the authors found that the strength of debaters improved the overall judgements of weaker models, indicating that debate can serve as a viable mechanism for scalable oversight in increasingly sophisticated AI systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=iLCZtl7FTa&name=pdf" class="link-primary">https://openreview.net/attachment?id=iLCZtl7FTa&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">uncertainty decomposition</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">epistemic uncertainty</span>
<span class="badge bg-primary">aleatoric uncertainty</span>
<span class="badge bg-primary">input clarification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes a novel framework called input clarification ensembling that effectively decomposes the uncertainty of large language models into aleatoric and epistemic components, enhancing their reliability and interpretability.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of uncertainty decomposition in large language models (LLMs), distinguishing between aleatoric uncertainty (due to input ambiguity) and epistemic uncertainty (due to model knowledge gaps). The authors introduce a framework, named input clarification ensembling, which generates clarifications for model inputs to reduce ambiguity and allows for effective uncertainty quantification. This method employs pre-trained LLMs to provide clarifications and measures the resultant predictions' uncertainty. Empirical evaluations across various language processing tasks demonstrate that the proposed approach reliably quantifies total uncertainty and accurately decomposes it into aleatoric and epistemic components, thus fostering improved trustworthiness and interpretability of LLM predictions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=byxXa99PtF&name=pdf" class="link-primary">https://openreview.net/attachment?id=byxXa99PtF&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Differentiable Mapper for Topological Optimization of Data Representation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Topological Data Analysis</span>
<span class="badge bg-primary">Mapper Graph</span>
<span class="badge bg-primary">Filter Optimization</span>
<span class="badge bg-primary">Persistent Homology</span>
<span class="badge bg-primary">Stochastic Methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Soft Mapper, a novel method for optimizing filter functions in Mapper graphs for improved data representation through topological analysis.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the development of Soft Mapper, a generalized and smoother variant of the Mapper graph, designed to automate the tuning of filter functions, which has historically been a manual and complex task. The authors introduce a new optimization framework that leverages stochastic methods to enhance the representation of data by utilizing topological losses derived from persistent homology. Through various experiments involving 3-dimensional shapes and single-cell RNA sequencing datasets, they demonstrate that Mapper graphs generated using optimized filters exhibit significantly improved structural qualities and better encapsulate topological features compared to those constructed with arbitrary filters. The findings underscore the utility of incorporating topological considerations in data representation, paving the way for more robust analytical techniques in data science.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=QZ1DVzr6N9&name=pdf" class="link-primary">https://openreview.net/attachment?id=QZ1DVzr6N9&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DiJiang: Efficient Large Language Models through Compact Kernelization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">linear attention</span>
<span class="badge bg-primary">kernelization</span>
<span class="badge bg-primary">discrete cosine transform</span>
<span class="badge bg-primary">efficient Transformers</span>
<span class="badge bg-primary">large language models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents DiJiang, a novel Frequency Domain Kernelization method that effectively transforms pre-trained Transformers into models with linear complexity while incurring minimal retraining costs.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">DiJiang is introduced as a new approach to reduce the computational burden of Transformer architectures by leveraging Frequency Domain Kernelization, specifically using Discrete Cosine Transform (DCT). The method employs a weighted Quasi-Monte Carlo technique to improve approximation efficiency, allowing the transformation of conventional attention mechanisms from quadratic to linear complexity without extensive retraining. Extensive experiments with various model sizes demonstrate that DiJiang achieves performance comparable to traditional Transformers while requiring significantly lower training costs (about 1/10) and offering faster inference speeds, thereby making it a promising solution for deploying large language models in resource-constrained environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=0uUHfhXdnH&name=pdf" class="link-primary">https://openreview.net/attachment?id=0uUHfhXdnH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Discovering Environments with XRM</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">environment discovery</span>
<span class="badge bg-primary">out-of-distribution generalization</span>
<span class="badge bg-primary">CROSS-RISKMINIMIZATION (XRM)</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">spurious correlations</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces CROSS-RISKMINIMIZATION (XRM), a novel method for automated discovery of environments in datasets that achieves oracle-level performance without requiring human annotations or early stopping.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of obtaining environment annotations, crucial for enhancing out-of-distribution (OOD) generalization in machine learning, which is often hampered by biases of human annotators. It presents CROSS-RISKMINIMIZATION (XRM), an innovative algorithm that trains two twin networks on random halves of the training data, guiding each to learn from the confident mistakes of its counterpart. This strategy creates an echo chamber that isolates environments based on spurious correlations, enabling XRM to automatically annotate both training and validation data without human input. The results demonstrate that algorithms utilizing XRM-inferred environments achieve oracle worst-group-accuracy across several benchmarks, overcoming the limitations of previous methods that rely on manually annotated environments. The findings open new avenues for robust generalization in large-scale datasets, making OOD generalization more accessible.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gPStP3FSY9&name=pdf" class="link-primary">https://openreview.net/attachment?id=gPStP3FSY9&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">discrete data</span>
<span class="badge bg-primary">score entropy</span>
<span class="badge bg-primary">language modeling</span>
<span class="badge bg-primary">generative performance</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Score Entropy Discrete Diffusion models (SEDD), which significantly improve performance on language modeling tasks compared to traditional autoregressive models and existing language diffusion paradigms by utilizing a novel score entropy loss.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges faced by diffusion models in generating discrete data, particularly in natural language processing tasks, where autoregressive models have dominated. The authors propose Score Entropy, a new loss function that extends score matching to discrete spaces, allowing for effective training of discrete diffusion models through the estimation of probability ratios. Experimental results demonstrate that SEDD outperforms current language diffusion models by reducing perplexity significantly (by 25-75%) and competes favorably with autoregressive models like GPT-2. The findings indicate that SEDD not only generates high-quality text samples without reliance on distribution annealing techniques but also allows for flexible prompting and infilling strategies, highlighting its potential as a robust alternative to traditional generative models in language tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=CNicRIVIPA&name=pdf" class="link-primary">https://openreview.net/attachment?id=CNicRIVIPA&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DITTO: Diffusion Inference-Time T-Optimization for Music Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Diffusion models</span>
<span class="badge bg-primary">Music generation</span>
<span class="badge bg-primary">Inference-time optimization</span>
<span class="badge bg-primary">Control mechanisms</span>
<span class="badge bg-primary">Feature matching</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents DITTO, a novel framework for controlling pre-trained text-to-music diffusion models at inference time through optimization of initial noise latents, achieving state-of-the-art performance across various music generation tasks without requiring additional training.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Diffusion Inference-Time T-Optimization (DITTO), a framework designed to enhance control over pre-trained text-to-music diffusion models by optimizing initial noise latents at inference time. This method allows for flexible and efficient control across multiple music generation tasks, including inpainting, outpainting, looping, and melody control, using any differentiable feature matching loss. DITTO demonstrates superior performance in controllability, audio quality, and computational efficiency compared to existing methods, such as state-of-the-art training-based and guidance-based techniques, without the need for extensive training. The findings highlight the expressive potential of the diffusion latent space in music generation, facilitating innovative editing capabilities while minimizing the impact on underlying model architectures.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=z5Ux2u6t7U&name=pdf" class="link-primary">https://openreview.net/attachment?id=z5Ux2u6t7U&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Does Label Smoothing Help Deep Partial Label Learning?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">deep partial label learning</span>
<span class="badge bg-primary">label smoothing</span>
<span class="badge bg-primary">classification performance</span>
<span class="badge bg-primary">theoretical analysis</span>
<span class="badge bg-primary">empirical validation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that label smoothing enhances the performance of deep partial label learning by deriving theoretical insights and validating them through extensive empirical experiments.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of deep partial label learning (PLL), particularly the detrimental effects of label noise on classification performance, and introduces label smoothing as a potential solution. The authors present a theoretical framework that includes lower and upper bounds on expected risk associated with label smoothing in PLL and derive an optimal smoothing rate based on the generalized ambiguity degree of candidate labels. They formulate a novel optimization algorithm named Label Smoothing-based Partial Label Learning (LS-PLL) and conduct extensive experiments on various datasets, showing that label smoothing significantly improves classification accuracy and helps in learning more distinguishable representations. The findings contribute to a deeper understanding of the interplay between label noise and model confidence in deep learning contexts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=drjjxmi2Ha&name=pdf" class="link-primary">https://openreview.net/attachment?id=drjjxmi2Ha&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">DoRA: Weight-Decomposed Low-Rank Adaptation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Low-Rank Adaptation</span>
<span class="badge bg-primary">Fine-Tuning</span>
<span class="badge bg-primary">Weight Decomposition</span>
<span class="badge bg-primary">Parameter Efficiency</span>
<span class="badge bg-primary">Multimodal Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel parameter-efficient fine-tuning method that decomposes pre-trained model weights into magnitude and direction components, enhancing learning capacity while avoiding additional inference costs compared to conventional methods like LoRA.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This research explores the limitations of existing parameter-efficient fine-tuning methods, particularly Low-Rank Adaptation (LoRA), which often fall short in accuracy compared to full fine-tuning. The authors propose DoRA, a new method that decomposes the pre-trained weights into magnitude and directional components, fine-tuning both while leveraging LoRA for the directional updates. Extensive experiments demonstrate that DoRA consistently outperforms LoRA across various language and vision tasks, achieving similar or better accuracy with a reduced number of trainable parameters. The findings suggest that DoRA can effectively bridge the performance gap between LoRA and full fine-tuning, representing a significant advancement in model training efficiency without incurring additional inference overhead.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3d5CIRG1n2&name=pdf" class="link-primary">https://openreview.net/attachment?id=3d5CIRG1n2&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">causal effect estimation</span>
<span class="badge bg-primary">networked interference</span>
<span class="badge bg-primary">targeted learning</span>
<span class="badge bg-primary">doubly robust estimator</span>
<span class="badge bg-primary">neural networks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel doubly robust causal effect estimator called TNet for estimating causal effects under networked interference by adapting targeted learning techniques within a neural network framework.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of estimating causal effects in the presence of networked interference, where interconnected units influence each other's outcomes, violating traditional assumptions in causal inference. The authors propose TNet, an end-to-end causal effect estimator that employs targeted learning techniques to achieve doubly robustness, ensuring that the estimator remains reliable even when one of the required nuisance models is misspecified. Through theoretical analysis, the paper establishes the conditions for achieving double robustness and demonstrates faster convergence rates compared to existing estimators. Extensive experimental results on two real-world networks using semisynthetic data illustrate the efficacy and stability of TNet in accurately estimating average and individual causal effects, highlighting its potential applicability in fields like epidemiology and marketing.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=5lI9wm4dws&name=pdf" class="link-primary">https://openreview.net/attachment?id=5lI9wm4dws&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Emergent Equivariance in Deep Ensembles</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Deep Ensembles</span>
<span class="badge bg-primary">Equivariance</span>
<span class="badge bg-primary">Neural Tangent Kernel</span>
<span class="badge bg-primary">Data Augmentation</span>
<span class="badge bg-primary">Machine Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that deep ensembles exhibit emergent equivariance for all inputs and training times when applying full data augmentation, a finding supported by theoretical analysis and empirical evidence.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the concept of emergent equivariance in deep ensembles, illustrating that such models can achieve equivariant predictions concerning data symmetries without requiring individual ensemble members to possess equivariant architectures. Utilizing neural tangent kernel (NTK) theory, the authors show that this equivariance arises when the models are trained with full data augmentation, maintaining its properties off-manifold and throughout the training process, including at initialization. Their theoretical insights are backed by extensive numerical experiments across various settings, including classification tasks and continuous symmetry applications. The results indicate that deep ensembles can effectively enforce equivariance, thereby enhancing model robustness and uncertainty estimation in critical applications such as medical diagnostics and machine learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=plXXbXjvQ9&name=pdf" class="link-primary">https://openreview.net/attachment?id=plXXbXjvQ9&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Environment Design for Inverse Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Inverse Reinforcement Learning</span>
<span class="badge bg-primary">Environment Design</span>
<span class="badge bg-primary">Sample Efficiency</span>
<span class="badge bg-primary">Robustness</span>
<span class="badge bg-primary">Bayesian Methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel framework for Inverse Reinforcement Learning (IRL) that adapts the design of environments to improve the sample efficiency and robustness of reward function inference from expert demonstrations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of low sample efficiency and lack of robustness in learning reward functions through Inverse Reinforcement Learning (IRL) in static environments. It proposes an adaptive environment design framework where a learner interacts with an expert across a sequence of modified environments, thus facilitating more informative demonstrations. The authors introduce an objective based on maximin Bayesian regret to guide the selection of these environments, leading to a more effective exploration of the reward space. Through multiple experiments in discrete maze tasks and continuous control problems, the results demonstrate that their approach significantly enhances the recovery of true reward functions and exhibits greater robustness to variations in environment dynamics compared to existing methods, such as fixed environments and domain randomization. The findings highlight the importance of strategically selecting environments to optimize the learning process in IRL.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Ar0dsOMStE&name=pdf" class="link-primary">https://openreview.net/attachment?id=Ar0dsOMStE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Equivariant Graph Neural Networks</span>
<span class="badge bg-primary">Ligand Binding Site Prediction</span>
<span class="badge bg-primary">Protein Structures</span>
<span class="badge bg-primary">Deep Learning</span>
<span class="badge bg-primary">Geometric Information</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces EquiPocket, an E(3)-equivariant Graph Neural Network framework designed to improve ligand binding site prediction in proteins by effectively utilizing local geometric and global structural features.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents EquiPocket, an innovative E(3)-equivariant Graph Neural Network (GNN) framework aimed at predicting ligand binding sites on proteins, addressing the limitations of previous CNN-based methods that rely on voxelization and are sensitive to protein size and rotation. EquiPocket integrates three core modules: a local geometric modeling module for extracting detailed geometric information from surface atoms, a global structure modeling module to characterize the overall chemical and spatial structures of proteins, and a surface message passing module that facilitates comprehensive feature aggregation from surface atoms. The proposed system significantly outperformed state-of-the-art methods in various benchmark tests, demonstrating enhanced accuracy and robustness against variability in protein sizes, ultimately contributing valuable insights for drug discovery and protein-ligand interaction studies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=1vGN3CSxVs&name=pdf" class="link-primary">https://openreview.net/attachment?id=1vGN3CSxVs&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Syntax-Aware Fill-in-the-Middle</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Code Generation</span>
<span class="badge bg-primary">Benchmarking</span>
<span class="badge bg-primary">Pretraining Strategies</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces the Syntax-Aware Fill-in-the-Middle (SAFIM) benchmark, demonstrating that pretraining strategies and data quality significantly affect the performance of Large Language Models (LLMs) in code generation tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the Syntax-Aware Fill-in-the-Middle (SAFIM) benchmark, aimed at evaluating Large Language Models (LLMs) on their capabilities in syntax-aware code completion across multiple programming languages. SAFIM consists of 17,720 examples with a focus on completing algorithmic blocks, control-flow expressions, and API function calls, sourced from recent code submissions post-April 2022 to reduce data contamination. The authors conduct comprehensive evaluations of 15 LLMs using various prompt designs and syntax-aware output post-processing techniques, revealing that pretraining methods have a more profound impact on performance than model size alone. The results indicate that Fill-in-the-Middle (FIM) pretraining enhances not only FIM proficiency but also Left-to-Right (L2R) inference capabilities, advocating for FIM as a primary objective in code LLM development while establishing SAFIM as a valuable resource for future research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jKYyFbH8ap&name=pdf" class="link-primary">https://openreview.net/attachment?id=jKYyFbH8ap&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">automatic heuristic design</span>
<span class="badge bg-primary">evolutionary computation</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">algorithm optimization</span>
<span class="badge bg-primary">combinatorial problems</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents the Evolution of Heuristic (EoH) framework, which effectively utilizes large language models and evolutionary computation for automated algorithm design, outperforming traditional heuristics in various optimization problems.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study introduces the Evolution of Heuristic (EoH) framework, combining large language models (LLMs) with evolutionary computation to automate the design of heuristics for complex optimization tasks. EoH progresses through an evolutionary search that evolves both the linguistic representations of heuristics and their corresponding code implementations, leveraging curated prompt strategies to enhance diversity and effectiveness. Experimental results demonstrate that EoH significantly outperforms traditional handcrafted heuristics and existing automatic heuristic design methods across three well-studied combinatorial optimization benchmarks: online bin packing, the traveling salesman problem, and flow shop scheduling, achieving high-quality solutions with drastically fewer resources compared to prior approaches. These findings indicate EoH's potential as a robust tool for efficient algorithm design in real-world applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=BwAkaxqiLB&name=pdf" class="link-primary">https://openreview.net/attachment?id=BwAkaxqiLB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">checkpoint compression</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">weight-momentum pruning</span>
<span class="badge bg-primary">quantization</span>
<span class="badge bg-primary">computational efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents an Extreme Checkpoint Compression (ExCP) framework that significantly reduces the storage size of training checkpoints for large language models while maintaining nearly lossless performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the urgent need for compressing checkpoints in the training of large language models (LLMs) due to their substantial computational and storage requirements. The authors propose a novel framework, ExCP, that efficiently utilizes the residuals of adjacent checkpoints along with a weight-momentum joint pruning method to discard redundant parameters without harming performance. Additionally, non-uniform quantization is employed to further reduce storage. Experimental evaluations demonstrate that ExCP achieves up to a 70× reduction in storage for models like Pythia-410M while maintaining strong accuracy across various downstream tasks, thereby suggesting its effectiveness for large-scale AI training scenarios.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=hlvKd7Vdxm&name=pdf" class="link-primary">https://openreview.net/attachment?id=hlvKd7Vdxm&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Expressivity and Generalization: Fragment-Biases for Molecular GNNs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Graph Neural Networks (GNNs)</span>
<span class="badge bg-primary">expressivity</span>
<span class="badge bg-primary">fragmentation</span>
<span class="badge bg-primary">molecular modeling</span>
<span class="badge bg-primary">generalization.</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Fragment-WL test to analyze fragment-biased GNNs and proposes a new architecture, FragNet, that significantly enhances expressiveness and generalization in molecular property prediction tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper aims to bridge the gap between theoretical expressiveness and empirical performance of Graph Neural Networks (GNNs) in molecular property prediction by introducing the Fragment-WL test, an extension of the Weisfeiler & Leman test, to assess fragment-biased GNNs. It presents a novel GNN architecture, FragNet, that leverages a new fragmentation scheme employing rings and paths as basic components, which enables the model to effectively learn complex molecular structures. Empirically, FragNet outperforms existing GNNs on synthetic and real-world datasets, including a 12% lower error on the ZINC dataset and boosting generalization capabilities compared to transformer-based models. The findings underscore the importance of fragment information in improving GNN performance for molecular-related tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=rPm5cKb1VB&name=pdf" class="link-primary">https://openreview.net/attachment?id=rPm5cKb1VB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Fast Co-Training under Weak Dependence via Stream-Based Active Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">co-training</span>
<span class="badge bg-primary">active learning</span>
<span class="badge bg-primary">label complexity</span>
<span class="badge bg-primary">weak dependence</span>
<span class="badge bg-primary">hypothesis classes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents efficient co-training algorithms that achieve low label complexity while learning various concept classes under the weak dependence assumption through a stream-based active learning framework.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the co-training problem within the context of stream-based active learning, providing efficient algorithms that maintain low label complexity while learning a range of natural concept classes under the weak dependence assumption. The authors establish a black-box reduction from the problem of co-training with label queries to online learning, demonstrating how existing online learning algorithms can be adapted for co-training. They also show that for hypothesis classes with finite VC dimension, such as unions of intervals and homogeneous halfspaces, their algorithms achieve errors independent of label complexity. The findings extend the applicability of co-training and highlight its efficiency in labeling, which is crucial given the high costs associated with labeled data in many machine learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=GqWy1wZKeE&name=pdf" class="link-primary">https://openreview.net/attachment?id=GqWy1wZKeE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Fast Timing-Conditioned Latent Audio Diffusion</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">audio generation</span>
<span class="badge bg-primary">latent diffusion</span>
<span class="badge bg-primary">variable-length outputs</span>
<span class="badge bg-primary">text conditioning</span>
<span class="badge bg-primary">music synthesis</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents "Stable Audio," an efficient latent diffusion model that generates long-form, variable-length stereo audio at 44.1kHz from text prompts, outperforming existing models in quality and structure while maintaining fast inference times.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Stable Audio, a latent diffusion model designed for generating high-fidelity, long-form stereo music and sound effects up to 95 seconds in length from text and timing inputs. Utilizing a fully-convolutional variational autoencoder for efficient latent representation, the model incorporates timing embeddings to allow fine control over the duration of generated audio. Evaluation through novel quantitative metrics demonstrates Stable Audio's competitive performance against state-of-the-art models in audio quality and semantic alignment with text prompts. The findings indicate that this model not only excels in speed and output fidelity but also leads in producing coherent musical structures, marking a significant advancement in the field of text-conditioned audio generation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jOlO8t1xdx&name=pdf" class="link-primary">https://openreview.net/attachment?id=jOlO8t1xdx&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">FedMBridge: Bridgeable Multimodal Federated Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Multimodal Federated Learning</span>
<span class="badge bg-primary">Architecture-personalized MFL</span>
<span class="badge bg-primary">Topology-aware Hypernetwork</span>
<span class="badge bg-primary">Knowledge Sharing</span>
<span class="badge bg-primary">Heterogeneity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents FedMBridge, a novel framework for Architecture-personalized Multimodal Federated Learning that efficiently manages statistical and architectural heterogeneity among clients using a topology-aware hypernetwork to enhance knowledge sharing.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of Architecture-personalized Multimodal Federated Learning (AMFL), where diverse clients with different neural architectures work together without a common architecture for model sharing. It introduces FedMBridge, which employs a topology-aware hypernetwork to facilitate efficient knowledge sharing by learning a bridge function that balances statistical and architectural heterogeneity. The authors demonstrate the effectiveness of their method through experiments across four AMFL simulation scenarios, showing improved performance over existing knowledge-sharing approaches. The findings suggest a significant advancement in handling architectural diversity in federated learning systems, enhancing data privacy and client collaboration.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jrHUbftLd6&name=pdf" class="link-primary">https://openreview.net/attachment?id=jrHUbftLd6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Flextron: Many-in-One Flexible Large Language Model</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">flexible model deployment</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">elastic networks</span>
<span class="badge bg-primary">input-adaptive routing</span>
<span class="badge bg-primary">post-training optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces FLEXTRON, a novel flexible architecture and optimization framework that enables the deployment of large language models with adjustable accuracy and latency targets, achieving superior performance without additional fine-tuning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The FLEXTRON framework addresses the challenges of resource-intensive training and deployment of large language models (LLMs) by providing a flexible architecture that allows models to adapt quickly to user-defined latency and accuracy requirements without the need for fine-tuning. It incorporates a nested elastic structure and innovative routing algorithms that enable efficient sub-network selection tailored to specific inputs. The authors demonstrate that FLEXTRON outperforms existing model variants and state-of-the-art elastic networks through a single pretraining run that uses a reduced number of tokens. Rigorous evaluations on models like GPT-3 and Llama-2 show that FLEXTRON enhances model efficiency while ensuring competitive performance across various natural language processing tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=9vKRhnflAs&name=pdf" class="link-primary">https://openreview.net/attachment?id=9vKRhnflAs&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Multi-granularity</span>
<span class="badge bg-primary">Graph Self-supervised Learning</span>
<span class="badge bg-primary">Knowledge Distillation</span>
<span class="badge bg-primary">Representation Learning</span>
<span class="badge bg-primary">Ensemble Methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Multi-granularity Graph Semantic Ensemble (MGSE), a plug-and-play framework that enhances existing graph self-supervised learning models through knowledge distillation by capturing multi-granular semantic representations, leading to improved performance across various downstream tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of capturing both abstract and fine-grained features in graph self-supervised learning (SSL) frameworks, which can limit their generalization across downstream tasks. To overcome this issue, the authors propose the Multi-granularity Graph Semantic Ensemble (MGSE), a framework that utilizes knowledge distillation by training multiple student models to learn from a single teacher model at different granular semantic levels. The experimental results demonstrate that MGSE significantly enhances the performance of several state-of-the-art graph SSL frameworks on various datasets, achieving improvements of up to 9.2%. Through comprehensive analysis and ablation studies, the authors highlight the importance of multi-granularity in effectively representing graph features, thereby improving model adaptability and accuracy in diverse applications such as molecular and protein property predictions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=JnA9IveEwg&name=pdf" class="link-primary">https://openreview.net/attachment?id=JnA9IveEwg&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">memory efficiency</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">gradient low-rank projection</span>
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">neural network training</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces GaLore, a novel training strategy that improves memory efficiency for large language models by employing gradient low-rank projection, significantly reducing optimizer states and enabling effective training on consumer-grade GPUs.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the significant memory challenges faced when training large language models (LLMs) by proposing GaLore, a memory-efficient training strategy based on gradient low-rank projection. GaLore allows for full-parameter learning while significantly reducing memory usage by up to 65.5% in optimizer states compared to conventional methods like low-rank adaptation (LoRA). The authors demonstrate the effectiveness of GaLore through experiments pre-training the LLaMA model on large datasets and fine-tuning RoBERTa on GLUE tasks, achieving comparable or superior performance to existing approaches while allowing the training of a 7B parameter model on a single GPU without using model parallelism or checkpointing techniques. Overall, GaLore offers a promising solution for practical LLM training in memory-constrained environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=hYHsrKDiX7&name=pdf" class="link-primary">https://openreview.net/attachment?id=hYHsrKDiX7&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Genie: Generative Interactive Environments</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Generative AI</span>
<span class="badge bg-primary">Interactive Environments</span>
<span class="badge bg-primary">Video Generation</span>
<span class="badge bg-primary">Unsupervised Learning</span>
<span class="badge bg-primary">Latent Actions</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Genie, an innovative generative interactive environment model that creates action-controllable virtual worlds from unlabelled video data, enabling users to generate and interact with complex environments in real-time.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Genie, a generative interactive environment model that learns from over 200,000 hours of unlabelled Internet gaming videos to create diverse, action-controllable virtual worlds, all trained in an unsupervised manner. With 11 billion parameters, Genie employs a unique architecture comprising a spatiotemporal video tokenizer, an autoregressive dynamics model, and a latent action model to facilitate frame-by-frame interactions without requiring ground-truth action labels. Experimental results highlight Genie’s ability to produce high-quality video generations and consistent latent actions across various inputs, including those from out-of-distribution sources, demonstrating its potential as a foundation model for future generalist agent training. This capability suggests profound implications for enhancing creativity and interactivity in generative AI applications, particularly in gaming and robotics.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bJbSbJskOS&name=pdf" class="link-primary">https://openreview.net/attachment?id=bJbSbJskOS&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">GPTSwarm: Language Agents as Optimizable Graphs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">language agents</span>
<span class="badge bg-primary">computational graphs</span>
<span class="badge bg-primary">optimization</span>
<span class="badge bg-primary">LLMs</span>
<span class="badge bg-primary">multi-agent systems</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces GPTSwarm, a framework that utilizes optimizable computational graphs to enhance the collaborative efficiency and performance of language agents based on large language models (LLMs).</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents GPTSwarm, a novel framework that conceptualizes language agents as optimizable computational graphs, where nodes represent specific operations (e.g., querying LLMs) and edges define the communication flow between these operations. The framework enables the recursive combination of agent graphs into composite swarms, facilitating greater inter-agent collaboration. The authors develop automatic optimization methods for both node-level prompts and edge connections, employing techniques like reinforcement learning for effective orchestration. Experiments demonstrate the framework's advantages across various benchmarks, showing improved performance in multi-agent settings while addressing challenges such as adversarial influences. The findings suggest that GPTSwarm can streamline the integration of LLMs into cohesive agent systems while enhancing their functionality through optimized communication structures.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uTC9AFXIhg&name=pdf" class="link-primary">https://openreview.net/attachment?id=uTC9AFXIhg&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">stochastic optimization</span>
<span class="badge bg-primary">high-probability convergence</span>
<span class="badge bg-primary">gradient clipping</span>
<span class="badge bg-primary">composite optimization</span>
<span class="badge bg-primary">variational inequalities</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents new stochastic methods for composite and distributed optimization that optimize high-probability convergence under heavy-tailed noise conditions by using gradient clipping on stochastic gradient differences.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenges of high-probability convergence for stochastic optimization methods in the context of composite and distributed problems, particularly when noise is heavy-tailed. It highlights the limitations of traditional gradient clipping approaches, which can hinder convergence even in noiseless scenarios. The authors propose novel algorithms that utilize clipping of stochastic gradient differences, proving these methods achieve tight convergence rates with optimal results. They also extend their findings to variational inequality problems, demonstrating that their new methods maintain high convergence performance while being robust to noise distributions, thus contributing significantly to the fields of machine learning and optimization.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=DBI6AuCD4a&name=pdf" class="link-primary">https://openreview.net/attachment?id=DBI6AuCD4a&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">honesty</span>
<span class="badge bg-primary">helpfulness</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">conversational norms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates how large language models (LLMs) navigate the trade-offs between honesty and helpfulness in conversation, revealing that reinforcement learning from human feedback enhances both qualities, while certain prompting strategies prioritize helpfulness over honesty.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the nuanced dynamics between honesty and helpfulness in large language models (LLMs) using experimental paradigms derived from cognitive psychology. By employing a signaling bandits approach, the authors analyze various LLMs to understand how they balance these conflicting values when generating responses. The experiments demonstrate that while reinforcement learning from human feedback improves both honesty and helpfulness, chain-of-thought prompting can skew models towards helpfulness at the expense of honesty. Notably, the latest iteration, GPT-4 Turbo, exhibits sensitivity to conversational context and shows human-like trade-off patterns, suggesting that LLMs can internalize and adapt to principles of effective communication. Overall, the findings offer insights into the internalized conversational values of LLMs and emphasize the potential to steer these values through prompting.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=685vj0lC9z&name=pdf" class="link-primary">https://openreview.net/attachment?id=685vj0lC9z&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">How Private are DP-SGD Implementations?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">DP-SGD</span>
<span class="badge bg-primary">Adaptive Batch Linear Queries</span>
<span class="badge bg-primary">Shuffling</span>
<span class="badge bg-primary">Poisson Subsampling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper reveals significant discrepancies in privacy guarantees between shuffling and Poisson subsampling techniques in differential private stochastic gradient descent (DP-SGD) implementations, advising caution in reporting privacy parameters.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the privacy guarantees associated with different batch sampling methods—shuffling and Poisson subsampling—within the context of Differentially Private Stochastic Gradient Descent (DP-SGD). The authors highlight a substantial gap in privacy assurances when interpreting adaptive batch linear queries (ABLQ) under these sampling methods, with shuffling being prevalent in practice but lacking rigorous privacy analysis. They demonstrate that while shuffling-based sampling yields weaker privacy loss amplification compared to Poisson subsampling at certain parameter settings, standard practices often overstate the privacy guarantees when reporting based on Poisson subsampling analysis. The findings indicate that the choice of batch sampling is critical in determining privacy outcomes and recommend care when conveying privacy parameters in DP-SGD implementations.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=xWI0MKwJSS&name=pdf" class="link-primary">https://openreview.net/attachment?id=xWI0MKwJSS&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Hybrid 2 Neural ODE Causal Modeling and an Application to Glycemic Response</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Hybrid modeling</span>
<span class="badge bg-primary">Causal modeling</span>
<span class="badge bg-primary">Neural ODE</span>
<span class="badge bg-primary">Glycemic response</span>
<span class="badge bg-primary">Type 1 diabetes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a hybrid causal modeling approach, Hybrid2Neural ODE Causal Modeling (H2NCM), that combines mechanistic ordinary differential equations (ODEs) with neural networks to improve predictive performance and causal validity in modeling glycemic responses in individuals with type 1 diabetes.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of integrating mechanistic models and neural networks to create interpretable, predictive models capable of capturing the causal dynamics of biological systems, specifically the glycemic response in type 1 diabetes during exercise. The authors propose a method called H2NCM, which combines predictive loss with a novel causal loss derived from ordinal knowledge of treatment effects among various interventions, thus preserving causal validity while leveraging the flexibility of neural networks. The results demonstrate that H2NCM achieves state-of-the-art predictive accuracy alongside valid causal inference, outperforming both purely mechanistic models and black-box models, thereby suggesting it as an effective approach in developing decision-support tools for diabetes management.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=GHZVjmaGQM&name=pdf" class="link-primary">https://openreview.net/attachment?id=GHZVjmaGQM&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">I/O Complexity of Attention, or How Optimal is FlashAttention?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">I/O complexity</span>
<span class="badge bg-primary">Attention mechanism</span>
<span class="badge bg-primary">FlashAttention</span>
<span class="badge bg-primary">Transformer models</span>
<span class="badge bg-primary">Matrix multiplication</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper proves that FlashAttention is optimal in terms of I/O complexity for computing self-attention in Transformers under certain memory constraints and introduces a new algorithm that further improves I/O complexity in smaller cache sizes.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the I/O complexity of the self-attention mechanism used in Transformer models, identifying it as a critical bottleneck for scaling performance. It specifically analyzes the performance of the FlashAttention algorithm, demonstrating that its I/O complexity is optimal across various cache sizes, particularly in the regime where the cache is comparatively small. The authors establish lower bounds for I/O complexity that match the upper bounds of FlashAttention, confirming its efficiency and proposing a better algorithm for scenarios where cache size is less than the square of the head dimension. They connect communication complexity with I/O complexity, opening up new avenues for proving lower bounds in future research while highlighting the theoretical and practical significance of efficient attention mechanisms in deep learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=MdPBVWTfwG&name=pdf" class="link-primary">https://openreview.net/attachment?id=MdPBVWTfwG&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Image Clustering with External Guidance</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">image clustering</span>
<span class="badge bg-primary">external knowledge</span>
<span class="badge bg-primary">mutual distillation</span>
<span class="badge bg-primary">WordNet</span>
<span class="badge bg-primary">deep learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel method for image clustering, called Text-Aided Clustering (TAC), which utilizes external textual knowledge from WordNet to significantly improve clustering performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of traditional image clustering methods that rely mainly on internal supervision signals by proposing an externally guided clustering technique known as Text-Aided Clustering (TAC). TAC leverages the semantic information of words from WordNet to enhance image feature discriminability, first by selecting relevant nouns that describe images and then by creating textual counterparts for each image. The method incorporates a cross-modal mutual distillation strategy to improve collaboration between the image and text modalities. Experimental results demonstrate that TAC achieves state-of-the-art clustering performance across benchmark datasets, even outperforming zero-shot methods, highlighting the potential of utilizing external knowledge in clustering tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=JSYN891WnB&name=pdf" class="link-primary">https://openreview.net/attachment?id=JSYN891WnB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Improving Transformers with Dynamically Composable Multi-Head Attention</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">attention mechanisms</span>
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">dynamic composability</span>
<span class="badge bg-primary">Multi-Head Attention</span>
<span class="badge bg-primary">model efficiency</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Dynamically Composable Multi-Head Attention (DCMHA), a novel method that enhances the expressive power and efficiency of Transformers by allowing attention heads to dynamically compose based on input.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper proposes Dynamically Composable Multi-Head Attention (DCMHA) as an improvement to the traditional Multi-Head Attention (MHA) used in Transformer architectures. DCMHA addresses the limitations of MHA, such as low expressiveness and redundancies among attention heads, by enabling dynamic composition of attention scores and weights according to the input. This is achieved through a Compose function that adaptively transforms attention matrices, significantly increasing the model's capacity for language modeling and achieving superior performance compared to existing models with lower computational costs. Extensive experiments demonstrate that models utilizing DCMHA, such as DCPythia, outperform larger counterparts like Pythia-12B across various tasks, indicating DCMHA's effectiveness in enhancing both training efficiency and model performance in natural language processing.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=RbiBKPtuHp&name=pdf" class="link-primary">https://openreview.net/attachment?id=RbiBKPtuHp&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Long-term treatments</span>
<span class="badge bg-primary">Causal inference</span>
<span class="badge bg-primary">Short-term experiments</span>
<span class="badge bg-primary">Offline reinforcement learning</span>
<span class="badge bg-primary">Estimation methods</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a novel methodology for estimating the long-term causal effects of continual interventions based solely on short-term experimental data, employing techniques from offline reinforcement learning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of inferring long-term causal effects from short-term experiments, particularly focusing on long-term treatments that require ongoing exposure to an intervention. Unlike traditional surrogate methods, which rely on intermediate variables, the authors propose a framework that leverages offline reinforcement learning principles to learn temporal dynamics directly from short-term observations. They introduce doubly-robust estimators to estimate average treatment effects (ATE) based on differing treatment policies and validate their approach through simulations. Results demonstrate that their method can accurately estimate long-term effects even when the experimental data is limited to short-term observations, providing a valuable tool for practical applications in fields like healthcare and digital platforms.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=lQ2o7JteMO&name=pdf" class="link-primary">https://openreview.net/attachment?id=lQ2o7JteMO&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">mutual information</span>
<span class="badge bg-primary">neural network</span>
<span class="badge bg-primary">real-time estimation</span>
<span class="badge bg-primary">test-time optimization</span>
<span class="badge bg-primary">attention mechanism</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces InfoNet, a neural network that efficiently estimates mutual information without the need for test-time optimization, achieving significant improvements in accuracy and speed over existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents InfoNet, a novel neural network designed to estimate mutual information (MI) between random variables directly from data streams, circumventing the inefficiencies of test-time optimization typical in existing methods like MINE. By leveraging an attention mechanism and extensive training on various distributions, InfoNet maintains a balance between efficiency and accuracy while generalizing well across different scenarios, including real-world applications like object discovery from videos. The authors demonstrate InfoNet's superior performance by conducting a comprehensive evaluation on diverse distribution families, showcasing its effectiveness and potential implications for real-time decision-making in intelligent systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=40hCy8n5XH&name=pdf" class="link-primary">https://openreview.net/attachment?id=40hCy8n5XH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Stochastic Convex Optimization</span>
<span class="badge bg-primary">Memorization</span>
<span class="badge bg-primary">Generalization</span>
<span class="badge bg-primary">Conditional Mutual Information</span>
<span class="badge bg-primary">Learning Algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper characterizes the tradeoff between the accuracy of learning algorithms and their memorization capacity through the lens of conditional mutual information in stochastic convex optimization.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors investigate the complex relationship between memorization and learning within stochastic convex optimization (SCO) frameworks, defining memorization in terms of the information a learning algorithm reveals about its training data. They employ conditional mutual information (CMI) to quantify this relationship and derive key results, including lower bounds linking CMI and excess error for both Lipschitz-bounded and strongly convex loss functions. Their findings indicate that a significant tradeoff exists: achieving low excess error requires high CMI, suggesting that memorization is an essential component for effective learning in certain SCO problems. The paper has notable implications for generalization bounds based on CMI, emphasizing limitations in existing frameworks and ruling out dimension-independent sample compression schemes.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=CyEJn71Z00&name=pdf" class="link-primary">https://openreview.net/attachment?id=CyEJn71Z00&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Interpreting and Improving Large Language Models in Arithmetic Calculation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">arithmetic calculation</span>
<span class="badge bg-primary">interpretability</span>
<span class="badge bg-primary">fine-tuning</span>
<span class="badge bg-primary">attention heads</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper explores the mechanisms by which large language models perform arithmetic calculations, revealing that a small fraction of attention heads and multi-layer perceptrons (MLPs) are critical, and demonstrates that selectively fine-tuning these components can significantly enhance calculation performance without degrading language capabilities.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The study investigates the inner workings of large language models (LLMs) in arithmetic computation, focusing on how they process operations like addition, subtraction, multiplication, and division. Through careful experiments, the authors identify that under 5% of attention heads and associated MLPs are primarily responsible for focusing on operands and operators during calculations. This insight allows the researchers to apply a precise fine-tuning strategy that enhances mathematical performance across various datasets, achieving notable improvements while maintaining the models' general task performance. The findings contribute to a deeper interpretation of LLMs and set the stage for further exploration of their arithmetic abilities in more complex tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=CfOtiepP8s&name=pdf" class="link-primary">https://openreview.net/attachment?id=CfOtiepP8s&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">DPO</span>
<span class="badge bg-primary">PPO</span>
<span class="badge bg-primary">RLHF</span>
<span class="badge bg-primary">LLM alignment</span>
<span class="badge bg-primary">reward optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The study investigates the performance comparison between Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) for aligning large language models (LLMs), revealing that PPO consistently outperforms DPO across various tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper examines the effectiveness of two alignment methods for large language models: Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) within the framework of Reinforcement Learning from Human Feedback (RLHF). Through both theoretical analysis and empirical experiments, the authors identify fundamental limitations in DPO, particularly its sensitivity to distribution shifts between model outputs and preference data, while highlighting key factors that enhance PPO performance, such as advantage normalization and large batch sizes. Experimental results show that PPO outperforms DPO in multiple benchmarks, including complex code generation challenges, suggesting that PPO is the more robust method for aligning LLMs with human preferences. The findings underscore the importance of analyzing and mitigating distribution shifts for better model performance, particularly when deploying models in practical applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6XH8R7YrSk&name=pdf" class="link-primary">https://openreview.net/attachment?id=6XH8R7YrSk&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">out-of-distribution generalization</span>
<span class="badge bg-primary">LCA distance</span>
<span class="badge bg-primary">class taxonomy</span>
<span class="badge bg-primary">Vision-Language Models</span>
<span class="badge bg-primary">benchmark evaluation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces the LCA-on-the-Line framework, leveraging the Lowest Common Ancestor (LCA) distance to predict out-of-distribution (OOD) performance of models based on in-distribution (ID) accuracy, demonstrating a strong correlation between LCA distance and OOD accuracy across various models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of predicting the out-of-distribution performance of models using in-distribution measurements by proposing the LCA-on-the-Line framework, which utilizes the Lowest Common Ancestor (LCA) distance based on predefined class hierarchies like WordNet. By evaluating 75 models, including both Vision Models and Vision-Language Models, the authors reveal a robust linear relationship between in-distribution LCA distance and OOD accuracy, particularly highlighting how this metric outperforms traditional measures of ID accuracy. The method offers insights into the generalization capabilities of models trained with different types of supervision and demonstrates its effectiveness by aligning model predictions through soft labels or prompt engineering. Additionally, the authors present a technique for constructing latent class taxonomies via K-means clustering, showcasing the resilience of the LCA distance across various hierarchies. The findings indicate the potential for enhancing model generalization through structured semantic knowledge derived from class taxonomies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=HPXRzM9BYZ&name=pdf" class="link-primary">https://openreview.net/attachment?id=HPXRzM9BYZ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning to Model the World With Language</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">language understanding</span>
<span class="badge bg-primary">future prediction</span>
<span class="badge bg-primary">multimodal agents</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">world models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents Dynalang, a multimodal agent that integrates diverse language inputs to improve its understanding and prediction of the environment through a self-supervised learning approach.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces Dynalang, an advanced agent designed to leverage varied forms of language, such as environment descriptions, corrections, and instructions, to enhance its ability to predict future states and rewards in the world. By employing a multimodal world model that predicts text and image representations based on learned experiences, Dynalang realizes the integration of language understanding and future prediction as a unified learning objective. The authors demonstrate that Dynalang outperforms existing language-conditioned policies across multiple environments, showcasing its capacity to generalize and handle diverse language types successfully. Additionally, the model benefits from the ability to pretrain on text-only data, further enhancing its performance and capabilities, indicating significant potential for developing more interactive and capable AI agents.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=7dP6Yq9Uwv&name=pdf" class="link-primary">https://openreview.net/attachment?id=7dP6Yq9Uwv&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Learning Useful Representations of Recurrent Neural Network Weight Matrices</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">RNN weights</span>
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">functionalist approach</span>
<span class="badge bg-primary">probing methods</span>
<span class="badge bg-primary">self-supervised learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces novel techniques for learning useful representations of Recurrent Neural Network (RNN) weights, demonstrating the superiority of functionalist probing approaches in predicting RNN behavior compared to mechanistic methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of learning effective representations of RNN weight matrices essential for analyzing RNN behavior and enhancing downstream tasks. It distinguishes between mechanistic methods, which examine the weight matrices directly, and functionalist approaches that focus on the functionality of RNNs through probing techniques. The authors propose six neural network architectures for processing RNN weights, including interactive and non-interactive probing methods, and establish a theoretical framework supporting their effectiveness. They release two datasets comprising thousands of RNNs trained on different tasks, then evaluate the proposed methods through an emulation-based self-supervised learning technique. Results indicate that interactive probing outperforms other techniques, especially on complex tasks like formal languages, suggesting significant implications for representation learning in deep learning models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=QBj7Uurdwf&name=pdf" class="link-primary">https://openreview.net/attachment?id=QBj7Uurdwf&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Less is More: on the Over-Globalizing Problem in Graph Transformers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Graph Transformer</span>
<span class="badge bg-primary">Over-globalizing</span>
<span class="badge bg-primary">Bi-Level Global Attention</span>
<span class="badge bg-primary">Collaborative Training</span>
<span class="badge bg-primary">Node Classification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper identifies the over-globalizing problem in Graph Transformers and introduces CoBFormer, a novel architecture that combines a Bi-Level Global Attention module with collaborative training to enhance performance in graph-based tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the over-globalizing issue in Graph Transformers, where the global attention mechanism excessively focuses on distant nodes while neglecting closer, more informative nodes. To address this, the authors propose CoBFormer, which consists of a Bi-Level Global Attention (BGA) module that splits attention processing into intra-cluster and inter-cluster components, along with a Graph Convolution Network (GCN) as a local information module. Collaborative training is introduced to integrate insights from both the GCN and BGA, improving the model's generalization ability. Through extensive experiments across various datasets, the authors demonstrate that CoBFormer effectively mitigates the over-globalizing problem and outperforms state-of-the-art Graph Transformers in node classification tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=uKmcyyrZae&name=pdf" class="link-primary">https://openreview.net/attachment?id=uKmcyyrZae&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Listenable Maps for Audio Classifiers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">audio classification</span>
<span class="badge bg-primary">interpretability</span>
<span class="badge bg-primary">masking</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">user study</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Listenable Maps for Audio Classifiers (L-MAC), a novel method that generates listenable, faithful interpretations of audio classifier decisions using binary masks derived from latent representations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents Listenable Maps for Audio Classifiers (L-MAC), a post-hoc interpretation technique designed to improve the transparency of deep learning audio classifiers. L-MAC employs a decoder to create binary masks that highlight significant audio segments, enhancing interpretability by producing listenable audio outputs. The decoder is trained to maximize the classifier's confidence on the masked sections while minimizing it for the unmasked segments. Experimental evaluations demonstrated that L-MAC consistently outperformed various baseline interpretation methods across multiple metrics and user studies, indicating a preference for the generated interpretations. The findings suggest that L-MAC offers a promising approach to making audio classification models more interpretable while maintaining user engagement through listenable outputs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=kAfYYg6PX8&name=pdf" class="link-primary">https://openreview.net/attachment?id=kAfYYg6PX8&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Locality-Sensitive Hashing</span>
<span class="badge bg-primary">Efficient Transformers</span>
<span class="badge bg-primary">Point Clouds</span>
<span class="badge bg-primary">High-Energy Physics</span>
<span class="badge bg-primary">Geometric Deep Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents the LSH-based Efficient Point Transformer (HEPT), which enhances point cloud processing in scientific applications by integrating locality-sensitive hashing to achieve near-linear complexity while maintaining high accuracy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The study introduces HEPT, a novel transformer model aimed at improving the efficiency of large-scale point cloud processing in scientific fields such as high-energy physics and astrophysics. The model overcomes the computational challenges posed by traditional graph neural networks and standard transformers by leveraging locality-sensitive hashing (LSH) to minimize errors while ensuring computational regularity. A quantitative analysis of various sparseness techniques highlighted the superior performance of OR & AND-construction LSH methods in kernel approximation. HEPT demonstrates exceptional performance on critical high-energy physics tasks, significantly outpacing existing models in both accuracy and computational speed. The results highlight HEPT's potential in accelerating large-scale scientific data processing while effectively addressing the unique challenges of point cloud data.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=vJx6fld6l0&name=pdf" class="link-primary">https://openreview.net/attachment?id=vJx6fld6l0&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LoRA Training in the NTK Regime has No Spurious Local Minima</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">LoRA</span>
<span class="badge bg-primary">Neural Tangent Kernel</span>
<span class="badge bg-primary">Fine-tuning</span>
<span class="badge bg-primary">Low-rank adaptation</span>
<span class="badge bg-primary">Generalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a theoretical analysis showing that low-rank adaptation (LoRA) in the neural tangent kernel (NTK) regime has no spurious local minima, enabling successful gradient descent to find low-rank solutions which generalize well.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper theoretically investigates low-rank adaptation (LoRA) for parameter-efficient fine-tuning of large language models within the neural tangent kernel (NTK) regime, addressing gaps in the understanding of its effectiveness. The authors demonstrate that full fine-tuning admits low-rank solutions and that using LoRA eliminates spurious local minima, thereby allowing (stochastic) gradient descent to effectively find low-rank global minima. Furthermore, they establish that these solutions exhibit good generalization properties. The work lays foundational ground for optimizing the fine-tuning process and suggests that using lower ranks can lead to slower convergence due to unfavorable landscapes, while ensuring that LoRA remains a practical method for efficient model adaptation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=s1sdx6vNsU&name=pdf" class="link-primary">https://openreview.net/attachment?id=s1sdx6vNsU&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Low-Cost High-Power Membership Inference Attacks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">membership inference attacks</span>
<span class="badge bg-primary">RMIA</span>
<span class="badge bg-primary">privacy auditing</span>
<span class="badge bg-primary">statistical testing</span>
<span class="badge bg-primary">machine learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Robust Membership Inference Attacks (RMIA), a highly effective and low-cost approach for inferring membership in machine learning models, outperforming existing methods across various scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents a novel statistical test for membership inference attacks called Robust Membership Inference Attacks (RMIA), which offers superior effectiveness and efficiency compared to prior methods. By fine-tuning the null hypothesis in likelihood ratio tests and leveraging limited reference models, RMIA enhances the ability to distinguish between member and non-member data points, achieving high true positive rates even at very low false positive rates. Extensive empirical evaluations demonstrate the robustness of RMIA under practical computation constraints and distribution shifts, making it suitable for real-world privacy auditing tasks in machine learning. The method significantly advances the field by addressing the limitations of existing MIA techniques, particularly in low-resource settings.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=sT7UJh5CTc&name=pdf" class="link-primary">https://openreview.net/attachment?id=sT7UJh5CTc&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">graph clustering</span>
<span class="badge bg-primary">unknown cluster number</span>
<span class="badge bg-primary">structural information</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">hyperbolic space</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces the Lorentz Structural Entropy Neural Network (LSEnet) for graph clustering without a predefined cluster number, leveraging differentiable structural information in hyperbolic space to optimize node assignments.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study addresses the challenge of graph clustering when the number of clusters is unknown, which has been a significant limitation in existing deep learning approaches. The authors propose a novel framework called LSEnet, which integrates differentiable structural information (DSI) within the Lorentz model of hyperbolic space to learn an optimal partitioning tree of nodes. By formulating DSI as a new graph clustering objective, the method effectively reveals cluster structures without prior knowledge of cluster counts. Empirical evaluations demonstrate that LSEnet outperforms state-of-the-art models on several real-world datasets, highlighting its effectiveness and efficiency in handling graph clustering tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=L6SRXG92s6&name=pdf" class="link-primary">https://openreview.net/attachment?id=L6SRXG92s6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">image retrieval</span>
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">open-ended instructions</span>
<span class="badge bg-primary">multimodal models</span>
<span class="badge bg-primary">semantic relations</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces MagicLens, a series of self-supervised image retrieval models that significantly enhance retrieval accuracy by interpreting open-ended text instructions and leveraging naturally occurring image pairs from the web.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents MagicLens, an innovative approach to image retrieval that addresses the limitations of existing models by incorporating open-ended text instructions to capture diverse user intents beyond visual similarity. Utilizing a dataset of 36.7 million triplets generated from naturally co-occurring image pairs on web pages, MagicLens employs self-supervised learning techniques to better understand and interpret complex queries. The results show that MagicLens outperforms state-of-the-art methods on multiple benchmarks while maintaining a smaller model size, demonstrating its effectiveness in handling complex and nuanced retrieval tasks. This advancement holds significant implications for real-world applications of visual search and multimodal interactions, suggesting a path forward for future developments in image retrieval systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Zc22RDtsvP&name=pdf" class="link-primary">https://openreview.net/attachment?id=Zc22RDtsvP&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Making Old Things New: A Unified Algorithm for Differentially Private Clustering</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Clustering Algorithm</span>
<span class="badge bg-primary">K-means</span>
<span class="badge bg-primary">Continual Observation</span>
<span class="badge bg-primary">Privacy Models</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a unified algorithm for various models of differentially private clustering, successfully adapting a 20-year-old algorithm to provide improved results across multiple privacy settings, including continual observation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the problem of differentially private clustering, addressing the complexities of various privacy models such as centralized, local, and shuffle differential privacy, and introducing a unified algorithm that modifies an existing greedy algorithm from Mettu and Plaxton to work across these models. The authors focus on k-means clustering and highlight how their approach can achieve multiplicative approximation and additive error guarantees while maintaining privacy. The algorithm leverages techniques such as dimension reduction, generalized summation for estimating costs, and the exponential mechanism for selections, demonstrating significant improvements and extending results to a new continual observation setting, thereby enhancing the robustness and applicability of differential privacy in unsupervised learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3ajK5xplDL&name=pdf" class="link-primary">https://openreview.net/attachment?id=3ajK5xplDL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Mean-field Chaos Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Mean-field theory</span>
<span class="badge bg-primary">Generative models</span>
<span class="badge bg-primary">High-dimensional data</span>
<span class="badge bg-primary">Score-based diffusion models</span>
<span class="badge bg-primary">Chaotic dynamics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Mean-field Chaos Diffusion Models (MF-CDMs) as a new class of score-based generative models that effectively handle high-cardinality data distributions by utilizing principles from mean-field theory.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a novel approach to score-based generative models (SGMs) through the development of Mean-field Chaos Diffusion Models (MF-CDMs), which aim to address the challenges posed by high-dimensional and high-cardinality data. By leveraging the propagation of chaos concept from mean-field theory, the authors propose a score-matching method adapted for infinite-dimensional chaotic particle systems. Theoretical analyses confirm the effectiveness of MF-CDMs in managing large data structures, specifically in tasks such as 3D point cloud generation. Experimental results demonstrate superior performance compared to existing methods, indicating that MF-CDMs can efficiently handle high-dimensional data complexities while maintaining scalability and robustness.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=lgcFX4VFrM&name=pdf" class="link-primary">https://openreview.net/attachment?id=lgcFX4VFrM&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Multimodal Language Models</span>
<span class="badge bg-primary">Evaluation Benchmark</span>
<span class="badge bg-primary">Judging Tasks</span>
<span class="badge bg-primary">Human Preferences</span>
<span class="badge bg-primary">Hallucination Biases</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the MLLM-as-a-Judge benchmark to evaluate the performance of Multimodal Large Language Models (MLLMs) in judging tasks and highlights significant discrepancies from human evaluations, particularly in scoring and ranking tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents the MLLM-as-a-Judge benchmark aimed at assessing the capabilities of Multimodal Large Language Models (MLLMs) in various judging tasks, including scoring evaluation, pair comparison, and batch ranking. It entails a comprehensive dataset of 4,414 image-instruction pairs and evaluates the performance of several advanced MLLMs, notably GPT-4V, across different tasks. The results indicate that while MLLMs like GPT-4V show considerable effectiveness in pair comparison, significant gaps remain in scoring evaluation and batch ranking when compared to human preferences. The study reveals issues such as biases, hallucinations, and inconsistencies in judgment within MLLMs, suggesting the necessity for further improvements and research to enhance their reliability as evaluators in complex multimodal scenarios.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dbFEFHAD79&name=pdf" class="link-primary">https://openreview.net/attachment?id=dbFEFHAD79&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">AI-generated content</span>
<span class="badge bg-primary">Peer review</span>
<span class="badge bg-primary">Language models</span>
<span class="badge bg-primary">Statistical estimation</span>
<span class="badge bg-primary">Conference analysis</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a method for estimating the fraction of peer review texts likely modified by large language models, revealing significant changes in content and tendencies associated with user behavior post-ChatGPT release.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study investigates the impact of large language models (LLMs), specifically ChatGPT, on academic peer review processes in top AI conferences by developing a statistical method to quantify the proportion of text substantially altered or generated by AI. Employing maximum likelihood estimation (MLE), the authors analyze a corpus of peer reviews from conferences like ICLR, NeurIPS, and EMNLP, finding between 6.5% and 16.9% of text may have been generated or modified beyond mere proofreading. The analysis demonstrates that LLM-generated text appears more frequently in low-confidence reviews, those submitted close to deadlines, and lacks scholarly citations. The findings prompt a call for interdisciplinary research into the evolving use of AI in academic practices, emphasizing the potentially homogenizing effects of AI on unique reviewer feedback and the integrity of the peer review system.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=bX3J7ho18S&name=pdf" class="link-primary">https://openreview.net/attachment?id=bX3J7ho18S&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">neuronal morphology</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">MorphGrower</span>
<span class="badge bg-primary">variational autoencoder</span>
<span class="badge bg-primary">neuroscience</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents MorphGrower, a novel synchronized layer-by-layer approach for generating plausible neuronal morphologies that significantly outperforms existing methods in realism and structural validity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The objective of this research is to generate realistic neuronal morphologies, which are crucial for studying brain function and neurodegenerative disorders, using computational methods that reduce reliance on expensive data collection. The authors propose MorphGrower, a deep learning model that generates morphologies layer by layer while ensuring topological validity by concurrently generating sibling branches, contrasting with prior methods that relied on variable-length 3D walks. The results demonstrate that MorphGrower outperforms the existing learning-based method MorphV AE, achieving higher plausibility in terms of morphological metrics and electrophysiological response simulation. This advancement holds promise for augmenting the available morphology database and enhancing understanding of neuronal structure and function, thereby contributing to neuroscience research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ZTN866OsGx&name=pdf" class="link-primary">https://openreview.net/attachment?id=ZTN866OsGx&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">densest subgraph</span>
<span class="badge bg-primary">multiplicative weights update</span>
<span class="badge bg-primary">area convexity</span>
<span class="badge bg-primary">random coordinate descent</span>
<span class="badge bg-primary">optimization algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents improved algorithms for the densest subgraph problem using multiplicative weights update and area convexity, achieving faster convergence and practical efficiency.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the densest subgraph problem along with its generalization to dense subgraph decompositions in graphs. The authors develop two key algorithms: one utilizing a multiplicative weights update (MWU) framework that converges in O(log m²) iterations with nearly-linear time per iteration, and another leveraging area convexity techniques that improves iteration complexity to O(log m) while matching the best known theoretical runtime. Additionally, the authors introduce a practical iterative algorithm for dense subgraph decomposition with a linear convergence rate. Empirically, these algorithms demonstrate efficiency on large graphs, exhibiting competitive performance compared to existing methods, even those with weaker theoretical guarantees. The results contribute to advancing optimization techniques in machine learning and data mining.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=d2E2i5rJ4x&name=pdf" class="link-primary">https://openreview.net/attachment?id=d2E2i5rJ4x&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Text-to-Speech</span>
<span class="badge bg-primary">Diffusion Models</span>
<span class="badge bg-primary">Factorized Codec</span>
<span class="badge bg-primary">Zero-Shot Synthesis</span>
<span class="badge bg-primary">Speech Disentanglement</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">NaturalSpeech 3 is a state-of-the-art text-to-speech system that utilizes factorized diffusion models and a novel neural codec to achieve high-quality, zero-shot speech synthesis by disentangling speech attributes.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents NaturalSpeech 3, an advanced text-to-speech (TTS) system that addresses limitations in existing models by employing factorized diffusion models and a new neural codec called FACodec. This system decomposes speech into distinct attributes—content, prosody, acoustic details, and timbre—allowing for more effective and efficient speech generation. Through extensive experiments, NaturalSpeech 3 demonstrates superior performance compared to state-of-the-art TTS systems across various metrics, including speech quality, speaker similarity, prosody, and intelligibility. The results highlight the potential for enhanced controllability and zero-shot capabilities in speech synthesis, setting a significant milestone in TTS research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dVhrnjZJad&name=pdf" class="link-primary">https://openreview.net/attachment?id=dVhrnjZJad&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Neural Collapse</span>
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Representation Learning</span>
<span class="badge bg-primary">Noisy Gradient Descent</span>
<span class="badge bg-primary">Feature Shift Parameter</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the intersection of Neural Collapse and Differential Privacy, demonstrating that pre-trained models enhance the robustness of differentially private fine-tuning through a dimension-independent feature representation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores how large-scale representation learning through pre-trained models improves differentially private (DP) learning in downstream tasks, particularly under the phenomenon of Neural Collapse (NC), where features converge toward distinct points. By defining a feature shift parameter that quantifies the proximity of actual features to ideal NC features, the authors establish that misclassification errors can remain dimension-independent given that this parameter stays below a certain threshold. They empirically validate their theoretical findings through experiments on datasets like CIFAR-10, showing that better pre-trained models yield superior feature representations, while also demonstrating that DP fine-tuning is more sensitive to perturbations compared to non-DP methods. Their results suggest strategies such as feature normalization and dimensionality reduction, particularly PCA, to enhance the robustness of differentially private fine-tuning, ultimately providing insights into how to leverage Neural Collapse for improved privacy-utility trade-offs in deep learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=7rrN6E4KU0&name=pdf" class="link-primary">https://openreview.net/attachment?id=7rrN6E4KU0&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">NExT-GPT: Any-to-Any Multimodal LLM</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">any-to-any multimodal LLM</span>
<span class="badge bg-primary">multimodal understanding</span>
<span class="badge bg-primary">NExT-GPT</span>
<span class="badge bg-primary">instruction tuning</span>
<span class="badge bg-primary">cross-modal generation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents NExT-GPT, an end-to-end any-to-any multimodal large language model capable of understanding and generating content across text, image, video, and audio modalities.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The research introduces NExT-GPT, a novel any-to-any multimodal large language model (MM-LLM) designed to accept and produce inputs in diverse combinations of text, image, video, and audio. The system integrates a large language model with multimodal adaptors and diffusion decoders, allowing for enhanced multimodal understanding and generation with minimal parameter tuning (1%) and low-cost training. Additionally, the authors propose a modality-switching instruction tuning (MosIT) approach, backed by a manually curated dataset that facilitates complex cross-modal semantic comprehension and content creation. Experimental results demonstrate that NExT-GPT exhibits superior performance in various multimodal tasks, highlighting its potential for more human-like AI interactions and paving the way for future developments in universal AI agents.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=NZQkumsNlf&name=pdf" class="link-primary">https://openreview.net/attachment?id=NZQkumsNlf&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Offline Actor-Critic Reinforcement Learning Scales to Large Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">offline reinforcement learning</span>
<span class="badge bg-primary">actor-critic algorithms</span>
<span class="badge bg-primary">scaling laws</span>
<span class="badge bg-primary">Perceiver architecture</span>
<span class="badge bg-primary">multi-task training</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that offline actor-critic reinforcement learning can scale effectively to large models, outperforming strong behavioral cloning baselines in multi-task training on various continuous control tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents the Perceiver-Actor-Critic (PAC) model, a scalable offline actor-critic reinforcement learning approach that leverages large models, such as transformers, to improve training on diverse datasets that include sub-optimal and expert behaviors. The authors conducted extensive experiments on 132 continuous control tasks, revealing that simple offline actor-critic algorithms can outperform traditional behavioral cloning methods and exhibit similar scaling laws to supervised learning. Key innovations include a KL-regularized RL objective and architectural modifications to enable efficient training with heterogeneous input modalities. The findings highlight the potential of offline RL to learn robust multi-task policies in robotics and suggest a gradual transition from behavioral cloning to reinforcement learning to enhance performance.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=tl2qmO5kpD&name=pdf" class="link-primary">https://openreview.net/attachment?id=tl2qmO5kpD&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">OMPO: A Unified Framework for RL under Policy and Dynamics Shifts</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">policy shifts</span>
<span class="badge bg-primary">dynamics shifts</span>
<span class="badge bg-primary">occupancy matching</span>
<span class="badge bg-primary">policy optimization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents Occupancy-Matching Policy Optimization (OMPO), a unified framework for online reinforcement learning that effectively addresses policy and dynamics shifts through transition occupancy matching, leading to superior performance across various scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the challenges of training reinforcement learning (RL) policies using data from environments exhibiting shifts in policy and dynamics, which can lead to distribution discrepancies and suboptimal performance. To address these challenges, the authors propose a unified framework called Occupancy-Matching Policy Optimization (OMPO), which introduces a surrogate policy learning objective based on transition occupancy discrepancies and casts it into a min-max optimization problem. OMPO employs a specialized actor-critic structure equipped with a distribution discriminator and aims to handle diverse scenarios, including stationary and non-stationary dynamics as well as domain adaptation. Extensive experiments conducted across various environments demonstrate that OMPO outperforms existing specialized baselines, showing particular strength in sim-to-real applications in RL-based robotics.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=R83VIZtHXA&name=pdf" class="link-primary">https://openreview.net/attachment?id=R83VIZtHXA&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">On the Last-Iterate Convergence of Shuffling Gradient Methods</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">shuffling gradient methods</span>
<span class="badge bg-primary">convergence rates</span>
<span class="badge bg-primary">stochastic optimization</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">strongly convex functions</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper establishes the first theoretical last-iterate convergence rates for shuffling gradient methods with respect to the objective value, significantly advancing the understanding of their performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the convergence properties of shuffling gradient methods, which include Random Reshuffle, Shuffle Once, and Incremental Gradient techniques, widely utilized in machine learning due to their computational efficiency. Prior theoretical analyses primarily focused on average iterates and lacked coverage on last-iterate convergence, especially in non-strongly convex settings and with general regularizers. The authors prove new last-iterate convergence rates related to objective value gaps, showing that their results align closely with established lower bounds. By employing novel proof techniques influenced by recent advancements in stochastic optimization, the research asserts that the last iterate of these methods converges effectively, addressing significant gaps in current literature and enhancing their applicability in constrained optimization scenarios.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Xdy9bjwHDu&name=pdf" class="link-primary">https://openreview.net/attachment?id=Xdy9bjwHDu&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">online matching</span>
<span class="badge bg-primary">stochastic rewards</span>
<span class="badge bg-primary">adversarial reinforcement learning</span>
<span class="badge bg-primary">competitive ratio</span>
<span class="badge bg-primary">algorithm design</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel adversarial reinforcement learning framework that improves the competitive ratio for online matching with stochastic rewards from 0.621 to 0.597, demonstrating how reinforcement learning can aid in understanding the hardness of online problems.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The study investigates the online matching with stochastic rewards (OMSR) problem, an extension of the online bipartite matching problem with unknown optimal competitive ratios. The authors develop an adversarial reinforcement learning framework that trains two agents: one for generating challenging instances and another for learning robust algorithms. Through iterative training, the algorithm agent achieves significant empirical gains in performance, surpassing state-of-the-art benchmarks, while also identifying structural properties of the hard instances that allow for a theoretical improvement of the upper bound on the competitive ratio of OMSR from 0.621 to 0.597. This work illustrates how reinforcement learning can not only enhance algorithm performance but also deepen the theoretical understanding of online optimization challenges.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=TujtZgdRxB&name=pdf" class="link-primary">https://openreview.net/attachment?id=TujtZgdRxB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Bilevel optimization</span>
<span class="badge bg-primary">Nonconvex problems</span>
<span class="badge bg-primary">Polyak-Łojasiewicz condition</span>
<span class="badge bg-primary">Hessian-free methods</span>
<span class="badge bg-primary">Gradient complexity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces an efficient Hessian/Jacobian-free method for solving nonconvex bilevel optimization problems that satisfy the Polyak-Łojasiewicz condition, achieving optimal convergence complexity and gradient efficiency.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses nonconvex bilevel optimization problems commonly encountered in machine learning where the lower-level function meets the Polyak-Łojasiewicz (PL) condition. Existing methods typically rely on convexity assumptions, making them inefficient for this problem class, particularly due to their reliance on costly Hessian or Jacobian computations. The authors propose a novel Hessian/Jacobian-free method, referred to as HJFBiO, which utilizes finite-difference estimators and a new projection operator to simplify the optimization process. They demonstrate that HJFBiO achieves an optimal convergence rate of \( O(1/T) \) and an optimal gradient complexity of \( O(1) \) for locating \(\epsilon\)-stationary solutions. Numerical experiments conducted on bilevel PL games and hyper-representation learning tasks confirm the effectiveness and efficiency of the proposed method, establishing it as a substantial improvement over existing approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=eZiQWM5U0E&name=pdf" class="link-primary">https://openreview.net/attachment?id=eZiQWM5U0E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Parameterized Physics-informed Neural Networks for Parameterized PDEs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Parameterized PDEs</span>
<span class="badge bg-primary">Physics-informed Neural Networks</span>
<span class="badge bg-primary">P2INNs</span>
<span class="badge bg-primary">Model Efficiency</span>
<span class="badge bg-primary">Scientific Machine Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents parameterized physics-informed neural networks (P2INNs), which significantly enhance the performance and efficiency of solving parameterized partial differential equations compared to existing methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces parameterized physics-informed neural networks (P2INNs) to address the limitations of conventional physics-informed neural networks (PINNs) in solving parameterized partial differential equations (PDEs) with high accuracy and efficiency. P2INNs incorporate a separate encoder to extract a latent representation of the PDE parameters and improve the solution formulation. The authors demonstrate through extensive empirical evaluations on benchmark PDEs that P2INNs outperform traditional PINN approaches across multiple scenarios, achieving significant reductions in both absolute and relative errors while eliminating the need for repetitive training. The findings suggest P2INNs are robust tools for efficiently modeling complex physical systems described by parameterized PDEs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=n3yYrtt9U7&name=pdf" class="link-primary">https://openreview.net/attachment?id=n3yYrtt9U7&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Pausing Policy Learning in Non-stationary Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">non-stationary environments</span>
<span class="badge bg-primary">policy updates</span>
<span class="badge bg-primary">dynamic regret</span>
<span class="badge bg-primary">aleatoric uncertainty</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper demonstrates that strategically pausing policy learning in non-stationary reinforcement learning environments leads to improved performance and reduced dynamic regret compared to continuous policy updates.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenges of real-time inference in non-stationary reinforcement learning environments, proposing a forecasting online reinforcement learning framework that incorporates the strategic pausing of policy updates. It argues against the common belief that continuous decision updates are optimal, instead showing that there are benefits to having non-zero policy hold durations to manage aleatoric uncertainty from environmental changes. The authors theoretically derive the optimal ratio between policy update and hold durations, compute a sharper upper bound on dynamic regret, and validate their approach through experiments demonstrating higher rewards in various environments compared to continuous policy updates. The findings suggest that efficiently managing policy update frequencies can enhance reinforcement learning applications in dynamic real-world scenarios.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qY622O6Ehg&name=pdf" class="link-primary">https://openreview.net/attachment?id=qY622O6Ehg&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: A Safe Harbor for AI Evaluation and Red Teaming</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">AI safety</span>
<span class="badge bg-primary">evaluation</span>
<span class="badge bg-primary">red teaming</span>
<span class="badge bg-primary">legal protections</span>
<span class="badge bg-primary">independent research</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes that generative AI companies should establish legal and technical safe harbors to protect independent safety research and evaluation, mitigating risks of legal repercussions and account suspensions for researchers.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper argues for the establishment of legal and technical safe harbors by generative AI companies to promote independent evaluation and red teaming of AI systems. It outlines the challenges faced by researchers, including legal risks from terms of service and enforcement practices that deter vital safety evaluations, leading to a chilling effect on research. The authors propose two primary commitments: a legal safe harbor that protects good faith research from liability and a technical safe harbor that safeguards researchers from account suspensions during their work. These proposals aim to enhance transparency, trustworthiness, and broad participation in AI safety research, ultimately contributing to better oversight of the risks associated with generative AI systems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dLojMSgSFW&name=pdf" class="link-primary">https://openreview.net/attachment?id=dLojMSgSFW&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">autonomous weapons</span>
<span class="badge bg-primary">geopolitical instability</span>
<span class="badge bg-primary">AI research</span>
<span class="badge bg-primary">military ethics</span>
<span class="badge bg-primary">regulation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper argues that the deployment of AI-powered Autonomous Weapons Systems (AWS) poses significant risks to geopolitical stability and threatens the integrity of AI research, calling for transparency and regulatory measures.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper examines the implications of AI-powered Autonomous Weapons Systems (AWS) on global stability and the field of AI research. The authors highlight the growing reliance on AWS by militaries worldwide, which may lead to increased frequency of conflicts due to reduced political costs associated with warfare. They note that AWS can operate with minimal human oversight, risking escalation in conflicts and complicating accountability for war crimes. The article emphasizes the need for regulatory measures to manage the burgeoning development of AWS, advocating for transparency in their deployment, international consensus on autonomy levels, and careful oversight of military-civilian research overlap. Ultimately, the authors call for a proactive approach to mitigate the negative effects AWS could have on international relations and the future of AI research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ZwUThOE7Zc&name=pdf" class="link-primary">https://openreview.net/attachment?id=ZwUThOE7Zc&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Automatic Environment Shaping is the Next Frontier in RL</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">environment shaping</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">robotics</span>
<span class="badge bg-primary">automation</span>
<span class="badge bg-primary">data collection</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper argues that the limited progress in robotic reinforcement learning is largely due to the manual nature of environment shaping and advocates for automating this process to enhance scalability and generalization.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper by Younghyo Park et al. discusses the challenge of scaling reinforcement learning (RL) in robotics due to the heavy reliance on manual environment shaping, which includes designing observations, actions, rewards, and simulation dynamics. The authors propose a framework that distinguishes modeling from shaping design choices and call for increased research on automating environment shaping to facilitate the learning of diverse robotic tasks. They identify existing bottlenecks, exemplified in various common tasks, and suggest that current benchmarks are artificially simplified through environment shaping, limiting the true assessment of RL algorithms. The paper emphasizes the need for an improved understanding of shaping strategies and presents potential directions for future research, including computational scaling and dynamic shaping within RL training loops.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dslUyy1rN4&name=pdf" class="link-primary">https://openreview.net/attachment?id=dslUyy1rN4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">agency</span>
<span class="badge bg-primary">ethics</span>
<span class="badge bg-primary">AI</span>
<span class="badge bg-primary">accountability</span>
<span class="badge bg-primary">political processes</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper critiques traditional anthropomorphic analysis of AI by contrasting mechanistic and volitional views of agency, arguing that AI should be understood as the outcome of political processes rather than as ethical agents.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores two competing philosophical frameworks regarding the concept of agency as it relates to artificial intelligence (AI): mechanistic agency, which implies AI can be an ethical simulator of humans, and volitional agency, which asserts that AI lacks the intrinsic desires necessary for moral agency. The authors argue that the mechanistic view, commonly assumed in current AI research, leads to confusion about accountability, as it hinges on the idea that AI can represent and enact ethical behavior akin to humans. Conversely, the volitional view disqualifies AI from being seen as an ethical agent, emphasizing instead the role of human motivations and political processes in shaping AI behavior. Ultimately, the paper advocates for a shift in analysis toward understanding AI within the context of political frameworks and accountability processes rather than attempting to ascribe ethical agency to AI systems themselves.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4XlGXIh2BB&name=pdf" class="link-primary">https://openreview.net/attachment?id=4XlGXIh2BB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Transfer Learning</span>
<span class="badge bg-primary">Public Pretraining</span>
<span class="badge bg-primary">Privacy Risks</span>
<span class="badge bg-primary">Machine Learning Benchmarks</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper critiques the effectiveness and validity of using large-scale public pretraining to enhance differentially private learning, highlighting the privacy risks of web-scraped data and the inadequacy of current benchmarks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper critically examines the emerging paradigm of combining large-scale public pretraining with differentially private learning, questioning the assumptions that public data is inherently non-sensitive and that current benchmarks effectively measure private learning performance. It identifies concerns about the potential memorization of sensitive information from pretraining datasets, the overlap between public and private data distributions in existing benchmarks, and the privacy implications of relying on large models that often necessitate data outsourcing. The authors argue that these issues dilute the claims of privacy preservation in such models and call for clearer privacy considerations, better benchmarks, and approaches to creating privacy-friendly pretrained models in future research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ncjhi4qAPV&name=pdf" class="link-primary">https://openreview.net/attachment?id=ncjhi4qAPV&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Do pretrained Transformers Learn In-Context by Gradient Descent?</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">In-Context Learning</span>
<span class="badge bg-primary">Gradient Descent</span>
<span class="badge bg-primary">Transformer Models</span>
<span class="badge bg-primary">Language Models</span>
<span class="badge bg-primary">Equivalence Hypothesis</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates the claim that In-Context Learning (ICL) in pretrained Transformers is equivalent to Gradient Descent (GD), finding significant differences in their operational dynamics.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper aims to explore whether the emergent In-Context Learning (ICL) behaviors of large pretrained language models can be explained by Gradient Descent (GD), as proposed in recent theoretical works. The authors emphasize the limitations of prior research which simplifies the equivalence between ICL and GD, particularly by using hand-constructed weights and training models explicitly for ICL, unlike the natural emergence observed in typical language model training. Through empirical analysis of models like LLaMa-7B across several datasets, the authors demonstrate that ICL shows a distinct sensitivity to the order of presented demonstrations, contrasting with GD's order-stability. Their findings suggest that the equivalence between ICL and GD is not valid, prompting the need for further nuanced studies to understand the true nature of ICL in pretrained models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=WsawczEqO6&name=pdf" class="link-primary">https://openreview.net/attachment?id=WsawczEqO6&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Embracing Negative Results in Machine Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">negative results</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">publication bias</span>
<span class="badge bg-primary">scientific progress</span>
<span class="badge bg-primary">empirical research</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper advocates for the normalization of publishing negative results in machine learning research to improve scientific integrity, efficiency, and relevance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents a position arguing that the machine learning research community should embrace the publication of negative results, which are often overlooked in favor of publications showcasing improved predictive performance. It discusses the inefficiencies and biases that arise from focusing primarily on positive results, emphasizing the need for a shift in publication norms. The authors highlight the benefits of publishing negative results, such as promoting scientific rigor, fostering innovation, and alleviating the reproducibility crisis. They provide concrete recommendations for how the community can move towards this new paradigm, including creating special issues, encouraging discussions of failures in research, and adapting the review process to accommodate negative findings.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3RXAiU7sss&name=pdf" class="link-primary">https://openreview.net/attachment?id=3RXAiU7sss&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Measure Dataset Diversity, Don't Just Claim It</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">dataset diversity</span>
<span class="badge bg-primary">measurement theory</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">bias</span>
<span class="badge bg-primary">operationalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper calls for clearer definitions and rigorous validation methods for measuring dataset diversity in machine learning, employing principles from measurement theory.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper addresses the ambiguity surrounding the concepts of diversity, bias, and quality in machine learning datasets, arguing that datasets are infused with social and political ideologies rather than being neutral. The authors analyze 135 image and text datasets, applying principles from measurement theory to develop frameworks for defining, operationalizing, and evaluating dataset diversity. They identify inconsistencies in how diversity is described and emphasize the importance of precise definitions and robust validation methods for dataset construction. The findings advocate for enhanced transparency in dataset curation, which has broader implications for improving the reproducibility and ethical considerations in machine learning research.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jsKr6RVDDs&name=pdf" class="link-primary">https://openreview.net/attachment?id=jsKr6RVDDs&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Generative AI</span>
<span class="badge bg-primary">Open Source</span>
<span class="badge bg-primary">Risks</span>
<span class="badge bg-primary">Opportunities</span>
<span class="badge bg-primary">Regulation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper advocates for the responsible open-sourcing of generative AI models, highlighting the near to mid-term risks and opportunities associated with openness compared to closed-source models.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper analyzes the near to mid-term risks and opportunities of open-source generative AI models, proposing a framework for responsible development and deployment. It introduces an AI openness taxonomy to assess the openness of 40 existing large language models (LLMs), discussing the varying benefits and risks of open versus closed-source approaches. The authors argue that open-source generative AI can enhance flexibility, transparency, and community engagement, thereby improving trust and innovation. However, the paper also acknowledges the potential for misuse and safety risks associated with open-source models and offers recommendations for mitigating these risks while maximizing their societal benefits. Overall, the findings contribute to the ongoing discourse on AI safety and regulation.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8q4EPdjTLE&name=pdf" class="link-primary">https://openreview.net/attachment?id=8q4EPdjTLE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: On the Societal Impact of Open Foundation Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">open foundation models</span>
<span class="badge bg-primary">societal impact</span>
<span class="badge bg-primary">risks and benefits</span>
<span class="badge bg-primary">governance</span>
<span class="badge bg-primary">risk assessment</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper analyzes the societal impact of open foundation models, outlining distinctive properties, associated benefits and risks, and proposing a framework for risk assessment to guide future research and policy.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper focuses on the societal impact of open foundation models, which are defined as models with widely available weights. The authors identify five unique properties of these models, including greater customizability and challenges in monitoring, which lead to significant benefits such as increased innovation, competition, and transparency, along with various risks related to misuse, such as disinformation and biosecurity threats. To evaluate these risks, the paper introduces a risk assessment framework that identifies marginal risks compared to existing technologies and emphasizes the need for empirical research to substantiate claims about their impacts. Ultimately, the authors urge AI developers, policymakers, and researchers to collaborate in establishing responsible practices and addressing the uncertainties surrounding the deployment of open foundation models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jRX6yCxFhx&name=pdf" class="link-primary">https://openreview.net/attachment?id=jRX6yCxFhx&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Open-Endedness is Essential for Artificial Superhuman Intelligence</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">open-endedness</span>
<span class="badge bg-primary">artificial superhuman intelligence (ASI)</span>
<span class="badge bg-primary">foundation models</span>
<span class="badge bg-primary">novelty</span>
<span class="badge bg-primary">learnability</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper proposes a formal definition of open-endedness as a key property for achieving artificial superhuman intelligence, emphasizing the potential of combining open-ended algorithms with foundation models to enable continuous novelty and learning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper argues that open-endedness is essential for the development of artificial superhuman intelligence (ASI) and posits that the necessary components to achieve it are becoming available through foundation models. The authors provide a formal definition of open-endedness based on the concepts of novelty and learnability, suggesting that ASI systems should be capable of producing artifacts that are both unpredictable and with meaning to human observers. They illustrate existing open-ended systems and highlight their limitations while proposing various research directions that integrate open-endedness with foundation models to enhance safety and overall system capabilities. The work stresses that addressing safety concerns must occur alongside advancing open-ended AI research to ensure beneficial outcomes for society.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Bc4vZ2CX7E&name=pdf" class="link-primary">https://openreview.net/attachment?id=Bc4vZ2CX7E&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">magnetic fusion energy</span>
<span class="badge bg-primary">disruption prediction</span>
<span class="badge bg-primary">simulation modeling</span>
<span class="badge bg-primary">materials discovery</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper identifies six key research challenges in magnetic fusion energy where machine learning can be applied to enhance efficiency, control, and materials discovery, advocating for collaboration between the ML community and fusion researchers.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper outlines the pressing need for machine learning (ML) applications in magnetic fusion energy, particularly in the context of tokamaks. It highlights six priority research areas—including disruption prediction, simulation modeling, and materials discovery—where ML can significantly improve understanding and operational efficiency. The authors review past ML efforts, propose potential future model features, and outline the challenges specific to each problem area. The discussion emphasizes the importance of updating the fusion data ecosystem to facilitate ML innovations and suggests that collaboration between ML practitioners and the fusion community is crucial for advancing clean energy solutions.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=arwP5FA2dO&name=pdf" class="link-primary">https://openreview.net/attachment?id=arwP5FA2dO&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Traveling Salesman Problem (TSP)</span>
<span class="badge bg-primary">Monte Carlo Tree Search (MCTS)</span>
<span class="badge bg-primary">Machine Learning (ML)</span>
<span class="badge bg-primary">Heatmap Generation</span>
<span class="badge bg-primary">Optimization Techniques</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper critiques machine learning approaches that generate heatmaps for guiding Monte Carlo tree search in solving large-scale Traveling Salesman Problems, introducing a simple baseline method, SoftDist, that outperforms complex ML methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper critically evaluates recent machine learning methods that utilize heatmap-guided Monte Carlo tree search (MCTS) for addressing large-scale Traveling Salesman Problems (TSP). It introduces SoftDist, a simple heatmap generation method based on softmax applied to the distance matrix, which not only achieves better performance than existing complex ML approaches but also aligns training objectives directly with test phases. The authors empirically demonstrate the superiority of SoftDist in both solution quality and inference speed, while also proposing a new Score metric to compare MCTS with the LKH-3 heuristic. Their findings highlight significant limitations in the current ML paradigm, suggesting a need for more theoretically robust strategies and better alignment between training and application phases in solving combinatorial optimization problems.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cEJ9jNJuJP&name=pdf" class="link-primary">https://openreview.net/attachment?id=cEJ9jNJuJP&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: Technical Research and Talent is Needed for Effective AI Governance</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">AI governance</span>
<span class="badge bg-primary">technical research</span>
<span class="badge bg-primary">regulatory frameworks</span>
<span class="badge bg-primary">talent gap</span>
<span class="badge bg-primary">machine learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper argues for increased collaboration between AI researchers and policymakers to bridge the gap between regulatory aspirations and the current state of AI technical capabilities, highlighting the urgent need for targeted research and technical expertise in governance institutions.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This position paper addresses the growing need for effective AI governance amid the rapid integration of AI systems into various sectors and highlights significant discrepancies between proposed regulations in the EU, US, and China and the current technical capabilities of AI. The authors analyze policy documents to identify areas where existing technical tools and methodologies fall short of meeting regulatory requirements. They emphasize the necessity for targeted AI/ML research to develop adequate governance tools and call for enhancing technical expertise within governing institutions to inform, operationalize, and enforce AI regulations effectively. The paper outlines actionable steps for integrating technical researchers into policy-making processes, aiming to ensure governance measures are informed and effective, culminating in a call for action to the AI/ML research community.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Be2B6f0ps1&name=pdf" class="link-primary">https://openreview.net/attachment?id=Be2B6f0ps1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Position: The Platonic Representation Hypothesis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">convergence</span>
<span class="badge bg-primary">Platonic representation</span>
<span class="badge bg-primary">neural networks</span>
<span class="badge bg-primary">multi-task learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper posits that AI models, particularly neural networks, are converging towards a shared statistical representation of reality, termed the Platonic representation, as they scale in size and task diversity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study explores the phenomenon of representational convergence in AI models, particularly neural networks, arguing that they are increasingly aligning in how they represent data across different modalities and architectures. The authors introduce the Platonic Representation Hypothesis, suggesting that as models are trained on larger datasets and a broader array of tasks, they move toward a unified representation of the underlying reality, akin to the philosophical concept of a perfect form. They present empirical evidence supporting this convergence through metrics of representational alignment, demonstrating that different models—regardless of their architecture—exhibit similar representational structures as their performance improves. The implications of this trend are discussed, including the advantages of training across multiple modalities and the potential to reduce biases and hallucinations in larger models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=BH8TYy0r6u&name=pdf" class="link-primary">https://openreview.net/attachment?id=BH8TYy0r6u&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Federated Learning</span>
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Synthetic Data</span>
<span class="badge bg-primary">Language Models</span>
<span class="badge bg-primary">On-device Training</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces PrE-Text, a method for generating differentially private synthetic textual data that outperforms on-device training while being more efficient in terms of communication and computation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents PrE-Text, a novel technique for training language models using private federated data by generating high-quality differentially private (DP) synthetic text data. The method consists of two main phases: an iterative process of generating synthetic data from private client samples, followed by a post-processing step that leverages large language models (LLMs) to expand this data efficiently. The experiments demonstrate that models trained on synthetic data produced by PrE-Text exceed the performance of models trained directly on-device, achieving significant reductions in communication rounds and client computation costs. This approach indicates that DP synthetic data can serve as a superior alternative to traditional on-device training methods, thereby facilitating better privacy-compliant learning strategies for language models amidst the growing demand for efficient data utilization.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=3WCvnkHnxV&name=pdf" class="link-primary">https://openreview.net/attachment?id=3WCvnkHnxV&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">molecule synthesis</span>
<span class="badge bg-primary">conditional residual energy-based models</span>
<span class="badge bg-primary">retrosynthetic planning</span>
<span class="badge bg-primary">preference optimization</span>
<span class="badge bg-primary">machine learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a framework employing conditional residual energy-based models to enhance the quality and feasibility of synthetic routes in molecule synthesis by considering specific criteria such as cost, yield, and step count.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study addresses the challenges in molecule synthesis for drug discovery, focusing on enhancing retrosynthetic planning through machine learning methods. The authors propose a novel framework utilizing conditional residual energy-based models (CREBMs) to improve the quality of synthetic routes generated by existing strategies. By incorporating an energy function that evaluates synthetic routes based on various criteria, the framework is designed to optimize the selection of routes while maintaining compatibility with current methodologies. Experimental results demonstrate that the proposed approach significantly boosts the performance of model predictions, achieving a 2.5% improvement in top-1 accuracy over previous state-of-the-art methods. The findings highlight the potential of integrating controllable generation techniques in retrosynthetic planning to develop more efficient and feasible molecule synthesis strategies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=oLfq1KKneW&name=pdf" class="link-primary">https://openreview.net/attachment?id=oLfq1KKneW&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Principled Preferential Bayesian Optimization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Bayesian Optimization</span>
<span class="badge bg-primary">Preference Feedback</span>
<span class="badge bg-primary">Cumulative Regret</span>
<span class="badge bg-primary">Gaussian Processes</span>
<span class="badge bg-primary">Optimization Algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a novel algorithm for preferential Bayesian optimization that achieves information-theoretic regret bounds and efficiently resolves cumulative regret through preference feedback without requiring direct scalar evaluations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the problem of preferential Bayesian optimization (BO), where only preference feedback between pairs of candidate solutions is available for optimizing a black-box function. It introduces the Principled Optimistic Preferential Bayesian Optimization (POP-BO) algorithm, which leverages a likelihood ratio-based confidence set and implements a systematic approach for reporting estimated optimal solutions. The authors establish information-theoretic bounds on cumulative regret for the POP-BO algorithm, marking a significant advancement in the theoretical framework of preferential BO. Experimental evaluations demonstrate that POP-BO outperforms existing heuristic methods across various settings, including Gaussian processes, standard test functions, and a thermal comfort optimization application, achieving competitive performance with considerable computational efficiency.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=YqMOM5W9GF&name=pdf" class="link-primary">https://openreview.net/attachment?id=YqMOM5W9GF&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">sequence compression</span>
<span class="badge bg-primary">action abstractions</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">behavior cloning</span>
<span class="badge bg-primary">Byte Pair Encoding</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents PRISE, a method that leverages Byte Pair Encoding for learning temporal action abstractions in continuous control, significantly enhancing performance in multitask robotic manipulation and few-shot learning scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces PRISE, a framework that treats the task of learning temporal action abstractions as a sequence compression problem, inspired by techniques from natural language processing, specifically Byte Pair Encoding (BPE). The authors propose a two-stage training process where continuous actions are first quantized into discrete codes, followed by the application of BPE to identify temporal action primitives. These abstractions are shown to enhance the efficiency of downstream behavior cloning in robotic manipulation tasks, demonstrating significant performance improvements over various baselines such as ACT. Extensive experimentation on multitask datasets, including MetaWorld and LIBERO, validates the effectiveness of PRISE in learning generalist policies and adapting to unseen tasks efficiently, highlighting its potential for advancing reinforcement learning applications in robotics.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=p225Od0aYt&name=pdf" class="link-primary">https://openreview.net/attachment?id=p225Od0aYt&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Privacy Preserving Adaptive Experiment Design</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">adaptive experiment design</span>
<span class="badge bg-primary">conditional average treatment effect</span>
<span class="badge bg-primary">differential privacy</span>
<span class="badge bg-primary">regret minimization</span>
<span class="badge bg-primary">contextual bandits</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel method for conducting adaptive experiments in a privacy-preserving manner, achieving a balance between estimation accuracy and social welfare loss.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenges of adaptive experiment design, particularly in clinical trials, where the estimation of conditional average treatment effect (CATE) must be balanced against the regret in a contextual bandit framework. The authors investigate the trade-off between social welfare loss and statistical power in a privacy-preserving context, proposing a matched upper and lower bound optimization problem under the constraints of differential privacy. The developed algorithms demonstrate that robust privacy protection can nearly negate the cost of privacy, allowing for optimal estimation accuracy alongside minimized regret. Additionally, the authors derive asymptotic normality for the estimators, which is crucial for statistical inference. The findings suggest that privacy-preserving mechanisms can efficiently conduct adaptive experiments without significant loss in estimation quality or social welfare.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=1QmFKwVwwI&name=pdf" class="link-primary">https://openreview.net/attachment?id=1QmFKwVwwI&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Private Truly-Everlasting Robust-Prediction</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Private Everlasting Prediction</span>
<span class="badge bg-primary">Robustness</span>
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Adversarial Learning</span>
<span class="badge bg-primary">Sample Complexity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Private Everlasting Robust Prediction (PERP), enhancing the concept of Private Everlasting Prediction (PEP) to include robustness against adversarial queries while optimizing sample complexity and runtime.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents Private Everlasting Robust Prediction (PERP), a robust extension of the Private Everlasting Prediction (PEP) framework that enables learners to handle adversarial queries without compromising performance on legitimate predictions. The authors refine the privacy and utility definitions to accommodate robustness against out-of-distribution attacks and introduce a relaxed privacy parameter decoupled from the time horizon. They provide efficient constructions for specific classes, such as axis-aligned rectangles and decision-stumps, demonstrating that PERP achieves significantly improved sample complexity and resilience to errors in adversarial settings. The results show that PERP could serve as a viable alternative to classical private learning approaches, especially in scenarios where outliers or adversarial inputs may corrupt the learning process.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=BdQTCAuT6L&name=pdf" class="link-primary">https://openreview.net/attachment?id=BdQTCAuT6L&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Probabilistic Generating Circuits - Demystified</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">probabilistic generating circuits</span>
<span class="badge bg-primary">probabilistic circuits</span>
<span class="badge bg-primary">tractable inference</span>
<span class="badge bg-primary">negative weights</span>
<span class="badge bg-primary">marginalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that probabilistic generating circuits (PGCs) can be effectively simulated by nonmonotone probabilistic circuits (PCs) with negative weights, revealing that the power of PGCs derives from allowing negative weights rather than their polynomial representation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the properties and capabilities of probabilistic generating circuits (PGCs) introduced by Zhang et al. and shows that despite their unique representation using probability generating polynomials, they do not outperform nonmonotone probabilistic circuits (PCs) with negative weights. The authors establish that any binary PGC can be transformed into a nonmonotone PC without significant overhead, highlighting the importance of negative weights in determining their computational power. Furthermore, they prove that PGCs for categorical variables with more than two values do not support efficient marginalization unless NP=P, while showing that nonmonotone PCs can effectively handle marginalization over categorical variables of arbitrary size. Overall, the findings clarify the relationship between PGCs and PCs, contributing to a deeper understanding of tractable probabilistic modeling.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=EqFxIbGWRU&name=pdf" class="link-primary">https://openreview.net/attachment?id=EqFxIbGWRU&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Sequential Monte Carlo</span>
<span class="badge bg-primary">Language Models</span>
<span class="badge bg-primary">Probabilistic Inference</span>
<span class="badge bg-primary">Twist Functions</span>
<span class="badge bg-primary">Reinforcement Learning from Human Feedback</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel framework called Twisted Sequential Monte Carlo (TSMC) for probabilistic inference in language models, allowing for efficient sampling from unnormalized target distributions by learning twist functions to guide the sampling process.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper explores the use of Twisted Sequential Monte Carlo (TSMC) to enhance various techniques for improving the capabilities and safety of large language models (LLMs). The authors position traditional tasks, such as generating controlled outputs and evaluating language model inference, within a probabilistic inference framework where the goal is to sample from unnormalized target distributions defined by potential functions. They propose a method for learning intermediate twist functions, which summarize future potential at each timestep, enabling more efficient sampling of promising partial sequences. Empirical results demonstrate that TSMC effectively improves sampling of undesirable outputs, facilitates sentiment-controlled generation, and performs well in infilling tasks, indicating its potential for applications in automated red-teaming and safe LLM deployment.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=frA0NNBS1n&name=pdf" class="link-primary">https://openreview.net/attachment?id=frA0NNBS1n&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">multi-task learning</span>
<span class="badge bg-primary">representation learning</span>
<span class="badge bg-primary">neural networks</span>
<span class="badge bg-primary">gradient descent</span>
<span class="badge bg-primary">feature extraction</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that multi-task pretraining of two-layer ReLU neural networks leads to effective feature learning by inducing a pseudo-contrastive loss that aligns representations of similar inputs across tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the effectiveness of multi-task pretraining on two-layer ReLU neural networks and provides the first theoretical results demonstrating that such training induces meaningful feature learning. By analyzing the training dynamics, the authors show that the multi-task setup results in a pseudo-contrastive loss that encourages the representations of inputs sharing similar labels to align in a learned feature space. The study proves that when tasks depend on a low-dimensional subspace of the input data, gradient-based updates during multi-task learning effectively identify and capture the ground-truth features, enabling superior performance on downstream tasks without reliance on the ambient dimension. Results and numerical simulations confirm the theoretical claims, underscoring the benefits of multi-task learning in scenarios where the task diversity impacts feature extraction significantly.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=M8UbECx485&name=pdf" class="link-primary">https://openreview.net/attachment?id=M8UbECx485&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Correlation Clustering</span>
<span class="badge bg-primary">Dynamic Algorithms</span>
<span class="badge bg-primary">Local Computation</span>
<span class="badge bg-primary">Parallel Computation</span>
<span class="badge bg-primary">Approximation Algorithms</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces PRUNED PIVOT, an efficient algorithm for correlation clustering that achieves improved approximation and runtime complexities in dynamic, parallel, and local computation settings.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the correlation clustering problem, where the aim is to cluster nodes in a graph with both positive and negative edge labels to minimize certain edge costs. The authors present a novel algorithm, PRUNED PIVOT, which optimizes previous methods by achieving an expected amortized constant update time in fully dynamic scenarios and providing a high-quality clustering approximation of 3 + O(1/k). The algorithm shows versatility, being effectively implementable across various computational models, including fully dynamic and local computation settings, while significantly reducing the number of nodes queried. Empirical evaluations on synthetic graphs indicate that PRUNED PIVOT closely matches the performance of existing algorithms, establishing it as a practical solution for large-scale and dynamic data contexts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=saP7s0ZgYE&name=pdf" class="link-primary">https://openreview.net/attachment?id=saP7s0ZgYE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Rate-Optimal Policy Optimization for Linear Markov Decision Processes</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Policy Optimization</span>
<span class="badge bg-primary">Regret Minimization</span>
<span class="badge bg-primary">Linear MDPs</span>
<span class="badge bg-primary">Bandit Feedback</span>
<span class="badge bg-primary">Adversarial Settings</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a rate-optimal policy optimization algorithm for linear Markov Decision Processes that achieves logarithmic regret in both adversarial and stochastic settings with minimal computational complexity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The authors investigate regret minimization in online episodic linear Markov Decision Processes (MDPs) and propose a computationally efficient policy optimization algorithm that achieves logarithmic regret. They demonstrate that an optimistic variant of the natural policy gradient, when combined with a reward-free warmup and a structured update schedule, leads to optimal regret bounds for both adversarial losses with full information feedback and stochastic losses with bandit feedback. This work represents a significant advancement in establishing optimal regret rates in the context of linear MDPs, contributing to the theoretical foundation of reinforcement learning and expanding practical applications across various fields such as robotics and large language models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=VJwsDwuiuH&name=pdf" class="link-primary">https://openreview.net/attachment?id=VJwsDwuiuH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Rejuvenating image-GPT as Strong Visual Representation Learners</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">autoregressive pretraining</span>
<span class="badge bg-primary">visual representation</span>
<span class="badge bg-primary">D-iGPT</span>
<span class="badge bg-primary">semantic tokens</span>
<span class="badge bg-primary">ImageNet-1K</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces D-iGPT, an enhanced version of image-GPT that achieves 90.0% top-1 accuracy on ImageNet-1K by shifting the model's focus from raw pixels to semantic tokens and incorporating supervision on visible tokens.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents D-iGPT, an improvement upon the image-GPT framework aimed at enhancing visual representation learning through autoregressive pretraining. The authors implement two key modifications: transitioning the prediction target from raw pixel values to semantic tokens, which are derived from discriminatively trained models like CLIP, and supplementing the autoregressive modeling with additional supervision on visible tokens. Extensive experiments demonstrate that D-iGPT achieves a substantial 90.0% top-1 accuracy on the ImageNet-1K dataset, significantly surpassing previous state-of-the-art methods while exclusively utilizing publicly available datasets. This work suggests a reevaluation of the potential of autoregressive pretraining in visual tasks and highlights its efficacy as a scalable approach for developing foundation models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mzGtunvpJH&name=pdf" class="link-primary">https://openreview.net/attachment?id=mzGtunvpJH&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Repoformer: Selective Retrieval for Repository-Level Code Completion</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">selective retrieval</span>
<span class="badge bg-primary">code completion</span>
<span class="badge bg-primary">retrieval-augmented generation</span>
<span class="badge bg-primary">self-evaluation</span>
<span class="badge bg-primary">repository-level</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents REPOFORMER, a selective retrieval framework that improves repository-level code completion efficiency and accuracy by allowing code language models to self-assess the necessity of retrieval, resulting in significant performance gains and faster inference.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the limitations of existing retrieval-augmented generation (RAG) methods in repository-level code completion, where many retrieved contexts can be irrelevant or harmful. The authors propose a novel framework, REPOFORMER, that enables code language models (code LMs) to self-evaluate the relevance of cross-file contexts before retrieval, thereby deciding when retrieval is beneficial. This self-supervised learning approach is demonstrated to enhance code completion performance across multiple benchmarks, including new long-form tasks, and achieves up to 70% speedup in inference without sacrificing accuracy. Extensive evaluations show that REPOFORMER generalizes well across different models, languages, and retrieval methods, indicating its potential for more efficient and accurate code completion in practical programming environments.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=moyG54Okrj&name=pdf" class="link-primary">https://openreview.net/attachment?id=moyG54Okrj&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Data Shapley</span>
<span class="badge bg-primary">data selection</span>
<span class="badge bg-primary">utility functions</span>
<span class="badge bg-primary">machine learning</span>
<span class="badge bg-primary">data valuation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper critiques Data Shapley for data selection tasks, showing that its effectiveness can be no better than random selection without specific constraints on utility functions, while identifying scenarios where it excels.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study investigates the inconsistencies in Data Shapley's performance for data selection tasks in machine learning, revealing that its effectiveness can be equivalent to random selection in the absence of specific structural assumptions about utility functions. The authors introduce a hypothesis testing framework to analyze the efficacy of comparing dataset utilities based on Shapley scores and identify a class of utility functions where Data Shapley performs optimally. Furthermore, they propose a heuristic for predicting Data Shapley's effectiveness in data selection by approximating utility functions with monotonically transformed modular functions. Experimental results support their theoretical findings, elucidating when Data Shapley is a beneficial tool for data selection and contributing valuable insights to data-centric machine learning practices.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=mKYBMf1hHG&name=pdf" class="link-primary">https://openreview.net/attachment?id=mKYBMf1hHG&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">robustness</span>
<span class="badge bg-primary">adversarial training</span>
<span class="badge bg-primary">multi-modal models</span>
<span class="badge bg-primary">vision-language models</span>
<span class="badge bg-primary">unsupervised fine-tuning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents FARE, an unsupervised adversarial fine-tuning method that enhances the robustness of the CLIP vision encoder used in large vision-language models without requiring retraining of downstream tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the vulnerability of large vision-language models (LVLMs) to adversarial attacks on the vision modality, which can lead to misinformation or user defraudation. It introduces FARE (Fine-tuning for Adversarially Robust Embeddings), an unsupervised adversarial fine-tuning scheme designed to make the CLIP vision encoder more robust while preserving its performance on clean data. Experimental results demonstrate that FARE not only outperforms previous supervised fine-tuning methods like TeCoA in both clean and adversarial settings across various downstream vision-language tasks, but it also maintains integration compatibility with LVLMs without additional fine-tuning required. The implications of this research extend to enhancing the safe deployment of LVLMs, making them less susceptible to stealthy attacks aimed at spreading false information.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=WLPhywf1si&name=pdf" class="link-primary">https://openreview.net/attachment?id=WLPhywf1si&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Robustness of Nonlinear Representation Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Nonlinear Representation Learning</span>
<span class="badge bg-primary">Independent Component Analysis (ICA)</span>
<span class="badge bg-primary">Robustness</span>
<span class="badge bg-primary">Misspecification</span>
<span class="badge bg-primary">Causal Representation Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper establishes robustness results for nonlinear representation learning by showing that approximate identifiability can be achieved under slight perturbations in the mixing function.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the robustness of nonlinear representation learning in slightly misspecified contexts, specifically focusing on independent component analysis (ICA) with perturbations. By formalizing conditions under which mixing functions can be approximated as local isometries, the authors demonstrate that the latent variables remain identifiable up to linear transformations despite small inaccuracies. Key contributions include theoretical results on approximate identifiability for nonlinear ICA, extending previous findings under relaxed assumptions. The work highlights the significance of these findings for real-world data representation, pointing towards improved performance and better interpretability in machine learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=GyV33H5Uuk&name=pdf" class="link-primary">https://openreview.net/attachment?id=GyV33H5Uuk&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">S  I: Score-based O-INFORMATION Estimation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">O-INFORMATION</span>
<span class="badge bg-primary">multivariate systems</span>
<span class="badge bg-primary">mutual information</span>
<span class="badge bg-primary">information theory</span>
<span class="badge bg-primary">neural estimation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces SI, a novel method for estimating O-INFORMATION in complex multivariate systems without relying on restrictive distributional assumptions, demonstrating its effectiveness through synthetic and real-world experiments.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the limitations of classical mutual information measures in analyzing multivariate systems by proposing a new methodology, SI, that allows for flexible and scalable estimation of O-INFORMATION. By leveraging recent advancements in mutual information estimation techniques and score functions, SI overcomes the need for specific distributional assumptions, making it applicable to a wide range of complex systems. The authors validate their approach through experiments on synthetic data and a case study on brain activity in mice, showcasing SI's robustness and accuracy in estimating information dynamics that capture both redundancy and synergy among variables. The findings indicate that SI could significantly enhance the analysis of complex systems in various scientific domains.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LuhWZ2oJ5L&name=pdf" class="link-primary">https://openreview.net/attachment?id=LuhWZ2oJ5L&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Semi-Supervised Learning</span>
<span class="badge bg-primary">Referring Expression Segmentation</span>
<span class="badge bg-primary">Pseudo-Label Refinement</span>
<span class="badge bg-primary">Segment Anything Model</span>
<span class="badge bg-primary">Performance Evaluation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces SemiRES, a novel semi-supervised framework that utilizes the Segment Anything Model (SAM) for effective pseudo-label refinement in referring expression segmentation (RES), achieving significant performance improvements with minimal labeled data.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents SemiRES, a semi-supervised learning framework designed for referring expression segmentation (RES), which addresses the challenge of noisy pseudo-labels commonly encountered in traditional methods. By leveraging the high-quality boundary demarcation capabilities of the Segment Anything Model (SAM), SemiRES refines pseudo-labels through two innovative strategies: IoU-based Optimal Matching (IOM) and Composite Parts Integration (CPI). The framework is evaluated across three benchmark datasets—RefCOCO, RefCOCO+, and G-Ref—demonstrating state-of-the-art performance, particularly beneficial when only 1% of the data is labeled, and showcasing an improvement of +18.64% on the RefCOCO validation set compared to fully supervised methods. The findings highlight the framework's potential for reducing annotation costs while maintaining robust segmentation performance in real-world applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=M5kn9NKIs4&name=pdf" class="link-primary">https://openreview.net/attachment?id=M5kn9NKIs4&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">Time Series Forecasting</span>
<span class="badge bg-primary">Sharpness-Aware Minimization</span>
<span class="badge bg-primary">Channel-Wise Attention</span>
<span class="badge bg-primary">Generalization Capacity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces SAMformer, a shallow transformer model optimized with sharpness-aware minimization to address the generalization issues faced by traditional transformer architectures in multivariate long-term time series forecasting.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper investigates the limitations of transformer-based architectures in multivariate long-term time series forecasting, finding that traditional transformers often converge to sharp local minima due to issues with their attention mechanisms. To remedy this, the authors propose a lightweight transformer model called SAMformer, which employs sharpness-aware minimization and channel-wise attention to improve generalization. They demonstrate through experiments on synthetic data and real-world datasets that SAMformer outperforms existing state-of-the-art models, achieving competitive results with significantly fewer parameters. The findings suggest that optimizing transformer architectures for better training stability can enhance their performance in practical forecasting applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=8kLzL5QBh2&name=pdf" class="link-primary">https://openreview.net/attachment?id=8kLzL5QBh2&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SAPG: Split and Aggregate Policy Gradients</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">policy gradients</span>
<span class="badge bg-primary">sample efficiency</span>
<span class="badge bg-primary">importance sampling</span>
<span class="badge bg-primary">parallel environments</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces SAPG, a novel on-policy reinforcement learning algorithm that enhances training efficiency by splitting environments and aggregating data through importance sampling, achieving superior performance compared to traditional methods like PPO.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the limitations of current on-policy reinforcement learning techniques, particularly Proximal Policy Optimization (PPO), in effectively utilizing large batches generated from massively parallel environments. The proposed algorithm, Split and Aggregate Policy Gradients (SAPG), optimizes performance by segmenting environments into multiple blocks that learn diverse policies and fuses their data via importance sampling for on-policy updates. Experimental results demonstrate that SAPG significantly outperforms PPO and other strong baselines across various challenging manipulation tasks, particularly in scenarios where PPO fails to achieve any meaningful progress. The findings suggest that leveraging data diversity and combining multiple policies can lead to higher asymptotic performance in large-scale reinforcement learning settings.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=4dOJAfXhNV&name=pdf" class="link-primary">https://openreview.net/attachment?id=4dOJAfXhNV&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Scalable AI Safety via Doubly-Efficient Debate</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">AI Safety</span>
<span class="badge bg-primary">Debate Protocols</span>
<span class="badge bg-primary">Human Judgement</span>
<span class="badge bg-primary">Large Language Models</span>
<span class="badge bg-primary">Verification</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a model of doubly-efficient debate to enable scalable AI safety by allowing polynomial-time provers to compete while using limited human input for verifying complex computations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of ensuring AI safety in complex tasks by proposing a framework called doubly-efficient debate, which facilitates a competitive dialogue between two polynomial-time AI models to convince a more efficient verifier with limited access to human judgments. It builds on previous work in AI safety via debate and interactive proofs, ensuring that the honest model can succeed using polynomial steps even against a potentially dishonest opponent employing exponential steps. The authors demonstrate that any polynomial-time computation can be verified with a constant number of human queries, making the approach feasible for training and oversight of powerful AI systems like large language models. Key findings include the ability to handle stochastic human judgments and the potential for scalable training with minimal human feedback, opening avenues for more robust AI applications without extensive reliance on human experts.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6jmdOTRMIO&name=pdf" class="link-primary">https://openreview.net/attachment?id=6jmdOTRMIO&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Diffusion models</span>
<span class="badge bg-primary">Rectified flow</span>
<span class="badge bg-primary">High-resolution image synthesis</span>
<span class="badge bg-primary">Transformer architecture</span>
<span class="badge bg-primary">Text-to-image generation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents advancements in rectified flow models for high-resolution text-to-image synthesis, demonstrating improved performance through novel noise sampling techniques and a multimodal transformer architecture.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This research improves the efficiency and effectiveness of rectified flow models in high-resolution image synthesis, particularly for text-to-image generation. The authors enhance existing noise sampling methods, biasing them toward perceptually relevant scales, and introduce a novel transformer-based architecture that facilitates bidirectional information flow between text and image modalities. Through extensive large-scale experiments, the model exhibits superior performance compared to established diffusion formulations and current state-of-the-art models, supported by lower validation loss and improved evaluation metrics. The implications suggest significant potential for refined image generation tasks, with plans for public accessibility of the resulting data, code, and model weights.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=FPnUhsQJ5B&name=pdf" class="link-primary">https://openreview.net/attachment?id=FPnUhsQJ5B&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">text-to-3D synthesis</span>
<span class="badge bg-primary">SceneCraft</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">Blender</span>
<span class="badge bg-primary">iterative refinement</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">SceneCraft is a large language model-based agent that effectively converts text descriptions into Blender-executable Python scripts for generating complex 3D scenes, improving upon traditional methods through a dual-loop self-improvement pipeline.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces SceneCraft, an innovative framework that utilizes a large language model (LLM) to translate textual descriptions into executable Blender scripts, resulting in intricate 3D scene creation. The process begins with generating a scene graph to establish spatial relationships among assets, followed by script generation that translates these relationships into numerical constraints for asset layout. SceneCraft employs feedback loops for iterative refinement of generated scenes, leveraging vision-language models to evaluate and enhance image fidelity. Additionally, it incorporates a library learning mechanism allowing continuous improvement of spatial skills without requiring extensive human input or LLM parameter tuning. The evaluation demonstrated that SceneCraft outperforms existing agents like BlenderGPT in generating coherent, high-fidelity 3D scenes from diverse queries, showcasing its potential in fields such as gaming and architectural design.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=gAyzjHw2ml&name=pdf" class="link-primary">https://openreview.net/attachment?id=gAyzjHw2ml&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Self-Composing Policies for Scalable Continual Reinforcement Learning</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Continual Reinforcement Learning</span>
<span class="badge bg-primary">Neural Networks</span>
<span class="badge bg-primary">Knowledge Transfer</span>
<span class="badge bg-primary">Modular Architecture</span>
<span class="badge bg-primary">Catastrophic Forgetting</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents CompoNet, a modular neural network architecture for scalable continual reinforcement learning that effectively mitigates catastrophic forgetting while enabling knowledge transfer across tasks with a linear growth in parameters.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces CompoNet, a novel growable and modular neural network architecture designed to enhance continual reinforcement learning (CRL) by addressing challenges such as catastrophic forgetting and interference. CompoNet operates by autonomously composing previously learned policy modules, allowing it to maintain linear growth in the number of parameters concerning the number of tasks while efficiently reusing knowledge. Thorough experiments across various benchmark tasks demonstrate that CompoNet outperforms existing CRL approaches, exhibiting superior knowledge transfer and robust performance even when tasked with unrelated challenges. This advancement is significant for developing CRL agents that can adapt and scale in dynamic environments, providing a framework that balances scalability and learning efficiency without sacrificing the ability to acquire new knowledge.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=f5gtX2VWSB&name=pdf" class="link-primary">https://openreview.net/attachment?id=f5gtX2VWSB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Deep Gaussian Processes</span>
<span class="badge bg-primary">Denoising Diffusion Variational Inference</span>
<span class="badge bg-primary">Inducing Points</span>
<span class="badge bg-primary">Variational Inference</span>
<span class="badge bg-primary">Bayesian Deep Learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces Denoising Diffusion Variational Inference (DDVI) as a novel method for posterior inference of inducing points in Deep Gaussian Processes, significantly improving accuracy and computational efficiency compared to traditional variational approaches.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of inferring the posterior distribution of inducing points in Deep Gaussian Processes (DGPs) by proposing a new method called Denoising Diffusion Variational Inference (DDVI). This approach utilizes a denoising diffusion stochastic differential equation (SDE) to generate posterior samples and applies score matching techniques for accurate score function approximation using neural networks. By deriving a new explicit variational lower bound through KL divergence minimization, DDVI effectively mitigates the biases present in conventional variational inference methods. The experimental results across various datasets demonstrate that DDVI outperforms traditional methods like Mean-field Gaussian Variational Inference (DSVI) and Implicit Posterior Variational Inference (IPVI) in both computational efficiency and inference accuracy, paving the way for more robust Bayesian deep learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=jTn4AIOgpM&name=pdf" class="link-primary">https://openreview.net/attachment?id=jTn4AIOgpM&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">SparseTSF: Modeling Long-term Time Series Forecasting with *1k* Parameters</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Long-term Time Series Forecasting</span>
<span class="badge bg-primary">Sparse Forecasting</span>
<span class="badge bg-primary">Lightweight Models</span>
<span class="badge bg-primary">Parameter Efficiency</span>
<span class="badge bg-primary">Temporal Dependencies</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents SparseTSF, a highly efficient model for long-term time series forecasting that utilizes a novel Cross-Period Sparse Forecasting technique to achieve competitive accuracy with fewer than 1,000 parameters.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper introduces SparseTSF, an innovative lightweight model designed for long-term time series forecasting by effectively decoupling periodicity and trend components through a Cross-Period Sparse Forecasting technique. This technique involves downsampling the time series data to focus on trend prediction, allowing the model to maintain a minimal parameter size while still capturing essential temporal dependencies. SparseTSF, with less than 1,000 parameters, demonstrates competitive performance against state-of-the-art models on various benchmarks and showcases robust generalization capabilities, making it suitable for applications with limited computational resources. The findings suggest that SparseTSF is a significant advancement in the realm of efficient time series forecasting methodologies.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=54NSHO0lFe&name=pdf" class="link-primary">https://openreview.net/attachment?id=54NSHO0lFe&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Speech Self-Supervised Learning Using Diffusion Model Synthetic Data</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">self-supervised learning</span>
<span class="badge bg-primary">speech representation</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">synthetic data augmentation</span>
<span class="badge bg-primary">low-resource languages</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces DIFFS4L, a pretraining scheme that enhances self-supervised learning for speech processing by generating synthetic data with varied speech characteristics using a diffusion model, resulting in significant performance improvements, especially in low-resource scenarios.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of self-supervised learning (SSL) in speech processing, particularly the dependency on large unannotated datasets, which is often impractical for low-resource languages. To mitigate this issue, it presents DIFFS4L, a novel pretraining approach that employs diffusion models to generate synthetic speech data with novel variations in prosody, speaker identity, and content. The authors demonstrate through extensive experiments that integrating this synthetic data with real speech significantly improves SSL model performance, achieving improvements in the word error rate (WER) for the HuBERT pretrained model by 6.26 percentage points in English ASR tasks. Additionally, the methodology proves effective across multiple languages and contexts, suggesting that underlying information in pretraining datasets is too often underutilized by existing approaches.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ecnpYYHjt9&name=pdf" class="link-primary">https://openreview.net/attachment?id=ecnpYYHjt9&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Stealing part of a production language model</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">model stealing</span>
<span class="badge bg-primary">language models</span>
<span class="badge bg-primary">embedding extraction</span>
<span class="badge bg-primary">black-box attacks</span>
<span class="badge bg-primary">security</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces a novel model-stealing attack that successfully extracts detailed information from black-box production language models, specifically retrieving their embedding projection layers.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents the first model-stealing attack capable of extracting precise information from production language models like OpenAI's ChatGPT and Google's PaLM-2 through API queries. By leveraging a top-down approach, the authors develop methods to recover the entire embedding projection layer of transformer models based on access to logit data. The attack effectively identifies the hidden dimensions of various models and successfully extracts their projection matrices at a minimal cost, indicating significant vulnerabilities in current model API designs. Additionally, the authors discuss potential defenses against such attacks and highlight the implications for future work in the field of adversarial machine learning.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=VE3yWXt3KB&name=pdf" class="link-primary">https://openreview.net/attachment?id=VE3yWXt3KB&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Stereo Risk: A Continuous Modeling Approach to Stereo Matching</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">stereo matching</span>
<span class="badge bg-primary">deep learning</span>
<span class="badge bg-primary">continuous risk minimization</span>
<span class="badge bg-primary">disparity estimation</span>
<span class="badge bg-primary">cross-domain generalization</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces "Stereo Risk," a novel deep-learning framework for stereo matching that formulates disparity estimation as a continuous risk minimization problem, significantly improving accuracy on various benchmark datasets.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper presents "Stereo Risk," a deep-learning approach to enhance stereo matching by addressing the challenge of continuous disparity estimation. Unlike traditional methods that discretize disparity values, Stereo Risk treats disparity as an optimal solution to a continuous risk minimization problem. The authors utilize L1 minimization for robustness against multi-modal distribution errors and employ the implicit function theorem for end-to-end differentiable network training. Extensive evaluations show that Stereo Risk outperforms state-of-the-art methods on multiple benchmark datasets, including KITTI and Middlebury, demonstrating better generalization across different domains, thereby advancing the field of stereo matching in computer vision.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Mfk6ZbD6eY&name=pdf" class="link-primary">https://openreview.net/attachment?id=Mfk6ZbD6eY&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Stop Regressing: Training Value Functions via Classification for Scalable Deep RL</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">value functions</span>
<span class="badge bg-primary">classification</span>
<span class="badge bg-primary">reinforcement learning</span>
<span class="badge bg-primary">scalability</span>
<span class="badge bg-primary">deep learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper demonstrates that using classification with categorical cross-entropy instead of regression with mean squared error significantly improves the performance and scalability of value functions in deep reinforcement learning.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of training value functions within deep reinforcement learning (RL) by proposing a shift from mean squared error regression to using classification via categorical cross-entropy. The authors investigate various methods for deriving classification labels and show that these classification approaches, particularly the histogram-based Gaussian (HL-Gauss) loss, lead to enhanced performance and scalability across multiple RL domains, including Atari games, robotics, chess, and a language-based game. The findings indicate that classification mitigates issues such as noisy targets and non-stationarity found in traditional value-based methods, suggesting that this approach may provide a pathway to effectively scale deep RL algorithms to larger networks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=dVpFKfqF3R&name=pdf" class="link-primary">https://openreview.net/attachment?id=dVpFKfqF3R&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">symbolic music generation</span>
<span class="badge bg-primary">non-differentiable rule guidance</span>
<span class="badge bg-primary">stochastic control</span>
<span class="badge bg-primary">diffusion models</span>
<span class="badge bg-primary">compositional tools</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces Stochastic Control Guidance (SCG), a novel method for guiding diffusion models in symbolic music generation while adhering to non-differentiable musical rules, significantly improving output quality and controllability.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of symbolic music generation by proposing Stochastic Control Guidance (SCG), a method allowing for the integration of non-differentiable musical rules into pre-trained diffusion models in a plug-and-play manner. The authors developed a latent diffusion architecture capable of generating high-resolution music samples, employing SCG to sample multiple realizations at each step and selecting those that align best with specified musical rules, such as chord progression and note density. The experimental results demonstrate that the proposed framework surpasses existing state-of-the-art models in music quality and rule-based controllability, providing musicians a versatile tool for music composition. The findings suggest that SCG can be generalized to other fields requiring adherence to non-differentiable constraints, paving the way for broader applications of this approach.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=g8AigOTNXL&name=pdf" class="link-primary">https://openreview.net/attachment?id=g8AigOTNXL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Test-Time Model Adaptation with Only Forward Passes</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Test-Time Adaptation</span>
<span class="badge bg-primary">Forward-Optimization Adaptation</span>
<span class="badge bg-primary">Covariance Matrix Adaptation</span>
<span class="badge bg-primary">Quantized Models</span>
<span class="badge bg-primary">Distribution Shift</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces a novel test-time adaptation method called Forward-Optimization Adaptation (FOA) that operates without backpropagation, enabling improvements in model performance on unseen test samples, especially when deployed on resource-limited devices.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the challenge of adapting neural networks to unseen test samples with distribution shifts, particularly in scenarios where backpropagation is infeasible due to computational constraints of edge devices. The authors propose a test-time Forward-Optimization Adaptation (FOA) method, which learns a new input prompt using a derivative-free covariance matrix adaptation strategy, while also employing a novel fitness function based on the discrepancy between test and training statistics. They further enhance adaptation performance with an activation shifting scheme that aligns model activations from the testing domain back to the source domain. Through extensive experiments, FOA demonstrates superior performance over gradient-based methods, achieving significant memory reductions while maintaining accuracy, especially when applied to quantized models. This advancement broadens the applicability of test-time adaptation in real-world applications, including smartphones and FPGAs.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=qz1Vx1v9iK&name=pdf" class="link-primary">https://openreview.net/attachment?id=qz1Vx1v9iK&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">copyright infringement</span>
<span class="badge bg-primary">backdoor attack</span>
<span class="badge bg-primary">data poisoning</span>
<span class="badge bg-primary">text-to-image diffusion models</span>
<span class="badge bg-primary">SilentBadDiffusion</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This study introduces SilentBadDiffusion, a backdoor attack method that induces copyright infringement in text-to-image diffusion models through subtle data poisoning without controlling the training process.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses copyright concerns associated with text-to-image diffusion models by proposing a novel backdoor attack method called SilentBadDiffusion. This approach involves embedding poisoned data into clean training datasets, which allows the diffusion models to generate copyrighted images when triggered by specific textual prompts while maintaining their general functionality with regular prompts. Through experiments, the authors demonstrate that even a small percentage of poisoned data (0.20%) can effectively induce copyright infringement, particularly highlighting that more sophisticated models are more vulnerable to such attacks. The findings emphasize the shortcomings of current copyright protection strategies and call for greater awareness of the potential misuse of these generative models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ZvFLbEPv6x&name=pdf" class="link-primary">https://openreview.net/attachment?id=ZvFLbEPv6x&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">learned database operations</span>
<span class="badge bg-primary">distribution shift</span>
<span class="badge bg-primary">distribution learnability</span>
<span class="badge bg-primary">indexing</span>
<span class="badge bg-primary">cardinality estimation</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper presents a theoretical framework for analyzing learned database operations under distribution shift, focusing on indexing, cardinality estimation, and sorting, and provides guarantees for their performance compared to non-learned methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper offers the first theoretical characterization of learned database operations in the context of dynamic datasets where data distribution may shift over time. Through the novel concept of distribution learnability, the authors analyze three fundamental database operations: indexing, cardinality estimation, and sorting. They establish bounds on the performance of learned models, showing when they outperform traditional non-learned methods and offering insights into the implications of distribution shifts. The results demonstrate that learned methods can provide efficiency benefits under certain conditions, while also addressing challenges related to model retraining and accuracy maintenance in dynamic environments. Overall, this work lays a groundwork for future investigations into learned database operations, enriching our understanding of their practical applicability and theoretical foundations.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=oowQ8LPA12&name=pdf" class="link-primary">https://openreview.net/attachment?id=oowQ8LPA12&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">optimal robust policy</span>
<span class="badge bg-primary">deep reinforcement learning</span>
<span class="badge bg-primary">Bellman infinity-error</span>
<span class="badge bg-primary">adversarial robustness</span>
<span class="badge bg-primary">consistency assumption</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper establishes the existence of an optimal robust policy (ORP) within the state-adversarial framework of deep reinforcement learning, demonstrating that minimizing Bellman infinity-error leads to superior robustness without sacrificing performance.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The research investigates the concept of an optimal robust policy (ORP) in deep reinforcement learning (DRL) agents that can withstand adversarial attacks. By introducing the consistency assumption of policy (CAP), the authors prove that the Bellman optimal policy coincides with the ORP under certain conditions. They highlight the necessity of using L-norm when minimizing Bellman error to achieve robustness, contrasting it with previous methods that often employed L1-norm. The authors propose a new algorithm, Consistent Adversarial Robust Deep Q-Network (CAR-DQN), designed to minimize a surrogate of Bellman infinity-error. Extensive experiments across various Atari benchmarks indicate that CAR-DQN outperforms state-of-the-art approaches in both natural and adversarial environments, reinforcing the theoretical findings and implications for implementing robust DRL systems in real-world scenarios.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=pgI9inG2Ny&name=pdf" class="link-primary">https://openreview.net/attachment?id=pgI9inG2Ny&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Trained Random Forests Completely Reveal your Dataset</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">dataset reconstruction</span>
<span class="badge bg-primary">privacy attacks</span>
<span class="badge bg-primary">random forests</span>
<span class="badge bg-primary">constraint programming</span>
<span class="badge bg-primary">machine learning ethics</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents a method to reconstruct training datasets from trained random forests using optimization-based techniques, demonstrating significant data recovery even in the presence of bootstrap aggregation.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This study introduces an optimization-based reconstruction attack that effectively reconstructs datasets used to train random forests, utilizing information from open-source libraries like scikit-learn. The authors formalize the dataset reconstruction problem as a maximum likelihood problem, showing it is NP-hard but solvable using constraint programming techniques. Their extensive experiments reveal that random forests trained without bootstrap aggregation can be nearly completely reconstructed, while even with bootstrap aggregation, most of the data can still be retrieved. The findings highlight critical vulnerabilities inherent in popular machine learning methods, raising important ethical and privacy concerns regarding the use of sensitive data in machine learning applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=cc72Vnfvoc&name=pdf" class="link-primary">https://openreview.net/attachment?id=cc72Vnfvoc&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Transformers</span>
<span class="badge bg-primary">In-context learning</span>
<span class="badge bg-primary">Mean-field dynamics</span>
<span class="badge bg-primary">Nonlinear features</span>
<span class="badge bg-primary">Optimization landscape</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper investigates how Transformers with MLP layers enhance in-context learning through exploration of their non-convex optimization landscape, demonstrating that mean-field dynamics effectively avoid saddle points and converge to globally optimal representations.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper explores the optimization dynamics of a Transformer model that incorporates a two-layer MLP followed by a linear attention layer, specifically focusing on how this architecture augments the model's ability to perform in-context learning (ICL). The authors prove that in the mean-field and two-timescale limits, the loss landscape for this infinite-dimensional model is highly non-convex yet behaves benignly, as all critical points are either global minima or saddle points. Through analysis of Wasserstein gradient flow, it is shown that the dynamics almost always evade saddle points, leading to favorable convergence rates in various scenarios. The authors also establish local stability properties and provide numerical experiments that support their theoretical findings, indicating the potential for improved understanding of nonlinear feature learning in Transformer architectures.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=xm2lU7tteQ&name=pdf" class="link-primary">https://openreview.net/attachment?id=xm2lU7tteQ&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Unified Training of Universal Time Series Forecasting Transformers</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">universal forecasting</span>
<span class="badge bg-primary">time series</span>
<span class="badge bg-primary">Transformer architecture</span>
<span class="badge bg-primary">pre-training</span>
<span class="badge bg-primary">probabilistic forecasting</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces MOIRAI, a masked encoder-based universal time series forecasting Transformer trained on the Large-scale Open Time Series Archive (LOTSA), demonstrating competitive zero-shot forecasting performance across diverse datasets.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the limitations of traditional single-dataset models for time series forecasting by proposing a novel architecture, MOIRAI, designed for universal forecasting. The authors enhance the conventional Transformer model to accommodate various challenges in time series data, such as cross-frequency learning and flexible distributional properties. They pre-train MOIRAI on the newly introduced LOTSA, which comprises over 27 billion observations from nine different domains. The experimental results indicate that MOIRAI achieves competitive or superior forecasting performance compared to existing models, particularly in zero-shot settings, thereby demonstrating the potential of universal forecasting models in real-world applications.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=Yd8eHMY1wz&name=pdf" class="link-primary">https://openreview.net/attachment?id=Yd8eHMY1wz&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">video-language pre-training</span>
<span class="badge bg-primary">multimodal models</span>
<span class="badge bg-primary">tokenization</span>
<span class="badge bg-primary">motion vectors</span>
<span class="badge bg-primary">generative modeling</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents Video-LaVIT, a unified video-language pre-training method that efficiently models video dynamics through a novel tokenization approach using keyframes and motion vectors, achieving competitive performance across various multimodal benchmarks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper introduces Video-LaVIT, a framework designed for pre-training large multimodal language models (LLMs) on video data, addressing the challenges posed by spatiotemporal dynamics in videos compared to images. The key innovation includes a decoupled tokenization strategy that represents videos using keyframes and motion vectors, which reduces redundancy and computational costs while allowing efficient integration into the LLM architecture. Experiments demonstrate that Video-LaVIT excels in both video comprehension and generation tasks, outperforming several existing models across 13 multimodal benchmarks without requiring additional fine-tuning. The framework enables enhanced reasoning and generation capabilities, contributing to the broader field of multimodal AI by leveraging efficient tokenization techniques.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=S9lk6dk4LL&name=pdf" class="link-primary">https://openreview.net/attachment?id=S9lk6dk4LL&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">video reasoning</span>
<span class="badge bg-primary">multimodal language model</span>
<span class="badge bg-primary">spatial-temporal scene graph</span>
<span class="badge bg-primary">commonsense reasoning</span>
<span class="badge bg-primary">Video-of-Thought framework</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces the Video-of-Thought reasoning framework, which employs a novel multimodal language model called MotionEpic to enhance the understanding and reasoning of complex video content through structured spatial-temporal scene graphs.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenges of in-depth video comprehension and reasoning by proposing a new framework termed Video-of-Thought (VoT), built on the MotionEpic multimodal language model (MLLM). MotionEpic achieves fine-grained spatial-temporal video grounding by integrating a video spatial-temporal scene graph (STSG) representation, enabling systematic decomposing of complex video questions into manageable sub-tasks. The VoT framework facilitates a step-by-step approach from pixel-level perception to high-level cognitive understanding, significantly boosting performance on various complex video question-answering benchmarks and setting new state-of-the-art results. The findings demonstrate the framework's potential to advance human-level reasoning capabilities in video understanding tasks, while also revealing the importance of integrating commonsense knowledge and definitive video structure in achieving this goal.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=fO31YAyNbI&name=pdf" class="link-primary">https://openreview.net/attachment?id=fO31YAyNbI&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">VideoPoet: A Large Language Model for Zero-Shot Video Generation</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">video generation</span>
<span class="badge bg-primary">large language models</span>
<span class="badge bg-primary">multimodal inputs</span>
<span class="badge bg-primary">zero-shot capability</span>
<span class="badge bg-primary">transformer architecture</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper introduces VideoPoet, a large language model designed for high-quality video generation from a wide range of inputs, demonstrating state-of-the-art zero-shot capabilities.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper presents VideoPoet, a model that synthesizes high-quality videos by leveraging a decoder-only transformer architecture that processes various multimodal inputs, including text, images, videos, and audio. The training process involves two stages: a broad pretraining phase using diverse multimodal generative objectives, followed by task-specific adaptation. VideoPoet showcases its ability to generate realistic motions and coherent long videos up to 10 seconds in length, achieving competitive performance against existing diffusion-based video generation models. Importantly, the model is capable of zero-shot video generation, enabling it to handle unseen inputs and tasks not present in its training dataset. Overall, the paper contributes to advancing video generation techniques by utilizing large language models, highlighting potential applications in creative media production.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=LRkJwPIDuE&name=pdf" class="link-primary">https://openreview.net/attachment?id=LRkJwPIDuE&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">ViP: A Differentially Private Foundation Model for Computer Vision</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Differential Privacy</span>
<span class="badge bg-primary">Foundation Models</span>
<span class="badge bg-primary">Self-Supervised Learning</span>
<span class="badge bg-primary">Vision Transformers</span>
<span class="badge bg-primary">Data Privacy</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">The paper presents ViP, a differentially private foundation model for computer vision, trained using self-supervised learning techniques that ensure privacy while achieving competitive performance on various vision tasks.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">This paper addresses the pressing issue of privacy and legal risks associated with training foundation models on uncurated data containing personal information by proposing a method to train differentially private vision models. The authors use masked autoencoders aligned with differential privacy techniques to develop ViP, a vision transformer model trained on the LAION400M dataset under a strict privacy budget. The results demonstrate that ViP achieves comparable representation learning performance to state-of-the-art models while ensuring strong privacy guarantees. Additionally, the study shows that leveraging synthetic datasets for warm-starting the training process enhances efficiency and quality, suggesting a successful recipe for future research on scaling differentially private learning methods.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=6aKwVmHQI1&name=pdf" class="link-primary">https://openreview.net/attachment?id=6aKwVmHQI1&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">weak supervision</span>
<span class="badge bg-primary">strong models</span>
<span class="badge bg-primary">generalization</span>
<span class="badge bg-primary">alignment</span>
<span class="badge bg-primary">reinforcement learning</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper explores whether weak supervision can effectively elicit the full capabilities of strong, pretrained models, proposing that weak-to-strong generalization is feasible with simple methods.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper investigates the challenge of aligning superhuman models when humans can only provide weak supervision, proposing the concept of weak-to-strong generalization, where strong pretrained models learn from weak models' outputs. The authors conducted experiments using the GPT-4 family of models on various tasks, finding that naive fine-tuning often results in models that outperform their weak supervisors. Despite this, there remains a significant performance gap compared to models trained with high-quality supervision. The authors demonstrate that integrating techniques like auxiliary confidence loss and bootstrapping can improve model performance and that weak-to-strong generalization is a promising direction for future research in AI alignment. The results indicate that it might be possible to develop effective reward models and safety classifiers from weak human supervision to assist in the alignment of superhuman models.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=ghNRg2mEgN&name=pdf" class="link-primary">https://openreview.net/attachment?id=ghNRg2mEgN&name=pdf</a></p>
</div>
</div>
</div>

<div class="col-sm-12 col-lg-6 col-xl-4 mb-4">
<div class="card shadow-sm">
<div class="card-body">
<h3 class="card-title h4">Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization</h3>
<div class="mb-3">
<div class="d-flex gap-2 flex-wrap">
<span class="badge bg-primary">Stochastic Optimization</span>
<span class="badge bg-primary">Nonsmooth Functions</span>
<span class="badge bg-primary">Nonconvex Problems</span>
<span class="badge bg-primary">Zeroth-Order Methods</span>
<span class="badge bg-primary">Approximate Stationarity</span>
</div>
</div>
<div class="mb-3">
<h3 class="h5">TL;DR</h3>
<p class="card-text">This paper introduces non-asymptotic convergence guarantees for stochastic zeroth-order methods aimed at solving constrained nonconvex nonsmooth optimization problems by defining new notions of approximate stationarity.</p>
</div>
<div class="mb-3">
<h3 class="h5">Summary</h3>
<p class="card-text">The paper addresses the challenge of constrained nonconvex nonsmooth optimization problems commonly encountered in machine learning. It critiques existing methods that transform such problems into unconstrained ones, which primarily offer asymptotic convergence analysis. By generalizing classical concepts like gradient mapping and the Frank-Wolfe gap to the nonsmooth context, the authors propose novel definitions of approximate stationarity. They develop several stochastic zeroth-order algorithms (both projection-based and projection-free) that demonstrate non-asymptotic convergence to the defined stationary points, significantly enhancing practical applicability. Experimental results affirm the efficiency of the proposed methods, underscoring their relevance in real-world optimization tasks.</p>
</div>
<p class="card-text"><strong>Paper URL:</strong> <a href="https://openreview.net/attachment?id=PxHmxoFOgI&name=pdf" class="link-primary">https://openreview.net/attachment?id=PxHmxoFOgI&name=pdf</a></p>
</div>
</div>
</div>

        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
